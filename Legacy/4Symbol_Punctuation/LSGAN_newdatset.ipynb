{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSGAN_newdatset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "foR_ZBqnWQu5",
        "colab_type": "code",
        "outputId": "fa2ee1b0-45dc-4fd9-d85c-1d39ff675440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!pip install python-resize-image\n",
        "!pip install -U scipy==1.2.0\n",
        "!pip install python-archive\n",
        "import numpy as np\n",
        "from numpy.random import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from skimage import data, io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "from archive import Archive\n",
        "from archive import extract\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-resize-image in /usr/local/lib/python3.6/dist-packages (1.1.19)\n",
            "Requirement already satisfied: requests>=2.19.1 in /usr/local/lib/python3.6/dist-packages (from python-resize-image) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from python-resize-image) (7.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (2.9)\n",
            "Requirement already up-to-date: scipy==1.2.0 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.18.4)\n",
            "Requirement already satisfied: python-archive in /usr/local/lib/python3.6/dist-packages (0.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KneN9w6nyOsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "from resizeimage import resizeimage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfDk5_9YWcj5",
        "colab_type": "code",
        "outputId": "b9a1a799-1f3c-4df1-ae05-083a752aaa65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3-eu-west-1.amazonaws.com/handwriting-curated-database/curated.tar.gz"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 08:27:00--  https://s3-eu-west-1.amazonaws.com/handwriting-curated-database/curated.tar.gz\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.57.219\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.57.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30058219 (29M) [application/x-gzip]\n",
            "Saving to: ‘curated.tar.gz.1’\n",
            "\n",
            "curated.tar.gz.1    100%[===================>]  28.67M  18.6MB/s    in 1.5s    \n",
            "\n",
            "2020-05-30 08:27:02 (18.6 MB/s) - ‘curated.tar.gz.1’ saved [30058219/30058219]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGG5g7yvqQNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extract('curated.tar.gz', 'curated_data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNAPAoDqd-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from scipy.misc import imresize, imrotate, imsave\n",
        "\n",
        "# display plots in this notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# set display defaults\n",
        "plt.rcParams['figure.figsize'] = (10, 10) \n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray' \n",
        "\n",
        "\n",
        "path = 'curated_data' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQIJql34rZiD",
        "colab_type": "code",
        "outputId": "626e1401-596e-494a-dec4-675bc98efcfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "#punc = '!\\\"\\'()*+,-./:;<=>?[]^_`{|}~' #remove @=64, $=36, &=38, #=35, %=37\n",
        "punc = '!,.?'\n",
        "#lett = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
        "#num = '0123456789'\n",
        "character_curated = [ord(c) for c in punc]\n",
        "print([chr(i) for i in character_curated])\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "for i in character_curated:\n",
        "    path_img = path + '/curated/' + str(i) + '/'\n",
        "    for file_name in [f for f in listdir(path_img) if isfile(join(path_img, f))]:\n",
        "        img = cv2.imread(path_img + file_name, 0)\n",
        "        img = cv2.resize(img,(28, 28), interpolation = cv2.INTER_AREA)\n",
        "        images += [img]\n",
        "        labels += [i]\n",
        "\n",
        "images = np.array(images, dtype=np.uint8)\n",
        "labels = np.array(labels, dtype=np.uint8)\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', ',', '.', '?']\n",
            "(1207, 28, 28)\n",
            "(1207,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_aMVoMdtQf1",
        "colab_type": "code",
        "outputId": "5211868d-f66d-42ab-d8dc-ed39decfe9f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "#for i in range(images.shape[0]):\n",
        "  #images[i] = cv2.blur(images[i],(2,2))\n",
        "plt.imshow(images[0])\n",
        "print(labels[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPC37DrQyNeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_discriminator(input_shape=(28, 28, 1), n_classes=4):\n",
        "    # label input and embedding\n",
        "    label_in = Input(shape=(1, ))\n",
        "    print(label_in.shape)\n",
        "    emb = Embedding(n_classes, 50)(label_in)\n",
        "    print(emb.shape)\n",
        "    label_h = Dense(input_shape[0] * input_shape[1])(emb)\n",
        "    print(label_h.shape)\n",
        "    re_label_h = Reshape((input_shape[0], input_shape[1], 1))(label_h)\n",
        "    print(re_label_h.shape)\n",
        "    # image input\n",
        "    image_in = Input(shape=input_shape)\n",
        "    # combine inputs\n",
        "    merge = Concatenate()([image_in, re_label_h])\n",
        "    # convnet\n",
        "    h1 = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(merge)\n",
        "    r1 = LeakyReLU(alpha=0.2)(h1)\n",
        "    h2 = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(r1)\n",
        "    r2 = LeakyReLU(alpha=0.2)(h2)\n",
        "    # fully connected net\n",
        "    fl = Flatten()(r2) \n",
        "    dr = Dropout(0.4)(fl)\n",
        "    # output\n",
        "    out = Dense(1, activation='sigmoid')(dr)\n",
        "    # define and compile model\n",
        "    model = Model([image_in, label_in], out)\n",
        "    opt = Adam(lr=2e-4, beta_1=0.5)\n",
        "    model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGeRstGNKcXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_generator(latent_dim, n_classes=4):\n",
        "    # label input and embedding\n",
        "    label_in = Input(shape=(1, ))\n",
        "    emb = Embedding(n_classes, 50)(label_in)\n",
        "    label_h = Dense(7*7)(emb)\n",
        "    re_label_h = Reshape((7, 7, 1))(label_h)\n",
        "    # noisy image input\n",
        "    noise_in = Input(shape=(latent_dim,))\n",
        "    noise_h = Dense(128*7*7)(noise_in)\n",
        "    noise_r = LeakyReLU(alpha=0.2)(noise_h)\n",
        "    re_noise_r = Reshape((7, 7, 128))(noise_r)\n",
        "    # combine inputs\n",
        "    merge = Concatenate()([re_noise_r, re_label_h])\n",
        "    # upsampling\n",
        "    u1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(merge)\n",
        "    r1 = LeakyReLU(alpha=0.2)(u1)\n",
        "    u2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(r1)\n",
        "    r2 = LeakyReLU(alpha=0.2)(u2)\n",
        "    # output\n",
        "    out = Conv2D(1, (7, 7), activation='tanh', padding='same')(r2)\n",
        "    # define model\n",
        "    model = Model([noise_in, label_in], out)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK-BXmPXLygJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_gan(gen, dis):\n",
        "    # discriminator shouldn't be trainable\n",
        "    dis.trainable = False\n",
        "    # get generator inputs and outputs\n",
        "    gen_noise, gen_label = gen.input\n",
        "    gen_output = gen.output\n",
        "    print\n",
        "    # feed to discriminator\n",
        "    gan_output = dis([gen_output, gen_label])\n",
        "    # define and compile GAN model\n",
        "    model = Model([gen_noise, gen_label], gan_output)\n",
        "    opt = Adam(lr=2e-4, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-lpv0kd5_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labels_updated(labels):\n",
        "  for i in range(labels.shape[0]):\n",
        "    #if labels[i] in range(65,91):\n",
        "      #labels[i] -=65\n",
        "    #if labels[i] in range(97,123):\n",
        "      #labels[i]-=71\n",
        "    #if labels[i] in range(48,58):\n",
        "      #labels[i]+=4\n",
        "    #if labels[i] in range(33,48):\n",
        "      #labels[i] -=33\n",
        "    #if labels[i] in range(58,65):\n",
        "      #labels[i]-=43\n",
        "    #if labels[i] == 91:\n",
        "      #labels[i] = 22\n",
        "    #if labels[i] in range(93,97):\n",
        "      #labels[i]-=70\n",
        "    #if labels[i] in range(123,127):\n",
        "      #labels[i]-=96\n",
        "    #if labels[i] in range(33,35):\n",
        "      #labels[i]-=33\n",
        "    #if labels[i] in range(39,48):\n",
        "      #labels[i]-=37\n",
        "    #if labels[i] in range(58,64): \n",
        "      #labels[i]-=47\n",
        "    #if labels[i] == 91:\n",
        "      #labels[i]-=74\n",
        "    #if labels[i] in range(93,97):\n",
        "      #labels[i]-=75\n",
        "    #if labels[i] in range(123,127):\n",
        "      #labels[i]-=101\n",
        "    if labels[i]==33:\n",
        "      labels[i]=0\n",
        "    if labels[i]==44: \n",
        "      labels[i]=1\n",
        "    if labels[i]==46:\n",
        "      labels[i]=2\n",
        "    if labels[i]==63:\n",
        "      labels[i]=3 \n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ENBmPwb2Zj",
        "colab_type": "text"
      },
      "source": [
        " if labels[i] in range(33,48):\n",
        "      labels[i]+=29\n",
        "    if labels[i] in range(58,65):\n",
        "      labels[i]+=19\n",
        "    if labels[i] == 91:\n",
        "      labels[i] = 84\n",
        "    if labels[i] in range(93,97):\n",
        "      labels[i]-=8\n",
        "    if labels[i] in range(123,127):\n",
        "      labels[i]-=34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFDDNNAnL2lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_inputs(images, labels):\n",
        "    if len(images.shape) > 2:\n",
        "      images = np.squeeze(images)\n",
        "    X = np.expand_dims(images, axis=-1)\n",
        "    X = X.astype('float32')\n",
        "    X = (X-127.5) / 127.5\n",
        "    labels = labels_updated(labels)\n",
        "    return [X, labels]\n",
        "    \n",
        "def generate_real_samples(images, labels, n_samples):\n",
        "    rand_index = randint(0, images.shape[0], n_samples)\n",
        "    X, labels = images[rand_index], labels[rand_index]\n",
        "    y = np.ones((n_samples, 1)) * 0.9 # discriminator target label\n",
        "    # label smoothing\n",
        "    #ind = np.random.choice(list(range(len(y))), size=int(len(y) * 0.01), replace=False)\n",
        "    #y[ind] = 1-y[ind]\n",
        "    return [X, labels], y\n",
        "\n",
        "def generate_latent_noise(latent_dim, n_samples, n_classes=4):\n",
        "    xin = randn(latent_dim * n_samples)\n",
        "    xin = xin.reshape(n_samples, latent_dim)\n",
        "    labels = randint(0, n_classes, n_samples)  #  generator class label\n",
        "    return xin, labels\n",
        "\n",
        "def generate_fake_samples(gen, latent_dim, n_samples):\n",
        "    zin, lin = generate_latent_noise(latent_dim, n_samples)\n",
        "    images = gen.predict([zin, lin])\n",
        "    y = np.zeros((n_samples, 1))  # discriminator target label\n",
        "    # label smoothing\n",
        "    #ind = np.random.choice(list(range(len(y))), size=int(len(y) * 0.01), replace=False)\n",
        "    #y[ind] = 1-y[ind]\n",
        "    return [images, lin], y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdaO0cQYoY1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_gan(gen, dis, gan_model, images, labels, latent_dim, n_epochs=100, batch_size=128):\n",
        "    batch_per_epoch = int(images.shape[0] / batch_size)\n",
        "    half_batch = int(batch_size / 2)\n",
        "    # enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        for j in range(batch_per_epoch):\n",
        "            # train discriminator on real images\n",
        "            [X_real, labels_real], y_real = generate_real_samples(images, labels, half_batch)\n",
        "            d_loss1, _ = dis.train_on_batch([X_real, labels_real], y_real)\n",
        "            # train discriminator on generated images\n",
        "            [X_fake, labels_fake], y_fake = generate_fake_samples(gen, latent_dim, half_batch)\n",
        "            #d_loss2 = dis.evaluate([X_fake, labels_fake], y_fake)[0]\n",
        "            #if d_loss2 > 0.5:\n",
        "            d_loss2, _ = dis.train_on_batch([X_fake, labels_fake], y_fake)\n",
        "            # prepare generator input\n",
        "            [zin, label_in] = generate_latent_noise(latent_dim, batch_size)\n",
        "            # invert labels for fake samples (prevent vanishing gradients)\n",
        "            y_gan = np.ones((batch_size, 1))\n",
        "            # label smoothing\n",
        "            #ind = np.random.choice(list(range(len(y_gan))), size=int(len(y_gan) * 0.01), replace=False)\n",
        "            #y_gan[ind] = 1-y_gan[ind]\n",
        "            # update generator loss\n",
        "            g_loss = gan_model.train_on_batch([zin, label_in], y_gan)\n",
        "            # output losses\n",
        "            if j % 50 == 0:\n",
        "              print('Epoch {}, batch {}/{}:\\tDiscriminator: real loss {}, fake loss {}\\tGenerator: loss {}'\n",
        "                  .format(i+1, j+1, batch_per_epoch, d_loss1, d_loss2, g_loss))\n",
        "    # save the models\n",
        "    gen.save('generator.h5')\n",
        "    dis.save('discriminator.h5')\n",
        "    gan_model.save('gan.h5')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QYYbvDtbiQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def labels_mapping():\n",
        "#  labels_map = {i-65 :i for i in range(65, 91)}\n",
        "#  labels_map.update({i-71 : i for i in range(97,123)})\n",
        "#  return labels_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0brxTIVodZ8",
        "colab_type": "code",
        "outputId": "9dea191c-95d2-4b34-f011-cc1a0256ab23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "latent_dim = 300\n",
        "images, labels = prepare_inputs(images, labels)\n",
        "print(images.shape, labels.shape)\n",
        "#labels_map = labels_mapping()\n",
        "dis = define_discriminator()\n",
        "gen = define_generator(latent_dim)\n",
        "gan_model = define_gan(gen, dis)\n",
        "print(\"\\nDiscriminator\\n\")\n",
        "dis.summary()\n",
        "print(\"\\nGenerator\\n\")\n",
        "gen.summary()\n",
        "print(\"\\nGAN\\n\")\n",
        "gan_model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1207, 28, 28, 1) (1207,)\n",
            "(None, 1)\n",
            "(None, 1, 50)\n",
            "(None, 1, 784)\n",
            "(None, 28, 28, 1)\n",
            "\n",
            "Discriminator\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 50)        200         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1, 784)       39984       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 28, 28, 1)    0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 28, 28, 2)    0           input_6[0][0]                    \n",
            "                                                                 reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 14, 14, 64)   1216        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 14, 14, 64)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 7, 7, 64)     36928       leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 7, 7, 64)     0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 3136)         0           leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 3136)         0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            3137        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 81,465\n",
            "Trainable params: 0\n",
            "Non-trainable params: 81,465\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Generator\n",
            "\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 6272)         1887872     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 50)        200         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 6272)         0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1, 49)        2499        embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 7, 7, 128)    0           leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 7, 7, 1)      0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 7, 7, 129)    0           reshape_5[0][0]                  \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 14, 14, 128)  264320      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 128)  0           conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 128)  262272      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 28, 28, 128)  0           conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 1)    6273        leaky_re_lu_9[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,423,436\n",
            "Trainable params: 2,423,436\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "GAN\n",
            "\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 6272)         1887872     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 50)        200         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 6272)         0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1, 49)        2499        embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 7, 7, 128)    0           leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 7, 7, 1)      0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 7, 7, 129)    0           reshape_5[0][0]                  \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 14, 14, 128)  264320      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 128)  0           conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 128)  262272      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 28, 28, 128)  0           conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 1)    6273        leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "model_3 (Model)                 (None, 1)            81465       conv2d_5[0][0]                   \n",
            "                                                                 input_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,504,901\n",
            "Trainable params: 2,423,436\n",
            "Non-trainable params: 81,465\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXPvq0QWoiGH",
        "colab_type": "code",
        "outputId": "d8b3b82e-bbdf-4821-8670-9b34dd0bcecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_gan(gen, dis, gan_model, images, labels, latent_dim, n_epochs=1000, batch_size=128)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, batch 1/9:\tDiscriminator: real loss 0.1704576313495636, fake loss 0.15703058242797852\tGenerator: loss 1.0406097173690796\n",
            "Epoch 2, batch 1/9:\tDiscriminator: real loss 0.17299368977546692, fake loss 0.1492937207221985\tGenerator: loss 1.079445481300354\n",
            "Epoch 3, batch 1/9:\tDiscriminator: real loss 0.15392673015594482, fake loss 0.2211114913225174\tGenerator: loss 0.9471443891525269\n",
            "Epoch 4, batch 1/9:\tDiscriminator: real loss 0.18527013063430786, fake loss 0.16319873929023743\tGenerator: loss 0.9635258913040161\n",
            "Epoch 5, batch 1/9:\tDiscriminator: real loss 0.14297688007354736, fake loss 0.18531908094882965\tGenerator: loss 0.9911876320838928\n",
            "Epoch 6, batch 1/9:\tDiscriminator: real loss 0.16920500993728638, fake loss 0.18954506516456604\tGenerator: loss 1.02012300491333\n",
            "Epoch 7, batch 1/9:\tDiscriminator: real loss 0.1797306090593338, fake loss 0.17484314739704132\tGenerator: loss 0.9788437485694885\n",
            "Epoch 8, batch 1/9:\tDiscriminator: real loss 0.19701510667800903, fake loss 0.21738307178020477\tGenerator: loss 0.978715181350708\n",
            "Epoch 9, batch 1/9:\tDiscriminator: real loss 0.20584885776042938, fake loss 0.20216771960258484\tGenerator: loss 0.9086955785751343\n",
            "Epoch 10, batch 1/9:\tDiscriminator: real loss 0.2094351351261139, fake loss 0.2090519666671753\tGenerator: loss 0.9916883707046509\n",
            "Epoch 11, batch 1/9:\tDiscriminator: real loss 0.20633286237716675, fake loss 0.17635534703731537\tGenerator: loss 1.08211350440979\n",
            "Epoch 12, batch 1/9:\tDiscriminator: real loss 0.23442889750003815, fake loss 0.19885599613189697\tGenerator: loss 1.04013192653656\n",
            "Epoch 13, batch 1/9:\tDiscriminator: real loss 0.18100526928901672, fake loss 0.2009100466966629\tGenerator: loss 0.9611004590988159\n",
            "Epoch 14, batch 1/9:\tDiscriminator: real loss 0.17080199718475342, fake loss 0.20362655818462372\tGenerator: loss 1.000915288925171\n",
            "Epoch 15, batch 1/9:\tDiscriminator: real loss 0.2012120485305786, fake loss 0.18456918001174927\tGenerator: loss 0.9601315259933472\n",
            "Epoch 16, batch 1/9:\tDiscriminator: real loss 0.17762887477874756, fake loss 0.20837245881557465\tGenerator: loss 0.9846833944320679\n",
            "Epoch 17, batch 1/9:\tDiscriminator: real loss 0.15161195397377014, fake loss 0.18619422614574432\tGenerator: loss 0.9616948962211609\n",
            "Epoch 18, batch 1/9:\tDiscriminator: real loss 0.17395710945129395, fake loss 0.20280197262763977\tGenerator: loss 0.9932196140289307\n",
            "Epoch 19, batch 1/9:\tDiscriminator: real loss 0.16683246195316315, fake loss 0.18648509681224823\tGenerator: loss 1.0949561595916748\n",
            "Epoch 20, batch 1/9:\tDiscriminator: real loss 0.1480552852153778, fake loss 0.152308851480484\tGenerator: loss 1.120635747909546\n",
            "Epoch 21, batch 1/9:\tDiscriminator: real loss 0.16873469948768616, fake loss 0.1888151913881302\tGenerator: loss 0.9445654153823853\n",
            "Epoch 22, batch 1/9:\tDiscriminator: real loss 0.18596407771110535, fake loss 0.156816765666008\tGenerator: loss 1.0205035209655762\n",
            "Epoch 23, batch 1/9:\tDiscriminator: real loss 0.17986956238746643, fake loss 0.14484016597270966\tGenerator: loss 1.037346363067627\n",
            "Epoch 24, batch 1/9:\tDiscriminator: real loss 0.15542073547840118, fake loss 0.16346710920333862\tGenerator: loss 0.998314380645752\n",
            "Epoch 25, batch 1/9:\tDiscriminator: real loss 0.1701403111219406, fake loss 0.1814478188753128\tGenerator: loss 0.9654209613800049\n",
            "Epoch 26, batch 1/9:\tDiscriminator: real loss 0.16205282509326935, fake loss 0.18755872547626495\tGenerator: loss 0.8963578939437866\n",
            "Epoch 27, batch 1/9:\tDiscriminator: real loss 0.2008398473262787, fake loss 0.2087254375219345\tGenerator: loss 1.039595603942871\n",
            "Epoch 28, batch 1/9:\tDiscriminator: real loss 0.23316550254821777, fake loss 0.22039508819580078\tGenerator: loss 0.8292940855026245\n",
            "Epoch 29, batch 1/9:\tDiscriminator: real loss 0.2305746078491211, fake loss 0.17769446969032288\tGenerator: loss 0.9105734825134277\n",
            "Epoch 30, batch 1/9:\tDiscriminator: real loss 0.2135416865348816, fake loss 0.1792137622833252\tGenerator: loss 0.9665486216545105\n",
            "Epoch 31, batch 1/9:\tDiscriminator: real loss 0.1878083348274231, fake loss 0.15761858224868774\tGenerator: loss 0.9859481453895569\n",
            "Epoch 32, batch 1/9:\tDiscriminator: real loss 0.17873606085777283, fake loss 0.18122728168964386\tGenerator: loss 0.9704967737197876\n",
            "Epoch 33, batch 1/9:\tDiscriminator: real loss 0.21363264322280884, fake loss 0.20119649171829224\tGenerator: loss 0.9134478569030762\n",
            "Epoch 34, batch 1/9:\tDiscriminator: real loss 0.17964421212673187, fake loss 0.1438787430524826\tGenerator: loss 1.02713942527771\n",
            "Epoch 35, batch 1/9:\tDiscriminator: real loss 0.21405988931655884, fake loss 0.17131344974040985\tGenerator: loss 0.9686049818992615\n",
            "Epoch 36, batch 1/9:\tDiscriminator: real loss 0.17200107872486115, fake loss 0.14671659469604492\tGenerator: loss 1.093889832496643\n",
            "Epoch 37, batch 1/9:\tDiscriminator: real loss 0.17085769772529602, fake loss 0.1753358095884323\tGenerator: loss 1.005049467086792\n",
            "Epoch 38, batch 1/9:\tDiscriminator: real loss 0.1656382530927658, fake loss 0.168999582529068\tGenerator: loss 1.1100811958312988\n",
            "Epoch 39, batch 1/9:\tDiscriminator: real loss 0.15931327641010284, fake loss 0.15864667296409607\tGenerator: loss 1.138034462928772\n",
            "Epoch 40, batch 1/9:\tDiscriminator: real loss 0.15131567418575287, fake loss 0.15930649638175964\tGenerator: loss 1.0795636177062988\n",
            "Epoch 41, batch 1/9:\tDiscriminator: real loss 0.17058846354484558, fake loss 0.18250949680805206\tGenerator: loss 1.0008490085601807\n",
            "Epoch 42, batch 1/9:\tDiscriminator: real loss 0.16251350939273834, fake loss 0.242505744099617\tGenerator: loss 0.8612580895423889\n",
            "Epoch 43, batch 1/9:\tDiscriminator: real loss 0.19986467063426971, fake loss 0.1915004551410675\tGenerator: loss 0.9623773097991943\n",
            "Epoch 44, batch 1/9:\tDiscriminator: real loss 0.1990649402141571, fake loss 0.20556765794754028\tGenerator: loss 0.9684926867485046\n",
            "Epoch 45, batch 1/9:\tDiscriminator: real loss 0.23159295320510864, fake loss 0.1473655104637146\tGenerator: loss 1.1570581197738647\n",
            "Epoch 46, batch 1/9:\tDiscriminator: real loss 0.19927354156970978, fake loss 0.17027857899665833\tGenerator: loss 1.04582679271698\n",
            "Epoch 47, batch 1/9:\tDiscriminator: real loss 0.215699702501297, fake loss 0.19598336517810822\tGenerator: loss 0.9778512120246887\n",
            "Epoch 48, batch 1/9:\tDiscriminator: real loss 0.2226782590150833, fake loss 0.17715978622436523\tGenerator: loss 1.0732417106628418\n",
            "Epoch 49, batch 1/9:\tDiscriminator: real loss 0.1698668897151947, fake loss 0.18105727434158325\tGenerator: loss 0.9937554597854614\n",
            "Epoch 50, batch 1/9:\tDiscriminator: real loss 0.1447337120771408, fake loss 0.19632592797279358\tGenerator: loss 0.974543571472168\n",
            "Epoch 51, batch 1/9:\tDiscriminator: real loss 0.14004303514957428, fake loss 0.17759540677070618\tGenerator: loss 0.9467507600784302\n",
            "Epoch 52, batch 1/9:\tDiscriminator: real loss 0.15566913783550262, fake loss 0.20953211188316345\tGenerator: loss 0.9556989669799805\n",
            "Epoch 53, batch 1/9:\tDiscriminator: real loss 0.15416571497917175, fake loss 0.17910529673099518\tGenerator: loss 1.0155575275421143\n",
            "Epoch 54, batch 1/9:\tDiscriminator: real loss 0.16783995926380157, fake loss 0.18166281282901764\tGenerator: loss 1.0046135187149048\n",
            "Epoch 55, batch 1/9:\tDiscriminator: real loss 0.17232944071292877, fake loss 0.2330731451511383\tGenerator: loss 0.8210954666137695\n",
            "Epoch 56, batch 1/9:\tDiscriminator: real loss 0.19055788218975067, fake loss 0.18824800848960876\tGenerator: loss 1.0852162837982178\n",
            "Epoch 57, batch 1/9:\tDiscriminator: real loss 0.1511886715888977, fake loss 0.18206489086151123\tGenerator: loss 1.0555264949798584\n",
            "Epoch 58, batch 1/9:\tDiscriminator: real loss 0.1844787746667862, fake loss 0.2073853611946106\tGenerator: loss 0.9809515476226807\n",
            "Epoch 59, batch 1/9:\tDiscriminator: real loss 0.1649528294801712, fake loss 0.18656891584396362\tGenerator: loss 1.0658127069473267\n",
            "Epoch 60, batch 1/9:\tDiscriminator: real loss 0.18638092279434204, fake loss 0.168804332613945\tGenerator: loss 1.0334162712097168\n",
            "Epoch 61, batch 1/9:\tDiscriminator: real loss 0.20288115739822388, fake loss 0.20175427198410034\tGenerator: loss 0.9442847371101379\n",
            "Epoch 62, batch 1/9:\tDiscriminator: real loss 0.21171921491622925, fake loss 0.18634812533855438\tGenerator: loss 0.9657177925109863\n",
            "Epoch 63, batch 1/9:\tDiscriminator: real loss 0.19812840223312378, fake loss 0.20153987407684326\tGenerator: loss 0.9809714555740356\n",
            "Epoch 64, batch 1/9:\tDiscriminator: real loss 0.20642565190792084, fake loss 0.20577725768089294\tGenerator: loss 0.9341135025024414\n",
            "Epoch 65, batch 1/9:\tDiscriminator: real loss 0.20523612201213837, fake loss 0.19755704700946808\tGenerator: loss 0.9608277678489685\n",
            "Epoch 66, batch 1/9:\tDiscriminator: real loss 0.20954079926013947, fake loss 0.19897738099098206\tGenerator: loss 0.9849607348442078\n",
            "Epoch 67, batch 1/9:\tDiscriminator: real loss 0.20471978187561035, fake loss 0.17871874570846558\tGenerator: loss 1.0066434144973755\n",
            "Epoch 68, batch 1/9:\tDiscriminator: real loss 0.17086176574230194, fake loss 0.17137357592582703\tGenerator: loss 0.9628632068634033\n",
            "Epoch 69, batch 1/9:\tDiscriminator: real loss 0.18073934316635132, fake loss 0.17730991542339325\tGenerator: loss 0.9989640712738037\n",
            "Epoch 70, batch 1/9:\tDiscriminator: real loss 0.18869216740131378, fake loss 0.1811247169971466\tGenerator: loss 1.0336074829101562\n",
            "Epoch 71, batch 1/9:\tDiscriminator: real loss 0.12902939319610596, fake loss 0.15291380882263184\tGenerator: loss 1.0583410263061523\n",
            "Epoch 72, batch 1/9:\tDiscriminator: real loss 0.16414141654968262, fake loss 0.1647367626428604\tGenerator: loss 1.0746065378189087\n",
            "Epoch 73, batch 1/9:\tDiscriminator: real loss 0.1637527346611023, fake loss 0.17099304497241974\tGenerator: loss 1.0182043313980103\n",
            "Epoch 74, batch 1/9:\tDiscriminator: real loss 0.13422001898288727, fake loss 0.16074857115745544\tGenerator: loss 0.9737257361412048\n",
            "Epoch 75, batch 1/9:\tDiscriminator: real loss 0.18400773406028748, fake loss 0.19551429152488708\tGenerator: loss 0.9159426689147949\n",
            "Epoch 76, batch 1/9:\tDiscriminator: real loss 0.1634751260280609, fake loss 0.18431177735328674\tGenerator: loss 1.0235884189605713\n",
            "Epoch 77, batch 1/9:\tDiscriminator: real loss 0.16548387706279755, fake loss 0.19071412086486816\tGenerator: loss 0.9591645002365112\n",
            "Epoch 78, batch 1/9:\tDiscriminator: real loss 0.17798647284507751, fake loss 0.20511570572853088\tGenerator: loss 0.9315587878227234\n",
            "Epoch 79, batch 1/9:\tDiscriminator: real loss 0.22243835031986237, fake loss 0.174250990152359\tGenerator: loss 0.9659620523452759\n",
            "Epoch 80, batch 1/9:\tDiscriminator: real loss 0.22935348749160767, fake loss 0.19094941020011902\tGenerator: loss 0.9836437702178955\n",
            "Epoch 81, batch 1/9:\tDiscriminator: real loss 0.17211715877056122, fake loss 0.2054036259651184\tGenerator: loss 0.9767811298370361\n",
            "Epoch 82, batch 1/9:\tDiscriminator: real loss 0.23322179913520813, fake loss 0.17632004618644714\tGenerator: loss 1.0007884502410889\n",
            "Epoch 83, batch 1/9:\tDiscriminator: real loss 0.20869119465351105, fake loss 0.1889040619134903\tGenerator: loss 0.952688455581665\n",
            "Epoch 84, batch 1/9:\tDiscriminator: real loss 0.2531069219112396, fake loss 0.16750508546829224\tGenerator: loss 0.9891939759254456\n",
            "Epoch 85, batch 1/9:\tDiscriminator: real loss 0.206648051738739, fake loss 0.19098421931266785\tGenerator: loss 0.9420680999755859\n",
            "Epoch 86, batch 1/9:\tDiscriminator: real loss 0.24529887735843658, fake loss 0.20699089765548706\tGenerator: loss 0.9381844997406006\n",
            "Epoch 87, batch 1/9:\tDiscriminator: real loss 0.23299597203731537, fake loss 0.1746138334274292\tGenerator: loss 1.0001723766326904\n",
            "Epoch 88, batch 1/9:\tDiscriminator: real loss 0.2565245032310486, fake loss 0.1470799446105957\tGenerator: loss 1.0817954540252686\n",
            "Epoch 89, batch 1/9:\tDiscriminator: real loss 0.21620497107505798, fake loss 0.15098577737808228\tGenerator: loss 1.1553304195404053\n",
            "Epoch 90, batch 1/9:\tDiscriminator: real loss 0.17233899235725403, fake loss 0.1770971119403839\tGenerator: loss 1.1040441989898682\n",
            "Epoch 91, batch 1/9:\tDiscriminator: real loss 0.19524165987968445, fake loss 0.1666634976863861\tGenerator: loss 1.067234754562378\n",
            "Epoch 92, batch 1/9:\tDiscriminator: real loss 0.16279521584510803, fake loss 0.194885715842247\tGenerator: loss 1.1121644973754883\n",
            "Epoch 93, batch 1/9:\tDiscriminator: real loss 0.19013246893882751, fake loss 0.1924227476119995\tGenerator: loss 1.0515151023864746\n",
            "Epoch 94, batch 1/9:\tDiscriminator: real loss 0.17326119542121887, fake loss 0.22990287840366364\tGenerator: loss 0.9252114295959473\n",
            "Epoch 95, batch 1/9:\tDiscriminator: real loss 0.17092007398605347, fake loss 0.187312513589859\tGenerator: loss 0.928738534450531\n",
            "Epoch 96, batch 1/9:\tDiscriminator: real loss 0.18507146835327148, fake loss 0.20574259757995605\tGenerator: loss 0.9577103853225708\n",
            "Epoch 97, batch 1/9:\tDiscriminator: real loss 0.22399039566516876, fake loss 0.1772570013999939\tGenerator: loss 1.0389347076416016\n",
            "Epoch 98, batch 1/9:\tDiscriminator: real loss 0.16414391994476318, fake loss 0.1803274154663086\tGenerator: loss 1.0002790689468384\n",
            "Epoch 99, batch 1/9:\tDiscriminator: real loss 0.18442268669605255, fake loss 0.20338106155395508\tGenerator: loss 0.9535293579101562\n",
            "Epoch 100, batch 1/9:\tDiscriminator: real loss 0.1918344348669052, fake loss 0.17262208461761475\tGenerator: loss 0.9287183284759521\n",
            "Epoch 101, batch 1/9:\tDiscriminator: real loss 0.19769856333732605, fake loss 0.21637871861457825\tGenerator: loss 0.9366403222084045\n",
            "Epoch 102, batch 1/9:\tDiscriminator: real loss 0.1588858962059021, fake loss 0.19957278668880463\tGenerator: loss 0.9175548553466797\n",
            "Epoch 103, batch 1/9:\tDiscriminator: real loss 0.20558476448059082, fake loss 0.21828807890415192\tGenerator: loss 0.9445277452468872\n",
            "Epoch 104, batch 1/9:\tDiscriminator: real loss 0.19137272238731384, fake loss 0.2030477523803711\tGenerator: loss 0.9596933126449585\n",
            "Epoch 105, batch 1/9:\tDiscriminator: real loss 0.17805863916873932, fake loss 0.1722254902124405\tGenerator: loss 1.0517818927764893\n",
            "Epoch 106, batch 1/9:\tDiscriminator: real loss 0.17353063821792603, fake loss 0.19049853086471558\tGenerator: loss 1.1446691751480103\n",
            "Epoch 107, batch 1/9:\tDiscriminator: real loss 0.1648361086845398, fake loss 0.16792792081832886\tGenerator: loss 1.101606845855713\n",
            "Epoch 108, batch 1/9:\tDiscriminator: real loss 0.17413637042045593, fake loss 0.21038562059402466\tGenerator: loss 0.9229771494865417\n",
            "Epoch 109, batch 1/9:\tDiscriminator: real loss 0.18496575951576233, fake loss 0.2100863754749298\tGenerator: loss 1.0022526979446411\n",
            "Epoch 110, batch 1/9:\tDiscriminator: real loss 0.18518269062042236, fake loss 0.1785098910331726\tGenerator: loss 1.0484585762023926\n",
            "Epoch 111, batch 1/9:\tDiscriminator: real loss 0.19611498713493347, fake loss 0.18497443199157715\tGenerator: loss 0.9814183712005615\n",
            "Epoch 112, batch 1/9:\tDiscriminator: real loss 0.18049544095993042, fake loss 0.20698776841163635\tGenerator: loss 0.9880025386810303\n",
            "Epoch 113, batch 1/9:\tDiscriminator: real loss 0.19857704639434814, fake loss 0.18839021027088165\tGenerator: loss 0.9125343561172485\n",
            "Epoch 114, batch 1/9:\tDiscriminator: real loss 0.18935854732990265, fake loss 0.20514456927776337\tGenerator: loss 0.963849663734436\n",
            "Epoch 115, batch 1/9:\tDiscriminator: real loss 0.22910873591899872, fake loss 0.18676012754440308\tGenerator: loss 0.96783047914505\n",
            "Epoch 116, batch 1/9:\tDiscriminator: real loss 0.2162424921989441, fake loss 0.1562773734331131\tGenerator: loss 1.0059291124343872\n",
            "Epoch 117, batch 1/9:\tDiscriminator: real loss 0.2186998724937439, fake loss 0.21705584228038788\tGenerator: loss 0.9550330638885498\n",
            "Epoch 118, batch 1/9:\tDiscriminator: real loss 0.18730127811431885, fake loss 0.16670110821723938\tGenerator: loss 1.033857822418213\n",
            "Epoch 119, batch 1/9:\tDiscriminator: real loss 0.19158050417900085, fake loss 0.15431685745716095\tGenerator: loss 1.1972153186798096\n",
            "Epoch 120, batch 1/9:\tDiscriminator: real loss 0.16133256256580353, fake loss 0.12221455574035645\tGenerator: loss 1.162947416305542\n",
            "Epoch 121, batch 1/9:\tDiscriminator: real loss 0.16231626272201538, fake loss 0.1614244282245636\tGenerator: loss 1.0267088413238525\n",
            "Epoch 122, batch 1/9:\tDiscriminator: real loss 0.15732163190841675, fake loss 0.14663510024547577\tGenerator: loss 1.0756072998046875\n",
            "Epoch 123, batch 1/9:\tDiscriminator: real loss 0.15677420794963837, fake loss 0.17270109057426453\tGenerator: loss 1.0953810214996338\n",
            "Epoch 124, batch 1/9:\tDiscriminator: real loss 0.14593465626239777, fake loss 0.19480669498443604\tGenerator: loss 1.0158571004867554\n",
            "Epoch 125, batch 1/9:\tDiscriminator: real loss 0.1609952449798584, fake loss 0.15982955694198608\tGenerator: loss 1.0336322784423828\n",
            "Epoch 126, batch 1/9:\tDiscriminator: real loss 0.15391625463962555, fake loss 0.18985004723072052\tGenerator: loss 0.9836929440498352\n",
            "Epoch 127, batch 1/9:\tDiscriminator: real loss 0.16297265887260437, fake loss 0.20773929357528687\tGenerator: loss 0.9698810577392578\n",
            "Epoch 128, batch 1/9:\tDiscriminator: real loss 0.18895100057125092, fake loss 0.1848408281803131\tGenerator: loss 0.9046534299850464\n",
            "Epoch 129, batch 1/9:\tDiscriminator: real loss 0.2230411171913147, fake loss 0.21043483912944794\tGenerator: loss 0.8943041563034058\n",
            "Epoch 130, batch 1/9:\tDiscriminator: real loss 0.22001424431800842, fake loss 0.2007211148738861\tGenerator: loss 1.0020556449890137\n",
            "Epoch 131, batch 1/9:\tDiscriminator: real loss 0.22297613322734833, fake loss 0.20885665714740753\tGenerator: loss 0.903496265411377\n",
            "Epoch 132, batch 1/9:\tDiscriminator: real loss 0.22934411466121674, fake loss 0.18414908647537231\tGenerator: loss 0.9950309991836548\n",
            "Epoch 133, batch 1/9:\tDiscriminator: real loss 0.2217434197664261, fake loss 0.19623029232025146\tGenerator: loss 0.8878148794174194\n",
            "Epoch 134, batch 1/9:\tDiscriminator: real loss 0.18063175678253174, fake loss 0.16277411580085754\tGenerator: loss 1.0056588649749756\n",
            "Epoch 135, batch 1/9:\tDiscriminator: real loss 0.17991934716701508, fake loss 0.17841938138008118\tGenerator: loss 0.981282114982605\n",
            "Epoch 136, batch 1/9:\tDiscriminator: real loss 0.18116536736488342, fake loss 0.1790122240781784\tGenerator: loss 0.9655451774597168\n",
            "Epoch 137, batch 1/9:\tDiscriminator: real loss 0.19163256883621216, fake loss 0.18204551935195923\tGenerator: loss 0.95217365026474\n",
            "Epoch 138, batch 1/9:\tDiscriminator: real loss 0.16986671090126038, fake loss 0.17102375626564026\tGenerator: loss 0.9886431694030762\n",
            "Epoch 139, batch 1/9:\tDiscriminator: real loss 0.176835298538208, fake loss 0.18821346759796143\tGenerator: loss 1.0306837558746338\n",
            "Epoch 140, batch 1/9:\tDiscriminator: real loss 0.17696097493171692, fake loss 0.16147509217262268\tGenerator: loss 1.0173192024230957\n",
            "Epoch 141, batch 1/9:\tDiscriminator: real loss 0.16019263863563538, fake loss 0.19955232739448547\tGenerator: loss 1.0950742959976196\n",
            "Epoch 142, batch 1/9:\tDiscriminator: real loss 0.1532181203365326, fake loss 0.16168835759162903\tGenerator: loss 1.0819711685180664\n",
            "Epoch 143, batch 1/9:\tDiscriminator: real loss 0.15002429485321045, fake loss 0.17827549576759338\tGenerator: loss 1.0392178297042847\n",
            "Epoch 144, batch 1/9:\tDiscriminator: real loss 0.13680413365364075, fake loss 0.19339454174041748\tGenerator: loss 0.9869858622550964\n",
            "Epoch 145, batch 1/9:\tDiscriminator: real loss 0.17318588495254517, fake loss 0.19607466459274292\tGenerator: loss 1.004710078239441\n",
            "Epoch 146, batch 1/9:\tDiscriminator: real loss 0.1984696090221405, fake loss 0.20165003836154938\tGenerator: loss 0.9441416263580322\n",
            "Epoch 147, batch 1/9:\tDiscriminator: real loss 0.17582932114601135, fake loss 0.20390307903289795\tGenerator: loss 0.9826706647872925\n",
            "Epoch 148, batch 1/9:\tDiscriminator: real loss 0.21112176775932312, fake loss 0.17232702672481537\tGenerator: loss 1.0048959255218506\n",
            "Epoch 149, batch 1/9:\tDiscriminator: real loss 0.18655773997306824, fake loss 0.19786015152931213\tGenerator: loss 0.9577233791351318\n",
            "Epoch 150, batch 1/9:\tDiscriminator: real loss 0.19249585270881653, fake loss 0.17243625223636627\tGenerator: loss 1.1108735799789429\n",
            "Epoch 151, batch 1/9:\tDiscriminator: real loss 0.17612874507904053, fake loss 0.1561993956565857\tGenerator: loss 1.0748168230056763\n",
            "Epoch 152, batch 1/9:\tDiscriminator: real loss 0.14167019724845886, fake loss 0.14756900072097778\tGenerator: loss 1.1383562088012695\n",
            "Epoch 153, batch 1/9:\tDiscriminator: real loss 0.13192930817604065, fake loss 0.14922842383384705\tGenerator: loss 1.0795292854309082\n",
            "Epoch 154, batch 1/9:\tDiscriminator: real loss 0.14028087258338928, fake loss 0.1588587760925293\tGenerator: loss 1.0910418033599854\n",
            "Epoch 155, batch 1/9:\tDiscriminator: real loss 0.12936195731163025, fake loss 0.16422581672668457\tGenerator: loss 1.0591421127319336\n",
            "Epoch 156, batch 1/9:\tDiscriminator: real loss 0.15086880326271057, fake loss 0.14484772086143494\tGenerator: loss 1.1696752309799194\n",
            "Epoch 157, batch 1/9:\tDiscriminator: real loss 0.134465754032135, fake loss 0.14389225840568542\tGenerator: loss 1.1574828624725342\n",
            "Epoch 158, batch 1/9:\tDiscriminator: real loss 0.13960354030132294, fake loss 0.14094144105911255\tGenerator: loss 1.20988130569458\n",
            "Epoch 159, batch 1/9:\tDiscriminator: real loss 0.10367359220981598, fake loss 0.1455913484096527\tGenerator: loss 1.1812292337417603\n",
            "Epoch 160, batch 1/9:\tDiscriminator: real loss 0.15288671851158142, fake loss 0.1644916534423828\tGenerator: loss 1.022228717803955\n",
            "Epoch 161, batch 1/9:\tDiscriminator: real loss 0.2076723277568817, fake loss 0.2047259360551834\tGenerator: loss 1.1755454540252686\n",
            "Epoch 162, batch 1/9:\tDiscriminator: real loss 0.17876183986663818, fake loss 0.097765251994133\tGenerator: loss 1.4332469701766968\n",
            "Epoch 163, batch 1/9:\tDiscriminator: real loss 0.13226789236068726, fake loss 0.16262927651405334\tGenerator: loss 1.2280253171920776\n",
            "Epoch 164, batch 1/9:\tDiscriminator: real loss 0.1533024162054062, fake loss 0.2082735002040863\tGenerator: loss 0.9499292373657227\n",
            "Epoch 165, batch 1/9:\tDiscriminator: real loss 0.19561251997947693, fake loss 0.19075530767440796\tGenerator: loss 0.9754483103752136\n",
            "Epoch 166, batch 1/9:\tDiscriminator: real loss 0.15855848789215088, fake loss 0.14253130555152893\tGenerator: loss 1.0908342599868774\n",
            "Epoch 167, batch 1/9:\tDiscriminator: real loss 0.153715580701828, fake loss 0.17157351970672607\tGenerator: loss 1.0475988388061523\n",
            "Epoch 168, batch 1/9:\tDiscriminator: real loss 0.176313579082489, fake loss 0.21208035945892334\tGenerator: loss 0.9761167764663696\n",
            "Epoch 169, batch 1/9:\tDiscriminator: real loss 0.16268125176429749, fake loss 0.19125601649284363\tGenerator: loss 0.9951893091201782\n",
            "Epoch 170, batch 1/9:\tDiscriminator: real loss 0.19067031145095825, fake loss 0.21584531664848328\tGenerator: loss 0.9545982480049133\n",
            "Epoch 171, batch 1/9:\tDiscriminator: real loss 0.19750317931175232, fake loss 0.1782110631465912\tGenerator: loss 0.9723764061927795\n",
            "Epoch 172, batch 1/9:\tDiscriminator: real loss 0.18683141469955444, fake loss 0.16550759971141815\tGenerator: loss 1.0380477905273438\n",
            "Epoch 173, batch 1/9:\tDiscriminator: real loss 0.17712539434432983, fake loss 0.160362109541893\tGenerator: loss 1.0571925640106201\n",
            "Epoch 174, batch 1/9:\tDiscriminator: real loss 0.16784347593784332, fake loss 0.18511544167995453\tGenerator: loss 1.0522575378417969\n",
            "Epoch 175, batch 1/9:\tDiscriminator: real loss 0.1628747582435608, fake loss 0.21627256274223328\tGenerator: loss 0.985424280166626\n",
            "Epoch 176, batch 1/9:\tDiscriminator: real loss 0.18985076248645782, fake loss 0.16783404350280762\tGenerator: loss 1.0835649967193604\n",
            "Epoch 177, batch 1/9:\tDiscriminator: real loss 0.16776752471923828, fake loss 0.17935273051261902\tGenerator: loss 0.9898035526275635\n",
            "Epoch 178, batch 1/9:\tDiscriminator: real loss 0.18599861860275269, fake loss 0.17467504739761353\tGenerator: loss 1.0503826141357422\n",
            "Epoch 179, batch 1/9:\tDiscriminator: real loss 0.17041520774364471, fake loss 0.17974984645843506\tGenerator: loss 1.0512396097183228\n",
            "Epoch 180, batch 1/9:\tDiscriminator: real loss 0.1673901379108429, fake loss 0.16656242311000824\tGenerator: loss 1.0617049932479858\n",
            "Epoch 181, batch 1/9:\tDiscriminator: real loss 0.19521358609199524, fake loss 0.20379090309143066\tGenerator: loss 0.895841658115387\n",
            "Epoch 182, batch 1/9:\tDiscriminator: real loss 0.2043701410293579, fake loss 0.17372354865074158\tGenerator: loss 0.9474897384643555\n",
            "Epoch 183, batch 1/9:\tDiscriminator: real loss 0.16598621010780334, fake loss 0.18320399522781372\tGenerator: loss 0.986379086971283\n",
            "Epoch 184, batch 1/9:\tDiscriminator: real loss 0.1911175549030304, fake loss 0.22677387297153473\tGenerator: loss 0.9383544921875\n",
            "Epoch 185, batch 1/9:\tDiscriminator: real loss 0.18589946627616882, fake loss 0.18851667642593384\tGenerator: loss 0.9329856038093567\n",
            "Epoch 186, batch 1/9:\tDiscriminator: real loss 0.20605842769145966, fake loss 0.20919321477413177\tGenerator: loss 0.9456415176391602\n",
            "Epoch 187, batch 1/9:\tDiscriminator: real loss 0.17118610441684723, fake loss 0.18159392476081848\tGenerator: loss 1.063736915588379\n",
            "Epoch 188, batch 1/9:\tDiscriminator: real loss 0.16154992580413818, fake loss 0.2013780176639557\tGenerator: loss 0.956697940826416\n",
            "Epoch 189, batch 1/9:\tDiscriminator: real loss 0.20183414220809937, fake loss 0.19911730289459229\tGenerator: loss 1.0063551664352417\n",
            "Epoch 190, batch 1/9:\tDiscriminator: real loss 0.19516180455684662, fake loss 0.18306107819080353\tGenerator: loss 1.0444550514221191\n",
            "Epoch 191, batch 1/9:\tDiscriminator: real loss 0.17495524883270264, fake loss 0.20768743753433228\tGenerator: loss 0.9024547338485718\n",
            "Epoch 192, batch 1/9:\tDiscriminator: real loss 0.1926468312740326, fake loss 0.2117052525281906\tGenerator: loss 1.002020001411438\n",
            "Epoch 193, batch 1/9:\tDiscriminator: real loss 0.18935440480709076, fake loss 0.16823014616966248\tGenerator: loss 1.0555602312088013\n",
            "Epoch 194, batch 1/9:\tDiscriminator: real loss 0.16674664616584778, fake loss 0.17549370229244232\tGenerator: loss 1.0419189929962158\n",
            "Epoch 195, batch 1/9:\tDiscriminator: real loss 0.1785842776298523, fake loss 0.18101045489311218\tGenerator: loss 1.0306816101074219\n",
            "Epoch 196, batch 1/9:\tDiscriminator: real loss 0.1669192761182785, fake loss 0.18137942254543304\tGenerator: loss 0.9698526859283447\n",
            "Epoch 197, batch 1/9:\tDiscriminator: real loss 0.1918942928314209, fake loss 0.1643887162208557\tGenerator: loss 1.0469566583633423\n",
            "Epoch 198, batch 1/9:\tDiscriminator: real loss 0.18087685108184814, fake loss 0.16956502199172974\tGenerator: loss 0.9903931617736816\n",
            "Epoch 199, batch 1/9:\tDiscriminator: real loss 0.18963837623596191, fake loss 0.19377067685127258\tGenerator: loss 1.0343194007873535\n",
            "Epoch 200, batch 1/9:\tDiscriminator: real loss 0.1802297681570053, fake loss 0.17729030549526215\tGenerator: loss 0.9420013427734375\n",
            "Epoch 201, batch 1/9:\tDiscriminator: real loss 0.20552052557468414, fake loss 0.19547419250011444\tGenerator: loss 0.9844018220901489\n",
            "Epoch 202, batch 1/9:\tDiscriminator: real loss 0.19812095165252686, fake loss 0.1952294111251831\tGenerator: loss 0.9198286533355713\n",
            "Epoch 203, batch 1/9:\tDiscriminator: real loss 0.23127618432044983, fake loss 0.16647681593894958\tGenerator: loss 0.9732180833816528\n",
            "Epoch 204, batch 1/9:\tDiscriminator: real loss 0.20872533321380615, fake loss 0.18046507239341736\tGenerator: loss 0.9028581380844116\n",
            "Epoch 205, batch 1/9:\tDiscriminator: real loss 0.21756485104560852, fake loss 0.17529311776161194\tGenerator: loss 0.9506984353065491\n",
            "Epoch 206, batch 1/9:\tDiscriminator: real loss 0.2034492790699005, fake loss 0.14326420426368713\tGenerator: loss 1.0192418098449707\n",
            "Epoch 207, batch 1/9:\tDiscriminator: real loss 0.182322159409523, fake loss 0.15647578239440918\tGenerator: loss 0.9763480424880981\n",
            "Epoch 208, batch 1/9:\tDiscriminator: real loss 0.1794271171092987, fake loss 0.17725399136543274\tGenerator: loss 1.0071163177490234\n",
            "Epoch 209, batch 1/9:\tDiscriminator: real loss 0.19092199206352234, fake loss 0.17728249728679657\tGenerator: loss 0.9769428968429565\n",
            "Epoch 210, batch 1/9:\tDiscriminator: real loss 0.19652247428894043, fake loss 0.18584409356117249\tGenerator: loss 1.0052473545074463\n",
            "Epoch 211, batch 1/9:\tDiscriminator: real loss 0.1572258621454239, fake loss 0.16000619530677795\tGenerator: loss 0.9960814714431763\n",
            "Epoch 212, batch 1/9:\tDiscriminator: real loss 0.19173482060432434, fake loss 0.16694411635398865\tGenerator: loss 1.0354254245758057\n",
            "Epoch 213, batch 1/9:\tDiscriminator: real loss 0.16670666635036469, fake loss 0.1815166175365448\tGenerator: loss 1.0340983867645264\n",
            "Epoch 214, batch 1/9:\tDiscriminator: real loss 0.1340678632259369, fake loss 0.1474844217300415\tGenerator: loss 1.069757103919983\n",
            "Epoch 215, batch 1/9:\tDiscriminator: real loss 0.14504338800907135, fake loss 0.1916467845439911\tGenerator: loss 0.9947679042816162\n",
            "Epoch 216, batch 1/9:\tDiscriminator: real loss 0.16908755898475647, fake loss 0.18523380160331726\tGenerator: loss 0.939217209815979\n",
            "Epoch 217, batch 1/9:\tDiscriminator: real loss 0.1795402318239212, fake loss 0.15066608786582947\tGenerator: loss 1.0433156490325928\n",
            "Epoch 218, batch 1/9:\tDiscriminator: real loss 0.1648794412612915, fake loss 0.16774538159370422\tGenerator: loss 1.1198303699493408\n",
            "Epoch 219, batch 1/9:\tDiscriminator: real loss 0.17216117680072784, fake loss 0.1525038778781891\tGenerator: loss 1.0093176364898682\n",
            "Epoch 220, batch 1/9:\tDiscriminator: real loss 0.14746388792991638, fake loss 0.1783633828163147\tGenerator: loss 1.0123759508132935\n",
            "Epoch 221, batch 1/9:\tDiscriminator: real loss 0.1849774718284607, fake loss 0.18788835406303406\tGenerator: loss 0.9342454671859741\n",
            "Epoch 222, batch 1/9:\tDiscriminator: real loss 0.18154574930667877, fake loss 0.1755736917257309\tGenerator: loss 1.0449384450912476\n",
            "Epoch 223, batch 1/9:\tDiscriminator: real loss 0.1592588871717453, fake loss 0.17102321982383728\tGenerator: loss 1.0462101697921753\n",
            "Epoch 224, batch 1/9:\tDiscriminator: real loss 0.18513110280036926, fake loss 0.20132291316986084\tGenerator: loss 0.9865903854370117\n",
            "Epoch 225, batch 1/9:\tDiscriminator: real loss 0.19842270016670227, fake loss 0.1958025097846985\tGenerator: loss 0.9550858736038208\n",
            "Epoch 226, batch 1/9:\tDiscriminator: real loss 0.1994197517633438, fake loss 0.18188607692718506\tGenerator: loss 1.042697787284851\n",
            "Epoch 227, batch 1/9:\tDiscriminator: real loss 0.18536220490932465, fake loss 0.17443442344665527\tGenerator: loss 0.9383307695388794\n",
            "Epoch 228, batch 1/9:\tDiscriminator: real loss 0.1622137725353241, fake loss 0.1678996980190277\tGenerator: loss 1.050826072692871\n",
            "Epoch 229, batch 1/9:\tDiscriminator: real loss 0.16555383801460266, fake loss 0.14777648448944092\tGenerator: loss 1.1621860265731812\n",
            "Epoch 230, batch 1/9:\tDiscriminator: real loss 0.1508501172065735, fake loss 0.13242027163505554\tGenerator: loss 1.2255971431732178\n",
            "Epoch 231, batch 1/9:\tDiscriminator: real loss 0.13937221467494965, fake loss 0.12152157723903656\tGenerator: loss 1.1361684799194336\n",
            "Epoch 232, batch 1/9:\tDiscriminator: real loss 0.13817159831523895, fake loss 0.146528422832489\tGenerator: loss 1.1911418437957764\n",
            "Epoch 233, batch 1/9:\tDiscriminator: real loss 0.12199808657169342, fake loss 0.11653950810432434\tGenerator: loss 1.1430739164352417\n",
            "Epoch 234, batch 1/9:\tDiscriminator: real loss 0.1499442756175995, fake loss 0.22065305709838867\tGenerator: loss 1.0204885005950928\n",
            "Epoch 235, batch 1/9:\tDiscriminator: real loss 0.13864551484584808, fake loss 0.17373886704444885\tGenerator: loss 1.0496954917907715\n",
            "Epoch 236, batch 1/9:\tDiscriminator: real loss 0.16894377768039703, fake loss 0.1931268274784088\tGenerator: loss 1.0595483779907227\n",
            "Epoch 237, batch 1/9:\tDiscriminator: real loss 0.1882551908493042, fake loss 0.19291022419929504\tGenerator: loss 1.1287362575531006\n",
            "Epoch 238, batch 1/9:\tDiscriminator: real loss 0.1967902034521103, fake loss 0.19401675462722778\tGenerator: loss 1.074413776397705\n",
            "Epoch 239, batch 1/9:\tDiscriminator: real loss 0.20688049495220184, fake loss 0.16661423444747925\tGenerator: loss 1.0236849784851074\n",
            "Epoch 240, batch 1/9:\tDiscriminator: real loss 0.23662298917770386, fake loss 0.19554048776626587\tGenerator: loss 1.0706043243408203\n",
            "Epoch 241, batch 1/9:\tDiscriminator: real loss 0.16873113811016083, fake loss 0.14080464839935303\tGenerator: loss 1.0885348320007324\n",
            "Epoch 242, batch 1/9:\tDiscriminator: real loss 0.14637458324432373, fake loss 0.15575435757637024\tGenerator: loss 1.1801223754882812\n",
            "Epoch 243, batch 1/9:\tDiscriminator: real loss 0.1284506767988205, fake loss 0.2073996216058731\tGenerator: loss 0.9214334487915039\n",
            "Epoch 244, batch 1/9:\tDiscriminator: real loss 0.1578287035226822, fake loss 0.17463326454162598\tGenerator: loss 0.9822651147842407\n",
            "Epoch 245, batch 1/9:\tDiscriminator: real loss 0.207748144865036, fake loss 0.22715631127357483\tGenerator: loss 0.9112570881843567\n",
            "Epoch 246, batch 1/9:\tDiscriminator: real loss 0.23317450284957886, fake loss 0.1837759166955948\tGenerator: loss 1.0533794164657593\n",
            "Epoch 247, batch 1/9:\tDiscriminator: real loss 0.21624313294887543, fake loss 0.2000581920146942\tGenerator: loss 0.9267547726631165\n",
            "Epoch 248, batch 1/9:\tDiscriminator: real loss 0.24126604199409485, fake loss 0.22977037727832794\tGenerator: loss 0.9942430853843689\n",
            "Epoch 249, batch 1/9:\tDiscriminator: real loss 0.22785307466983795, fake loss 0.16583138704299927\tGenerator: loss 0.989882230758667\n",
            "Epoch 250, batch 1/9:\tDiscriminator: real loss 0.19218899309635162, fake loss 0.1810399889945984\tGenerator: loss 0.9470840692520142\n",
            "Epoch 251, batch 1/9:\tDiscriminator: real loss 0.20185774564743042, fake loss 0.18548433482646942\tGenerator: loss 0.9671703577041626\n",
            "Epoch 252, batch 1/9:\tDiscriminator: real loss 0.18619170784950256, fake loss 0.17790237069129944\tGenerator: loss 0.9686412215232849\n",
            "Epoch 253, batch 1/9:\tDiscriminator: real loss 0.1741105616092682, fake loss 0.1617579609155655\tGenerator: loss 1.0062434673309326\n",
            "Epoch 254, batch 1/9:\tDiscriminator: real loss 0.16087353229522705, fake loss 0.1677018404006958\tGenerator: loss 0.9440887570381165\n",
            "Epoch 255, batch 1/9:\tDiscriminator: real loss 0.17686858773231506, fake loss 0.18300768733024597\tGenerator: loss 0.9513123035430908\n",
            "Epoch 256, batch 1/9:\tDiscriminator: real loss 0.21486425399780273, fake loss 0.2320716232061386\tGenerator: loss 0.8940324783325195\n",
            "Epoch 257, batch 1/9:\tDiscriminator: real loss 0.25464507937431335, fake loss 0.20468465983867645\tGenerator: loss 0.9678106307983398\n",
            "Epoch 258, batch 1/9:\tDiscriminator: real loss 0.2244589626789093, fake loss 0.17071890830993652\tGenerator: loss 1.0134268999099731\n",
            "Epoch 259, batch 1/9:\tDiscriminator: real loss 0.22858424484729767, fake loss 0.16338106989860535\tGenerator: loss 0.9696067571640015\n",
            "Epoch 260, batch 1/9:\tDiscriminator: real loss 0.2101295292377472, fake loss 0.19057732820510864\tGenerator: loss 0.9636391401290894\n",
            "Epoch 261, batch 1/9:\tDiscriminator: real loss 0.17736905813217163, fake loss 0.15800072252750397\tGenerator: loss 0.9495011568069458\n",
            "Epoch 262, batch 1/9:\tDiscriminator: real loss 0.17786580324172974, fake loss 0.16637563705444336\tGenerator: loss 0.9670365452766418\n",
            "Epoch 263, batch 1/9:\tDiscriminator: real loss 0.17829203605651855, fake loss 0.1689278781414032\tGenerator: loss 0.9684405326843262\n",
            "Epoch 264, batch 1/9:\tDiscriminator: real loss 0.18678352236747742, fake loss 0.16524460911750793\tGenerator: loss 1.0279676914215088\n",
            "Epoch 265, batch 1/9:\tDiscriminator: real loss 0.1988665759563446, fake loss 0.18612229824066162\tGenerator: loss 1.0592150688171387\n",
            "Epoch 266, batch 1/9:\tDiscriminator: real loss 0.18299201130867004, fake loss 0.14620926976203918\tGenerator: loss 1.0306626558303833\n",
            "Epoch 267, batch 1/9:\tDiscriminator: real loss 0.17323535680770874, fake loss 0.1731225699186325\tGenerator: loss 1.0261642932891846\n",
            "Epoch 268, batch 1/9:\tDiscriminator: real loss 0.1742224097251892, fake loss 0.16602718830108643\tGenerator: loss 1.0671783685684204\n",
            "Epoch 269, batch 1/9:\tDiscriminator: real loss 0.16902489960193634, fake loss 0.15303190052509308\tGenerator: loss 1.1101367473602295\n",
            "Epoch 270, batch 1/9:\tDiscriminator: real loss 0.2035684585571289, fake loss 0.21332131326198578\tGenerator: loss 0.9349587559700012\n",
            "Epoch 271, batch 1/9:\tDiscriminator: real loss 0.16252341866493225, fake loss 0.17765358090400696\tGenerator: loss 1.015441656112671\n",
            "Epoch 272, batch 1/9:\tDiscriminator: real loss 0.1626013070344925, fake loss 0.16833940148353577\tGenerator: loss 1.120085597038269\n",
            "Epoch 273, batch 1/9:\tDiscriminator: real loss 0.15992426872253418, fake loss 0.1767658144235611\tGenerator: loss 1.0838050842285156\n",
            "Epoch 274, batch 1/9:\tDiscriminator: real loss 0.16724523901939392, fake loss 0.17771965265274048\tGenerator: loss 1.0677530765533447\n",
            "Epoch 275, batch 1/9:\tDiscriminator: real loss 0.1538390815258026, fake loss 0.2085457742214203\tGenerator: loss 0.9276583194732666\n",
            "Epoch 276, batch 1/9:\tDiscriminator: real loss 0.1701941043138504, fake loss 0.18977349996566772\tGenerator: loss 0.9746642112731934\n",
            "Epoch 277, batch 1/9:\tDiscriminator: real loss 0.17394667863845825, fake loss 0.19731736183166504\tGenerator: loss 1.0724635124206543\n",
            "Epoch 278, batch 1/9:\tDiscriminator: real loss 0.14176009595394135, fake loss 0.1924668699502945\tGenerator: loss 1.06510591506958\n",
            "Epoch 279, batch 1/9:\tDiscriminator: real loss 0.1989063322544098, fake loss 0.18129539489746094\tGenerator: loss 1.0110256671905518\n",
            "Epoch 280, batch 1/9:\tDiscriminator: real loss 0.17124998569488525, fake loss 0.18569844961166382\tGenerator: loss 0.96656733751297\n",
            "Epoch 281, batch 1/9:\tDiscriminator: real loss 0.1864415854215622, fake loss 0.1508835256099701\tGenerator: loss 0.9752465486526489\n",
            "Epoch 282, batch 1/9:\tDiscriminator: real loss 0.17410948872566223, fake loss 0.1717630922794342\tGenerator: loss 1.0479505062103271\n",
            "Epoch 283, batch 1/9:\tDiscriminator: real loss 0.15816138684749603, fake loss 0.1727866232395172\tGenerator: loss 0.9796696901321411\n",
            "Epoch 284, batch 1/9:\tDiscriminator: real loss 0.15864473581314087, fake loss 0.19898387789726257\tGenerator: loss 0.9582449793815613\n",
            "Epoch 285, batch 1/9:\tDiscriminator: real loss 0.15968404710292816, fake loss 0.16152486205101013\tGenerator: loss 0.9721318483352661\n",
            "Epoch 286, batch 1/9:\tDiscriminator: real loss 0.20048701763153076, fake loss 0.16197150945663452\tGenerator: loss 1.0349299907684326\n",
            "Epoch 287, batch 1/9:\tDiscriminator: real loss 0.1830543577671051, fake loss 0.17366957664489746\tGenerator: loss 0.97364342212677\n",
            "Epoch 288, batch 1/9:\tDiscriminator: real loss 0.19692745804786682, fake loss 0.133495032787323\tGenerator: loss 1.0357303619384766\n",
            "Epoch 289, batch 1/9:\tDiscriminator: real loss 0.16036918759346008, fake loss 0.19120094180107117\tGenerator: loss 0.989209771156311\n",
            "Epoch 290, batch 1/9:\tDiscriminator: real loss 0.1532958298921585, fake loss 0.16101998090744019\tGenerator: loss 1.0697118043899536\n",
            "Epoch 291, batch 1/9:\tDiscriminator: real loss 0.15850377082824707, fake loss 0.15925240516662598\tGenerator: loss 1.0730751752853394\n",
            "Epoch 292, batch 1/9:\tDiscriminator: real loss 0.16573454439640045, fake loss 0.18846404552459717\tGenerator: loss 1.0017871856689453\n",
            "Epoch 293, batch 1/9:\tDiscriminator: real loss 0.17680901288986206, fake loss 0.20699477195739746\tGenerator: loss 0.9907197952270508\n",
            "Epoch 294, batch 1/9:\tDiscriminator: real loss 0.17848645150661469, fake loss 0.19421321153640747\tGenerator: loss 1.0125772953033447\n",
            "Epoch 295, batch 1/9:\tDiscriminator: real loss 0.18400515615940094, fake loss 0.16105154156684875\tGenerator: loss 1.059269666671753\n",
            "Epoch 296, batch 1/9:\tDiscriminator: real loss 0.19447724521160126, fake loss 0.19852247834205627\tGenerator: loss 0.9756184220314026\n",
            "Epoch 297, batch 1/9:\tDiscriminator: real loss 0.2030005007982254, fake loss 0.16021828353405\tGenerator: loss 1.087437391281128\n",
            "Epoch 298, batch 1/9:\tDiscriminator: real loss 0.1498580276966095, fake loss 0.15607810020446777\tGenerator: loss 1.112942099571228\n",
            "Epoch 299, batch 1/9:\tDiscriminator: real loss 0.1447192132472992, fake loss 0.1537857949733734\tGenerator: loss 1.1697988510131836\n",
            "Epoch 300, batch 1/9:\tDiscriminator: real loss 0.13150346279144287, fake loss 0.1475265622138977\tGenerator: loss 1.217286229133606\n",
            "Epoch 301, batch 1/9:\tDiscriminator: real loss 0.16505125164985657, fake loss 0.2731393575668335\tGenerator: loss 0.8625457882881165\n",
            "Epoch 302, batch 1/9:\tDiscriminator: real loss 0.20035597681999207, fake loss 0.2033078819513321\tGenerator: loss 1.089205265045166\n",
            "Epoch 303, batch 1/9:\tDiscriminator: real loss 0.215244323015213, fake loss 0.17918050289154053\tGenerator: loss 1.0597822666168213\n",
            "Epoch 304, batch 1/9:\tDiscriminator: real loss 0.1598963439464569, fake loss 0.19318974018096924\tGenerator: loss 0.9714020490646362\n",
            "Epoch 305, batch 1/9:\tDiscriminator: real loss 0.20061443746089935, fake loss 0.1664564460515976\tGenerator: loss 1.0904933214187622\n",
            "Epoch 306, batch 1/9:\tDiscriminator: real loss 0.20883113145828247, fake loss 0.1852293461561203\tGenerator: loss 1.074570655822754\n",
            "Epoch 307, batch 1/9:\tDiscriminator: real loss 0.17828837037086487, fake loss 0.16873085498809814\tGenerator: loss 1.1332106590270996\n",
            "Epoch 308, batch 1/9:\tDiscriminator: real loss 0.21914225816726685, fake loss 0.17762130498886108\tGenerator: loss 1.0525753498077393\n",
            "Epoch 309, batch 1/9:\tDiscriminator: real loss 0.18566548824310303, fake loss 0.21386238932609558\tGenerator: loss 0.9175394773483276\n",
            "Epoch 310, batch 1/9:\tDiscriminator: real loss 0.15492698550224304, fake loss 0.18530097603797913\tGenerator: loss 1.0799181461334229\n",
            "Epoch 311, batch 1/9:\tDiscriminator: real loss 0.15283262729644775, fake loss 0.15364259481430054\tGenerator: loss 1.1131830215454102\n",
            "Epoch 312, batch 1/9:\tDiscriminator: real loss 0.16236604750156403, fake loss 0.15555942058563232\tGenerator: loss 1.0804502964019775\n",
            "Epoch 313, batch 1/9:\tDiscriminator: real loss 0.1647191047668457, fake loss 0.17098164558410645\tGenerator: loss 1.0884438753128052\n",
            "Epoch 314, batch 1/9:\tDiscriminator: real loss 0.13767185807228088, fake loss 0.16089138388633728\tGenerator: loss 1.2088276147842407\n",
            "Epoch 315, batch 1/9:\tDiscriminator: real loss 0.14451849460601807, fake loss 0.16260814666748047\tGenerator: loss 1.1065669059753418\n",
            "Epoch 316, batch 1/9:\tDiscriminator: real loss 0.22240810096263885, fake loss 0.18018841743469238\tGenerator: loss 1.2684074640274048\n",
            "Epoch 317, batch 1/9:\tDiscriminator: real loss 0.17439615726470947, fake loss 0.1289970576763153\tGenerator: loss 1.3065645694732666\n",
            "Epoch 318, batch 1/9:\tDiscriminator: real loss 0.14908897876739502, fake loss 0.13639163970947266\tGenerator: loss 1.2232203483581543\n",
            "Epoch 319, batch 1/9:\tDiscriminator: real loss 0.13086023926734924, fake loss 0.17424973845481873\tGenerator: loss 1.1107956171035767\n",
            "Epoch 320, batch 1/9:\tDiscriminator: real loss 0.18236583471298218, fake loss 0.20702604949474335\tGenerator: loss 0.9441862106323242\n",
            "Epoch 321, batch 1/9:\tDiscriminator: real loss 0.19580000638961792, fake loss 0.19790641963481903\tGenerator: loss 0.9795289039611816\n",
            "Epoch 322, batch 1/9:\tDiscriminator: real loss 0.20122353732585907, fake loss 0.21226119995117188\tGenerator: loss 0.940503716468811\n",
            "Epoch 323, batch 1/9:\tDiscriminator: real loss 0.190515398979187, fake loss 0.18611086905002594\tGenerator: loss 0.995263934135437\n",
            "Epoch 324, batch 1/9:\tDiscriminator: real loss 0.1844460815191269, fake loss 0.19040751457214355\tGenerator: loss 1.0314819812774658\n",
            "Epoch 325, batch 1/9:\tDiscriminator: real loss 0.18788491189479828, fake loss 0.1518222689628601\tGenerator: loss 1.0095444917678833\n",
            "Epoch 326, batch 1/9:\tDiscriminator: real loss 0.18010590970516205, fake loss 0.16327273845672607\tGenerator: loss 1.104682207107544\n",
            "Epoch 327, batch 1/9:\tDiscriminator: real loss 0.16967108845710754, fake loss 0.1462971270084381\tGenerator: loss 1.178526759147644\n",
            "Epoch 328, batch 1/9:\tDiscriminator: real loss 0.13598115742206573, fake loss 0.1181701049208641\tGenerator: loss 1.1143392324447632\n",
            "Epoch 329, batch 1/9:\tDiscriminator: real loss 0.14668087661266327, fake loss 0.25831732153892517\tGenerator: loss 0.8788557052612305\n",
            "Epoch 330, batch 1/9:\tDiscriminator: real loss 0.24581588804721832, fake loss 0.2038150131702423\tGenerator: loss 0.9528785347938538\n",
            "Epoch 331, batch 1/9:\tDiscriminator: real loss 0.20699593424797058, fake loss 0.20823004841804504\tGenerator: loss 1.0266778469085693\n",
            "Epoch 332, batch 1/9:\tDiscriminator: real loss 0.20699664950370789, fake loss 0.1701928675174713\tGenerator: loss 0.9957400560379028\n",
            "Epoch 333, batch 1/9:\tDiscriminator: real loss 0.18593868613243103, fake loss 0.14937734603881836\tGenerator: loss 1.0460443496704102\n",
            "Epoch 334, batch 1/9:\tDiscriminator: real loss 0.15529868006706238, fake loss 0.15540891885757446\tGenerator: loss 1.053796648979187\n",
            "Epoch 335, batch 1/9:\tDiscriminator: real loss 0.15521010756492615, fake loss 0.16720375418663025\tGenerator: loss 1.062743902206421\n",
            "Epoch 336, batch 1/9:\tDiscriminator: real loss 0.13911545276641846, fake loss 0.1610335111618042\tGenerator: loss 1.0533726215362549\n",
            "Epoch 337, batch 1/9:\tDiscriminator: real loss 0.16043376922607422, fake loss 0.2051355242729187\tGenerator: loss 0.9440346956253052\n",
            "Epoch 338, batch 1/9:\tDiscriminator: real loss 0.16957597434520721, fake loss 0.16089265048503876\tGenerator: loss 1.192866563796997\n",
            "Epoch 339, batch 1/9:\tDiscriminator: real loss 0.15306997299194336, fake loss 0.1611056625843048\tGenerator: loss 1.1050543785095215\n",
            "Epoch 340, batch 1/9:\tDiscriminator: real loss 0.16128146648406982, fake loss 0.16154731810092926\tGenerator: loss 1.0406147241592407\n",
            "Epoch 341, batch 1/9:\tDiscriminator: real loss 0.14110895991325378, fake loss 0.18795272707939148\tGenerator: loss 1.0622529983520508\n",
            "Epoch 342, batch 1/9:\tDiscriminator: real loss 0.15382175147533417, fake loss 0.20466932654380798\tGenerator: loss 1.0675101280212402\n",
            "Epoch 343, batch 1/9:\tDiscriminator: real loss 0.14866766333580017, fake loss 0.20563240349292755\tGenerator: loss 0.9847893714904785\n",
            "Epoch 344, batch 1/9:\tDiscriminator: real loss 0.16755127906799316, fake loss 0.17729875445365906\tGenerator: loss 1.10627019405365\n",
            "Epoch 345, batch 1/9:\tDiscriminator: real loss 0.17724432051181793, fake loss 0.15178538858890533\tGenerator: loss 1.0841822624206543\n",
            "Epoch 346, batch 1/9:\tDiscriminator: real loss 0.1316269487142563, fake loss 0.1590246856212616\tGenerator: loss 1.1655986309051514\n",
            "Epoch 347, batch 1/9:\tDiscriminator: real loss 0.12259133160114288, fake loss 0.13968943059444427\tGenerator: loss 1.206498146057129\n",
            "Epoch 348, batch 1/9:\tDiscriminator: real loss 0.1216980442404747, fake loss 0.15657205879688263\tGenerator: loss 1.1491472721099854\n",
            "Epoch 349, batch 1/9:\tDiscriminator: real loss 0.13878872990608215, fake loss 0.12745292484760284\tGenerator: loss 1.227219820022583\n",
            "Epoch 350, batch 1/9:\tDiscriminator: real loss 0.12608996033668518, fake loss 0.16394296288490295\tGenerator: loss 1.0920627117156982\n",
            "Epoch 351, batch 1/9:\tDiscriminator: real loss 0.1173640638589859, fake loss 0.14087490737438202\tGenerator: loss 1.15664541721344\n",
            "Epoch 352, batch 1/9:\tDiscriminator: real loss 0.1921834945678711, fake loss 0.18265871703624725\tGenerator: loss 1.2264479398727417\n",
            "Epoch 353, batch 1/9:\tDiscriminator: real loss 0.21563835442066193, fake loss 0.194501131772995\tGenerator: loss 1.1054580211639404\n",
            "Epoch 354, batch 1/9:\tDiscriminator: real loss 0.23388633131980896, fake loss 0.24745407700538635\tGenerator: loss 0.9754360914230347\n",
            "Epoch 355, batch 1/9:\tDiscriminator: real loss 0.3366856575012207, fake loss 0.1626802682876587\tGenerator: loss 1.1164658069610596\n",
            "Epoch 356, batch 1/9:\tDiscriminator: real loss 0.2627648711204529, fake loss 0.16846276819705963\tGenerator: loss 1.0611546039581299\n",
            "Epoch 357, batch 1/9:\tDiscriminator: real loss 0.22580790519714355, fake loss 0.15045560896396637\tGenerator: loss 1.1027584075927734\n",
            "Epoch 358, batch 1/9:\tDiscriminator: real loss 0.20099210739135742, fake loss 0.14352881908416748\tGenerator: loss 1.1191598176956177\n",
            "Epoch 359, batch 1/9:\tDiscriminator: real loss 0.16075372695922852, fake loss 0.16642290353775024\tGenerator: loss 1.0278096199035645\n",
            "Epoch 360, batch 1/9:\tDiscriminator: real loss 0.14569266140460968, fake loss 0.17386072874069214\tGenerator: loss 1.150153398513794\n",
            "Epoch 361, batch 1/9:\tDiscriminator: real loss 0.15011155605316162, fake loss 0.1644822657108307\tGenerator: loss 1.0728366374969482\n",
            "Epoch 362, batch 1/9:\tDiscriminator: real loss 0.14473019540309906, fake loss 0.1777346134185791\tGenerator: loss 1.066295862197876\n",
            "Epoch 363, batch 1/9:\tDiscriminator: real loss 0.15147846937179565, fake loss 0.13819637894630432\tGenerator: loss 1.083349585533142\n",
            "Epoch 364, batch 1/9:\tDiscriminator: real loss 0.15013974905014038, fake loss 0.18633106350898743\tGenerator: loss 1.159681797027588\n",
            "Epoch 365, batch 1/9:\tDiscriminator: real loss 0.16029933094978333, fake loss 0.1491699367761612\tGenerator: loss 1.1847741603851318\n",
            "Epoch 366, batch 1/9:\tDiscriminator: real loss 0.15549978613853455, fake loss 0.15017306804656982\tGenerator: loss 1.113168478012085\n",
            "Epoch 367, batch 1/9:\tDiscriminator: real loss 0.17625577747821808, fake loss 0.1512223184108734\tGenerator: loss 1.03629469871521\n",
            "Epoch 368, batch 1/9:\tDiscriminator: real loss 0.16885784268379211, fake loss 0.17581583559513092\tGenerator: loss 1.0602096319198608\n",
            "Epoch 369, batch 1/9:\tDiscriminator: real loss 0.19309435784816742, fake loss 0.16071996092796326\tGenerator: loss 1.0928542613983154\n",
            "Epoch 370, batch 1/9:\tDiscriminator: real loss 0.1504271924495697, fake loss 0.19112935662269592\tGenerator: loss 1.0064475536346436\n",
            "Epoch 371, batch 1/9:\tDiscriminator: real loss 0.17713268101215363, fake loss 0.16236340999603271\tGenerator: loss 1.144556999206543\n",
            "Epoch 372, batch 1/9:\tDiscriminator: real loss 0.14987586438655853, fake loss 0.22380365431308746\tGenerator: loss 0.9590114951133728\n",
            "Epoch 373, batch 1/9:\tDiscriminator: real loss 0.23429708182811737, fake loss 0.14019429683685303\tGenerator: loss 1.1427505016326904\n",
            "Epoch 374, batch 1/9:\tDiscriminator: real loss 0.17137473821640015, fake loss 0.10684628784656525\tGenerator: loss 1.11557137966156\n",
            "Epoch 375, batch 1/9:\tDiscriminator: real loss 0.1814844012260437, fake loss 0.19806678593158722\tGenerator: loss 1.1095390319824219\n",
            "Epoch 376, batch 1/9:\tDiscriminator: real loss 0.15662439167499542, fake loss 0.1808350384235382\tGenerator: loss 0.9955815672874451\n",
            "Epoch 377, batch 1/9:\tDiscriminator: real loss 0.18682637810707092, fake loss 0.1911121904850006\tGenerator: loss 0.9603940844535828\n",
            "Epoch 378, batch 1/9:\tDiscriminator: real loss 0.1739499866962433, fake loss 0.18479278683662415\tGenerator: loss 0.9939759373664856\n",
            "Epoch 379, batch 1/9:\tDiscriminator: real loss 0.18022331595420837, fake loss 0.1893782615661621\tGenerator: loss 0.9882364869117737\n",
            "Epoch 380, batch 1/9:\tDiscriminator: real loss 0.23701739311218262, fake loss 0.1431748867034912\tGenerator: loss 1.1239831447601318\n",
            "Epoch 381, batch 1/9:\tDiscriminator: real loss 0.17789548635482788, fake loss 0.1566140055656433\tGenerator: loss 1.076434850692749\n",
            "Epoch 382, batch 1/9:\tDiscriminator: real loss 0.18454764783382416, fake loss 0.13480637967586517\tGenerator: loss 1.0466363430023193\n",
            "Epoch 383, batch 1/9:\tDiscriminator: real loss 0.1822878122329712, fake loss 0.13114722073078156\tGenerator: loss 1.1563074588775635\n",
            "Epoch 384, batch 1/9:\tDiscriminator: real loss 0.1479395478963852, fake loss 0.12884438037872314\tGenerator: loss 1.2152090072631836\n",
            "Epoch 385, batch 1/9:\tDiscriminator: real loss 0.17214898765087128, fake loss 0.1787661612033844\tGenerator: loss 1.0273442268371582\n",
            "Epoch 386, batch 1/9:\tDiscriminator: real loss 0.1565127670764923, fake loss 0.18375539779663086\tGenerator: loss 1.089327335357666\n",
            "Epoch 387, batch 1/9:\tDiscriminator: real loss 0.23050245642662048, fake loss 0.21038243174552917\tGenerator: loss 0.9959080219268799\n",
            "Epoch 388, batch 1/9:\tDiscriminator: real loss 0.20023402571678162, fake loss 0.14885461330413818\tGenerator: loss 1.1311252117156982\n",
            "Epoch 389, batch 1/9:\tDiscriminator: real loss 0.1816653162240982, fake loss 0.15198692679405212\tGenerator: loss 1.1283478736877441\n",
            "Epoch 390, batch 1/9:\tDiscriminator: real loss 0.17210917174816132, fake loss 0.1675199568271637\tGenerator: loss 1.05372953414917\n",
            "Epoch 391, batch 1/9:\tDiscriminator: real loss 0.18091562390327454, fake loss 0.18438231945037842\tGenerator: loss 1.0030395984649658\n",
            "Epoch 392, batch 1/9:\tDiscriminator: real loss 0.1898978352546692, fake loss 0.20170947909355164\tGenerator: loss 0.9339796900749207\n",
            "Epoch 393, batch 1/9:\tDiscriminator: real loss 0.233790785074234, fake loss 0.27241232991218567\tGenerator: loss 0.8256180286407471\n",
            "Epoch 394, batch 1/9:\tDiscriminator: real loss 0.2545841932296753, fake loss 0.17006860673427582\tGenerator: loss 1.0714492797851562\n",
            "Epoch 395, batch 1/9:\tDiscriminator: real loss 0.2551761865615845, fake loss 0.17496582865715027\tGenerator: loss 1.0740246772766113\n",
            "Epoch 396, batch 1/9:\tDiscriminator: real loss 0.18947601318359375, fake loss 0.19439324736595154\tGenerator: loss 0.9837284088134766\n",
            "Epoch 397, batch 1/9:\tDiscriminator: real loss 0.1988178938627243, fake loss 0.16789209842681885\tGenerator: loss 1.0450148582458496\n",
            "Epoch 398, batch 1/9:\tDiscriminator: real loss 0.18807823956012726, fake loss 0.17625264823436737\tGenerator: loss 0.9703030586242676\n",
            "Epoch 399, batch 1/9:\tDiscriminator: real loss 0.2060910165309906, fake loss 0.2371666133403778\tGenerator: loss 0.8994947075843811\n",
            "Epoch 400, batch 1/9:\tDiscriminator: real loss 0.19701945781707764, fake loss 0.19832801818847656\tGenerator: loss 0.9767740964889526\n",
            "Epoch 401, batch 1/9:\tDiscriminator: real loss 0.19240601360797882, fake loss 0.17456161975860596\tGenerator: loss 1.0131652355194092\n",
            "Epoch 402, batch 1/9:\tDiscriminator: real loss 0.19931046664714813, fake loss 0.20702695846557617\tGenerator: loss 1.0147652626037598\n",
            "Epoch 403, batch 1/9:\tDiscriminator: real loss 0.17956337332725525, fake loss 0.19331613183021545\tGenerator: loss 0.9911154508590698\n",
            "Epoch 404, batch 1/9:\tDiscriminator: real loss 0.17645138502120972, fake loss 0.1686728298664093\tGenerator: loss 1.0154725313186646\n",
            "Epoch 405, batch 1/9:\tDiscriminator: real loss 0.1619431972503662, fake loss 0.2021157443523407\tGenerator: loss 0.9827576875686646\n",
            "Epoch 406, batch 1/9:\tDiscriminator: real loss 0.1635710895061493, fake loss 0.1924111247062683\tGenerator: loss 1.008236050605774\n",
            "Epoch 407, batch 1/9:\tDiscriminator: real loss 0.17210033535957336, fake loss 0.1805163025856018\tGenerator: loss 0.9985620379447937\n",
            "Epoch 408, batch 1/9:\tDiscriminator: real loss 0.17947492003440857, fake loss 0.16888423264026642\tGenerator: loss 0.9453614354133606\n",
            "Epoch 409, batch 1/9:\tDiscriminator: real loss 0.17025639116764069, fake loss 0.17303158342838287\tGenerator: loss 0.9649529457092285\n",
            "Epoch 410, batch 1/9:\tDiscriminator: real loss 0.17344634234905243, fake loss 0.19705542922019958\tGenerator: loss 1.0195224285125732\n",
            "Epoch 411, batch 1/9:\tDiscriminator: real loss 0.1644217073917389, fake loss 0.18876704573631287\tGenerator: loss 0.9745191335678101\n",
            "Epoch 412, batch 1/9:\tDiscriminator: real loss 0.17354503273963928, fake loss 0.19198328256607056\tGenerator: loss 0.9942646622657776\n",
            "Epoch 413, batch 1/9:\tDiscriminator: real loss 0.1572519838809967, fake loss 0.194013774394989\tGenerator: loss 1.0111074447631836\n",
            "Epoch 414, batch 1/9:\tDiscriminator: real loss 0.1481582224369049, fake loss 0.19800496101379395\tGenerator: loss 0.992928147315979\n",
            "Epoch 415, batch 1/9:\tDiscriminator: real loss 0.2080584168434143, fake loss 0.2091974914073944\tGenerator: loss 1.0677075386047363\n",
            "Epoch 416, batch 1/9:\tDiscriminator: real loss 0.21900206804275513, fake loss 0.22149290144443512\tGenerator: loss 1.085358738899231\n",
            "Epoch 417, batch 1/9:\tDiscriminator: real loss 0.1761578917503357, fake loss 0.19523769617080688\tGenerator: loss 1.1095483303070068\n",
            "Epoch 418, batch 1/9:\tDiscriminator: real loss 0.20893317461013794, fake loss 0.1970827877521515\tGenerator: loss 0.9765169024467468\n",
            "Epoch 419, batch 1/9:\tDiscriminator: real loss 0.21013489365577698, fake loss 0.1847098469734192\tGenerator: loss 1.1610887050628662\n",
            "Epoch 420, batch 1/9:\tDiscriminator: real loss 0.20985907316207886, fake loss 0.15480738878250122\tGenerator: loss 1.078963041305542\n",
            "Epoch 421, batch 1/9:\tDiscriminator: real loss 0.21642859280109406, fake loss 0.1818751096725464\tGenerator: loss 1.2857518196105957\n",
            "Epoch 422, batch 1/9:\tDiscriminator: real loss 0.2039467990398407, fake loss 0.14515244960784912\tGenerator: loss 1.222545862197876\n",
            "Epoch 423, batch 1/9:\tDiscriminator: real loss 0.15095371007919312, fake loss 0.16005583107471466\tGenerator: loss 1.2402207851409912\n",
            "Epoch 424, batch 1/9:\tDiscriminator: real loss 0.16333109140396118, fake loss 0.16179317235946655\tGenerator: loss 1.1836973428726196\n",
            "Epoch 425, batch 1/9:\tDiscriminator: real loss 0.15400904417037964, fake loss 0.12883973121643066\tGenerator: loss 1.1705067157745361\n",
            "Epoch 426, batch 1/9:\tDiscriminator: real loss 0.11876174807548523, fake loss 0.1653183400630951\tGenerator: loss 1.1708650588989258\n",
            "Epoch 427, batch 1/9:\tDiscriminator: real loss 0.15224528312683105, fake loss 0.18627972900867462\tGenerator: loss 1.1972630023956299\n",
            "Epoch 428, batch 1/9:\tDiscriminator: real loss 0.19078022241592407, fake loss 0.18823815882205963\tGenerator: loss 1.1589388847351074\n",
            "Epoch 429, batch 1/9:\tDiscriminator: real loss 0.17861978709697723, fake loss 0.18360598385334015\tGenerator: loss 1.0349019765853882\n",
            "Epoch 430, batch 1/9:\tDiscriminator: real loss 0.17135025560855865, fake loss 0.16793331503868103\tGenerator: loss 1.0972703695297241\n",
            "Epoch 431, batch 1/9:\tDiscriminator: real loss 0.19334334135055542, fake loss 0.17684218287467957\tGenerator: loss 1.0155632495880127\n",
            "Epoch 432, batch 1/9:\tDiscriminator: real loss 0.21318721771240234, fake loss 0.17055757343769073\tGenerator: loss 1.0454654693603516\n",
            "Epoch 433, batch 1/9:\tDiscriminator: real loss 0.19432461261749268, fake loss 0.15755745768547058\tGenerator: loss 1.0171805620193481\n",
            "Epoch 434, batch 1/9:\tDiscriminator: real loss 0.18165360391139984, fake loss 0.1616382896900177\tGenerator: loss 1.0038294792175293\n",
            "Epoch 435, batch 1/9:\tDiscriminator: real loss 0.19819249212741852, fake loss 0.20555633306503296\tGenerator: loss 0.9539194107055664\n",
            "Epoch 436, batch 1/9:\tDiscriminator: real loss 0.26372408866882324, fake loss 0.20159685611724854\tGenerator: loss 1.1424369812011719\n",
            "Epoch 437, batch 1/9:\tDiscriminator: real loss 0.2408880889415741, fake loss 0.19470158219337463\tGenerator: loss 1.033036470413208\n",
            "Epoch 438, batch 1/9:\tDiscriminator: real loss 0.2294003963470459, fake loss 0.18801997601985931\tGenerator: loss 0.9994401931762695\n",
            "Epoch 439, batch 1/9:\tDiscriminator: real loss 0.21399323642253876, fake loss 0.1790236532688141\tGenerator: loss 1.0373167991638184\n",
            "Epoch 440, batch 1/9:\tDiscriminator: real loss 0.16920991241931915, fake loss 0.1584426760673523\tGenerator: loss 0.9749239087104797\n",
            "Epoch 441, batch 1/9:\tDiscriminator: real loss 0.16432933509349823, fake loss 0.1697717159986496\tGenerator: loss 1.0366116762161255\n",
            "Epoch 442, batch 1/9:\tDiscriminator: real loss 0.1946551501750946, fake loss 0.19995105266571045\tGenerator: loss 0.9653252959251404\n",
            "Epoch 443, batch 1/9:\tDiscriminator: real loss 0.16827429831027985, fake loss 0.14845570921897888\tGenerator: loss 1.078748106956482\n",
            "Epoch 444, batch 1/9:\tDiscriminator: real loss 0.18557465076446533, fake loss 0.20255742967128754\tGenerator: loss 0.9759138226509094\n",
            "Epoch 445, batch 1/9:\tDiscriminator: real loss 0.1748238205909729, fake loss 0.21921811997890472\tGenerator: loss 0.9917765855789185\n",
            "Epoch 446, batch 1/9:\tDiscriminator: real loss 0.19885031878948212, fake loss 0.1905662715435028\tGenerator: loss 1.0322306156158447\n",
            "Epoch 447, batch 1/9:\tDiscriminator: real loss 0.18072253465652466, fake loss 0.1684989333152771\tGenerator: loss 1.0494489669799805\n",
            "Epoch 448, batch 1/9:\tDiscriminator: real loss 0.20143932104110718, fake loss 0.16100043058395386\tGenerator: loss 1.0547703504562378\n",
            "Epoch 449, batch 1/9:\tDiscriminator: real loss 0.1873812973499298, fake loss 0.19862118363380432\tGenerator: loss 0.9980343580245972\n",
            "Epoch 450, batch 1/9:\tDiscriminator: real loss 0.1604708731174469, fake loss 0.18148089945316315\tGenerator: loss 1.0671323537826538\n",
            "Epoch 451, batch 1/9:\tDiscriminator: real loss 0.14811937510967255, fake loss 0.1731628030538559\tGenerator: loss 0.9559846520423889\n",
            "Epoch 452, batch 1/9:\tDiscriminator: real loss 0.17183533310890198, fake loss 0.1549614667892456\tGenerator: loss 1.1362890005111694\n",
            "Epoch 453, batch 1/9:\tDiscriminator: real loss 0.15758413076400757, fake loss 0.16746243834495544\tGenerator: loss 1.0061347484588623\n",
            "Epoch 454, batch 1/9:\tDiscriminator: real loss 0.17478322982788086, fake loss 0.22348088026046753\tGenerator: loss 0.8341353535652161\n",
            "Epoch 455, batch 1/9:\tDiscriminator: real loss 0.16126567125320435, fake loss 0.11984424293041229\tGenerator: loss 1.1987688541412354\n",
            "Epoch 456, batch 1/9:\tDiscriminator: real loss 0.1543945074081421, fake loss 0.16275152564048767\tGenerator: loss 1.030414342880249\n",
            "Epoch 457, batch 1/9:\tDiscriminator: real loss 0.16722995042800903, fake loss 0.17168325185775757\tGenerator: loss 1.082471251487732\n",
            "Epoch 458, batch 1/9:\tDiscriminator: real loss 0.17133764922618866, fake loss 0.18204057216644287\tGenerator: loss 1.0150389671325684\n",
            "Epoch 459, batch 1/9:\tDiscriminator: real loss 0.18457192182540894, fake loss 0.15746228396892548\tGenerator: loss 1.1071958541870117\n",
            "Epoch 460, batch 1/9:\tDiscriminator: real loss 0.17388805747032166, fake loss 0.23675835132598877\tGenerator: loss 0.9560023546218872\n",
            "Epoch 461, batch 1/9:\tDiscriminator: real loss 0.16867148876190186, fake loss 0.2083013355731964\tGenerator: loss 1.0278637409210205\n",
            "Epoch 462, batch 1/9:\tDiscriminator: real loss 0.21467894315719604, fake loss 0.1581306755542755\tGenerator: loss 1.0695973634719849\n",
            "Epoch 463, batch 1/9:\tDiscriminator: real loss 0.2024058699607849, fake loss 0.14310142397880554\tGenerator: loss 1.1337416172027588\n",
            "Epoch 464, batch 1/9:\tDiscriminator: real loss 0.17948326468467712, fake loss 0.16001051664352417\tGenerator: loss 1.079709768295288\n",
            "Epoch 465, batch 1/9:\tDiscriminator: real loss 0.17048490047454834, fake loss 0.18632197380065918\tGenerator: loss 1.1041569709777832\n",
            "Epoch 466, batch 1/9:\tDiscriminator: real loss 0.1127459704875946, fake loss 0.15215888619422913\tGenerator: loss 1.1933379173278809\n",
            "Epoch 467, batch 1/9:\tDiscriminator: real loss 0.15702863037586212, fake loss 0.13876651227474213\tGenerator: loss 1.1097649335861206\n",
            "Epoch 468, batch 1/9:\tDiscriminator: real loss 0.16252222657203674, fake loss 0.18721118569374084\tGenerator: loss 0.9513640403747559\n",
            "Epoch 469, batch 1/9:\tDiscriminator: real loss 0.18653632700443268, fake loss 0.16611289978027344\tGenerator: loss 1.151719570159912\n",
            "Epoch 470, batch 1/9:\tDiscriminator: real loss 0.17127278447151184, fake loss 0.14991334080696106\tGenerator: loss 1.0840336084365845\n",
            "Epoch 471, batch 1/9:\tDiscriminator: real loss 0.17631468176841736, fake loss 0.2041603922843933\tGenerator: loss 0.9506280422210693\n",
            "Epoch 472, batch 1/9:\tDiscriminator: real loss 0.20209001004695892, fake loss 0.22384433448314667\tGenerator: loss 1.1107652187347412\n",
            "Epoch 473, batch 1/9:\tDiscriminator: real loss 0.16090568900108337, fake loss 0.18392455577850342\tGenerator: loss 1.0187058448791504\n",
            "Epoch 474, batch 1/9:\tDiscriminator: real loss 0.19205448031425476, fake loss 0.17288890480995178\tGenerator: loss 0.9626219272613525\n",
            "Epoch 475, batch 1/9:\tDiscriminator: real loss 0.16872701048851013, fake loss 0.17029741406440735\tGenerator: loss 1.048902988433838\n",
            "Epoch 476, batch 1/9:\tDiscriminator: real loss 0.17182497680187225, fake loss 0.17487618327140808\tGenerator: loss 1.0527232885360718\n",
            "Epoch 477, batch 1/9:\tDiscriminator: real loss 0.18182381987571716, fake loss 0.14077778160572052\tGenerator: loss 1.1360576152801514\n",
            "Epoch 478, batch 1/9:\tDiscriminator: real loss 0.1460513174533844, fake loss 0.17209920287132263\tGenerator: loss 1.078458309173584\n",
            "Epoch 479, batch 1/9:\tDiscriminator: real loss 0.15025503933429718, fake loss 0.15045255422592163\tGenerator: loss 1.0742130279541016\n",
            "Epoch 480, batch 1/9:\tDiscriminator: real loss 0.15001283586025238, fake loss 0.17387108504772186\tGenerator: loss 1.0550878047943115\n",
            "Epoch 481, batch 1/9:\tDiscriminator: real loss 0.1491362303495407, fake loss 0.17957527935504913\tGenerator: loss 1.1329108476638794\n",
            "Epoch 482, batch 1/9:\tDiscriminator: real loss 0.1286759078502655, fake loss 0.12660391628742218\tGenerator: loss 1.325264573097229\n",
            "Epoch 483, batch 1/9:\tDiscriminator: real loss 0.11960840225219727, fake loss 0.13278411328792572\tGenerator: loss 1.2206140756607056\n",
            "Epoch 484, batch 1/9:\tDiscriminator: real loss 0.13333211839199066, fake loss 0.14522522687911987\tGenerator: loss 1.2440528869628906\n",
            "Epoch 485, batch 1/9:\tDiscriminator: real loss 0.129039466381073, fake loss 0.1384861171245575\tGenerator: loss 1.3103668689727783\n",
            "Epoch 486, batch 1/9:\tDiscriminator: real loss 0.11177871376276016, fake loss 0.17679616808891296\tGenerator: loss 1.2033201456069946\n",
            "Epoch 487, batch 1/9:\tDiscriminator: real loss 0.1546105593442917, fake loss 0.18163174390792847\tGenerator: loss 1.0525544881820679\n",
            "Epoch 488, batch 1/9:\tDiscriminator: real loss 0.15625731647014618, fake loss 0.14180120825767517\tGenerator: loss 1.1691536903381348\n",
            "Epoch 489, batch 1/9:\tDiscriminator: real loss 0.13639429211616516, fake loss 0.1482315957546234\tGenerator: loss 1.0992567539215088\n",
            "Epoch 490, batch 1/9:\tDiscriminator: real loss 0.20745238661766052, fake loss 0.16819849610328674\tGenerator: loss 1.0557140111923218\n",
            "Epoch 491, batch 1/9:\tDiscriminator: real loss 0.1538526713848114, fake loss 0.1689339429140091\tGenerator: loss 0.9974192380905151\n",
            "Epoch 492, batch 1/9:\tDiscriminator: real loss 0.18465584516525269, fake loss 0.1412663757801056\tGenerator: loss 1.1277471780776978\n",
            "Epoch 493, batch 1/9:\tDiscriminator: real loss 0.19295285642147064, fake loss 0.17345663905143738\tGenerator: loss 1.068756341934204\n",
            "Epoch 494, batch 1/9:\tDiscriminator: real loss 0.17957183718681335, fake loss 0.18948857486248016\tGenerator: loss 1.0277019739151\n",
            "Epoch 495, batch 1/9:\tDiscriminator: real loss 0.1834614872932434, fake loss 0.18023937940597534\tGenerator: loss 0.9999909400939941\n",
            "Epoch 496, batch 1/9:\tDiscriminator: real loss 0.1696387231349945, fake loss 0.15603867173194885\tGenerator: loss 1.1682041883468628\n",
            "Epoch 497, batch 1/9:\tDiscriminator: real loss 0.16930273175239563, fake loss 0.16889816522598267\tGenerator: loss 1.0241326093673706\n",
            "Epoch 498, batch 1/9:\tDiscriminator: real loss 0.18613478541374207, fake loss 0.17956174910068512\tGenerator: loss 1.043926477432251\n",
            "Epoch 499, batch 1/9:\tDiscriminator: real loss 0.18977874517440796, fake loss 0.19406729936599731\tGenerator: loss 1.0258231163024902\n",
            "Epoch 500, batch 1/9:\tDiscriminator: real loss 0.17560255527496338, fake loss 0.164010152220726\tGenerator: loss 0.9923961162567139\n",
            "Epoch 501, batch 1/9:\tDiscriminator: real loss 0.17071369290351868, fake loss 0.14809000492095947\tGenerator: loss 1.0552866458892822\n",
            "Epoch 502, batch 1/9:\tDiscriminator: real loss 0.16463321447372437, fake loss 0.1526826024055481\tGenerator: loss 0.9948520064353943\n",
            "Epoch 503, batch 1/9:\tDiscriminator: real loss 0.1597519814968109, fake loss 0.179485484957695\tGenerator: loss 1.039867877960205\n",
            "Epoch 504, batch 1/9:\tDiscriminator: real loss 0.17917855083942413, fake loss 0.15459755063056946\tGenerator: loss 1.0203245878219604\n",
            "Epoch 505, batch 1/9:\tDiscriminator: real loss 0.19489863514900208, fake loss 0.18270343542099\tGenerator: loss 0.9753456115722656\n",
            "Epoch 506, batch 1/9:\tDiscriminator: real loss 0.24303340911865234, fake loss 0.1570107489824295\tGenerator: loss 1.0140562057495117\n",
            "Epoch 507, batch 1/9:\tDiscriminator: real loss 0.21947386860847473, fake loss 0.17103621363639832\tGenerator: loss 1.0872125625610352\n",
            "Epoch 508, batch 1/9:\tDiscriminator: real loss 0.2275369018316269, fake loss 0.1973390132188797\tGenerator: loss 0.9563573598861694\n",
            "Epoch 509, batch 1/9:\tDiscriminator: real loss 0.2186480462551117, fake loss 0.18741995096206665\tGenerator: loss 0.9909932613372803\n",
            "Epoch 510, batch 1/9:\tDiscriminator: real loss 0.22192880511283875, fake loss 0.1824575662612915\tGenerator: loss 0.9828656911849976\n",
            "Epoch 511, batch 1/9:\tDiscriminator: real loss 0.20423679053783417, fake loss 0.1773829162120819\tGenerator: loss 0.9809960126876831\n",
            "Epoch 512, batch 1/9:\tDiscriminator: real loss 0.17848844826221466, fake loss 0.18008540570735931\tGenerator: loss 1.0307977199554443\n",
            "Epoch 513, batch 1/9:\tDiscriminator: real loss 0.1810922920703888, fake loss 0.19497226178646088\tGenerator: loss 0.9805364608764648\n",
            "Epoch 514, batch 1/9:\tDiscriminator: real loss 0.209637850522995, fake loss 0.1871323585510254\tGenerator: loss 0.9905557036399841\n",
            "Epoch 515, batch 1/9:\tDiscriminator: real loss 0.1916576623916626, fake loss 0.20883925259113312\tGenerator: loss 0.9319308996200562\n",
            "Epoch 516, batch 1/9:\tDiscriminator: real loss 0.24820321798324585, fake loss 0.20711150765419006\tGenerator: loss 0.9150370359420776\n",
            "Epoch 517, batch 1/9:\tDiscriminator: real loss 0.1844462752342224, fake loss 0.1954115629196167\tGenerator: loss 1.0164117813110352\n",
            "Epoch 518, batch 1/9:\tDiscriminator: real loss 0.1744738519191742, fake loss 0.18500742316246033\tGenerator: loss 0.9910262823104858\n",
            "Epoch 519, batch 1/9:\tDiscriminator: real loss 0.19114482402801514, fake loss 0.17609436810016632\tGenerator: loss 1.0414665937423706\n",
            "Epoch 520, batch 1/9:\tDiscriminator: real loss 0.18225538730621338, fake loss 0.17771951854228973\tGenerator: loss 1.1509422063827515\n",
            "Epoch 521, batch 1/9:\tDiscriminator: real loss 0.16300691664218903, fake loss 0.17216703295707703\tGenerator: loss 1.0631256103515625\n",
            "Epoch 522, batch 1/9:\tDiscriminator: real loss 0.16833406686782837, fake loss 0.12780997157096863\tGenerator: loss 1.1317979097366333\n",
            "Epoch 523, batch 1/9:\tDiscriminator: real loss 0.14100344479084015, fake loss 0.1626473367214203\tGenerator: loss 1.1059138774871826\n",
            "Epoch 524, batch 1/9:\tDiscriminator: real loss 0.14001229405403137, fake loss 0.1359807848930359\tGenerator: loss 1.139671802520752\n",
            "Epoch 525, batch 1/9:\tDiscriminator: real loss 0.1536414921283722, fake loss 0.1658724695444107\tGenerator: loss 1.090377926826477\n",
            "Epoch 526, batch 1/9:\tDiscriminator: real loss 0.17590919137001038, fake loss 0.18835130333900452\tGenerator: loss 1.085575819015503\n",
            "Epoch 527, batch 1/9:\tDiscriminator: real loss 0.18868374824523926, fake loss 0.17661477625370026\tGenerator: loss 1.035141110420227\n",
            "Epoch 528, batch 1/9:\tDiscriminator: real loss 0.1481674760580063, fake loss 0.16425490379333496\tGenerator: loss 1.135250449180603\n",
            "Epoch 529, batch 1/9:\tDiscriminator: real loss 0.17473255097866058, fake loss 0.17255018651485443\tGenerator: loss 1.0569915771484375\n",
            "Epoch 530, batch 1/9:\tDiscriminator: real loss 0.1643526703119278, fake loss 0.14240559935569763\tGenerator: loss 1.0329829454421997\n",
            "Epoch 531, batch 1/9:\tDiscriminator: real loss 0.16029131412506104, fake loss 0.15295663475990295\tGenerator: loss 1.0465601682662964\n",
            "Epoch 532, batch 1/9:\tDiscriminator: real loss 0.17513415217399597, fake loss 0.17750021815299988\tGenerator: loss 0.9995083808898926\n",
            "Epoch 533, batch 1/9:\tDiscriminator: real loss 0.194411039352417, fake loss 0.19430628418922424\tGenerator: loss 0.9793521165847778\n",
            "Epoch 534, batch 1/9:\tDiscriminator: real loss 0.20558924973011017, fake loss 0.1912762075662613\tGenerator: loss 1.013555884361267\n",
            "Epoch 535, batch 1/9:\tDiscriminator: real loss 0.17457257211208344, fake loss 0.16092243790626526\tGenerator: loss 0.9778143167495728\n",
            "Epoch 536, batch 1/9:\tDiscriminator: real loss 0.17763572931289673, fake loss 0.19327735900878906\tGenerator: loss 1.044276237487793\n",
            "Epoch 537, batch 1/9:\tDiscriminator: real loss 0.15941676497459412, fake loss 0.17004519701004028\tGenerator: loss 1.002041220664978\n",
            "Epoch 538, batch 1/9:\tDiscriminator: real loss 0.1769208312034607, fake loss 0.2201126217842102\tGenerator: loss 0.9703061580657959\n",
            "Epoch 539, batch 1/9:\tDiscriminator: real loss 0.20598313212394714, fake loss 0.1796381175518036\tGenerator: loss 1.1172878742218018\n",
            "Epoch 540, batch 1/9:\tDiscriminator: real loss 0.18712618947029114, fake loss 0.20641010999679565\tGenerator: loss 1.0467476844787598\n",
            "Epoch 541, batch 1/9:\tDiscriminator: real loss 0.17539194226264954, fake loss 0.1549215316772461\tGenerator: loss 1.1216959953308105\n",
            "Epoch 542, batch 1/9:\tDiscriminator: real loss 0.200391948223114, fake loss 0.19880007207393646\tGenerator: loss 1.0351283550262451\n",
            "Epoch 543, batch 1/9:\tDiscriminator: real loss 0.246847003698349, fake loss 0.19065284729003906\tGenerator: loss 1.0353789329528809\n",
            "Epoch 544, batch 1/9:\tDiscriminator: real loss 0.1937209814786911, fake loss 0.16270548105239868\tGenerator: loss 0.9876512289047241\n",
            "Epoch 545, batch 1/9:\tDiscriminator: real loss 0.1766209453344345, fake loss 0.20406651496887207\tGenerator: loss 0.9811371564865112\n",
            "Epoch 546, batch 1/9:\tDiscriminator: real loss 0.16073103249073029, fake loss 0.15853279829025269\tGenerator: loss 1.0792628526687622\n",
            "Epoch 547, batch 1/9:\tDiscriminator: real loss 0.13755691051483154, fake loss 0.14638236165046692\tGenerator: loss 1.138363003730774\n",
            "Epoch 548, batch 1/9:\tDiscriminator: real loss 0.15261812508106232, fake loss 0.171648308634758\tGenerator: loss 1.1801530122756958\n",
            "Epoch 549, batch 1/9:\tDiscriminator: real loss 0.1481742113828659, fake loss 0.13384906947612762\tGenerator: loss 1.1090822219848633\n",
            "Epoch 550, batch 1/9:\tDiscriminator: real loss 0.19117754697799683, fake loss 0.1666763722896576\tGenerator: loss 1.0814628601074219\n",
            "Epoch 551, batch 1/9:\tDiscriminator: real loss 0.19435477256774902, fake loss 0.19677484035491943\tGenerator: loss 0.9888643026351929\n",
            "Epoch 552, batch 1/9:\tDiscriminator: real loss 0.20122984051704407, fake loss 0.13225102424621582\tGenerator: loss 1.1226400136947632\n",
            "Epoch 553, batch 1/9:\tDiscriminator: real loss 0.15441951155662537, fake loss 0.130273699760437\tGenerator: loss 1.2055507898330688\n",
            "Epoch 554, batch 1/9:\tDiscriminator: real loss 0.15613365173339844, fake loss 0.16144351661205292\tGenerator: loss 1.0606729984283447\n",
            "Epoch 555, batch 1/9:\tDiscriminator: real loss 0.17027953267097473, fake loss 0.1317386031150818\tGenerator: loss 1.1676130294799805\n",
            "Epoch 556, batch 1/9:\tDiscriminator: real loss 0.1274164766073227, fake loss 0.17268435657024384\tGenerator: loss 1.0493303537368774\n",
            "Epoch 557, batch 1/9:\tDiscriminator: real loss 0.13007336854934692, fake loss 0.21222355961799622\tGenerator: loss 1.002426266670227\n",
            "Epoch 558, batch 1/9:\tDiscriminator: real loss 0.16102832555770874, fake loss 0.18521976470947266\tGenerator: loss 1.1128482818603516\n",
            "Epoch 559, batch 1/9:\tDiscriminator: real loss 0.16154137253761292, fake loss 0.16844424605369568\tGenerator: loss 1.1370346546173096\n",
            "Epoch 560, batch 1/9:\tDiscriminator: real loss 0.16040244698524475, fake loss 0.1525631546974182\tGenerator: loss 1.079503059387207\n",
            "Epoch 561, batch 1/9:\tDiscriminator: real loss 0.14599637687206268, fake loss 0.1640630066394806\tGenerator: loss 1.0479289293289185\n",
            "Epoch 562, batch 1/9:\tDiscriminator: real loss 0.15520206093788147, fake loss 0.22000840306282043\tGenerator: loss 0.8846468925476074\n",
            "Epoch 563, batch 1/9:\tDiscriminator: real loss 0.1746951937675476, fake loss 0.19608259201049805\tGenerator: loss 0.9616086483001709\n",
            "Epoch 564, batch 1/9:\tDiscriminator: real loss 0.17684303224086761, fake loss 0.1830924153327942\tGenerator: loss 1.0085933208465576\n",
            "Epoch 565, batch 1/9:\tDiscriminator: real loss 0.1927836835384369, fake loss 0.1715218424797058\tGenerator: loss 1.051501989364624\n",
            "Epoch 566, batch 1/9:\tDiscriminator: real loss 0.18874135613441467, fake loss 0.18159916996955872\tGenerator: loss 0.9722903370857239\n",
            "Epoch 567, batch 1/9:\tDiscriminator: real loss 0.17766636610031128, fake loss 0.1737033724784851\tGenerator: loss 1.1519854068756104\n",
            "Epoch 568, batch 1/9:\tDiscriminator: real loss 0.1488398015499115, fake loss 0.1413126140832901\tGenerator: loss 1.066638469696045\n",
            "Epoch 569, batch 1/9:\tDiscriminator: real loss 0.12780439853668213, fake loss 0.14726120233535767\tGenerator: loss 1.198655605316162\n",
            "Epoch 570, batch 1/9:\tDiscriminator: real loss 0.14874568581581116, fake loss 0.14899957180023193\tGenerator: loss 1.1532783508300781\n",
            "Epoch 571, batch 1/9:\tDiscriminator: real loss 0.15781520307064056, fake loss 0.1604720801115036\tGenerator: loss 1.0998364686965942\n",
            "Epoch 572, batch 1/9:\tDiscriminator: real loss 0.18897604942321777, fake loss 0.23755896091461182\tGenerator: loss 0.9193636178970337\n",
            "Epoch 573, batch 1/9:\tDiscriminator: real loss 0.2650776505470276, fake loss 0.17033210396766663\tGenerator: loss 0.9901312589645386\n",
            "Epoch 574, batch 1/9:\tDiscriminator: real loss 0.2608087956905365, fake loss 0.2698049545288086\tGenerator: loss 1.0824809074401855\n",
            "Epoch 575, batch 1/9:\tDiscriminator: real loss 0.2095526158809662, fake loss 0.15031784772872925\tGenerator: loss 1.2550554275512695\n",
            "Epoch 576, batch 1/9:\tDiscriminator: real loss 0.193924218416214, fake loss 0.20666494965553284\tGenerator: loss 1.0331695079803467\n",
            "Epoch 577, batch 1/9:\tDiscriminator: real loss 0.16726446151733398, fake loss 0.16735686361789703\tGenerator: loss 1.1468061208724976\n",
            "Epoch 578, batch 1/9:\tDiscriminator: real loss 0.19573882222175598, fake loss 0.1834554374217987\tGenerator: loss 1.0577375888824463\n",
            "Epoch 579, batch 1/9:\tDiscriminator: real loss 0.17299917340278625, fake loss 0.16618049144744873\tGenerator: loss 1.0847902297973633\n",
            "Epoch 580, batch 1/9:\tDiscriminator: real loss 0.15917783975601196, fake loss 0.16949865221977234\tGenerator: loss 1.0861554145812988\n",
            "Epoch 581, batch 1/9:\tDiscriminator: real loss 0.16327279806137085, fake loss 0.17672725021839142\tGenerator: loss 1.0740277767181396\n",
            "Epoch 582, batch 1/9:\tDiscriminator: real loss 0.16588272154331207, fake loss 0.1685566008090973\tGenerator: loss 1.0948492288589478\n",
            "Epoch 583, batch 1/9:\tDiscriminator: real loss 0.176498144865036, fake loss 0.16770873963832855\tGenerator: loss 1.0263944864273071\n",
            "Epoch 584, batch 1/9:\tDiscriminator: real loss 0.20924118161201477, fake loss 0.1491207331418991\tGenerator: loss 0.989026665687561\n",
            "Epoch 585, batch 1/9:\tDiscriminator: real loss 0.2086043655872345, fake loss 0.1718750298023224\tGenerator: loss 0.9762387275695801\n",
            "Epoch 586, batch 1/9:\tDiscriminator: real loss 0.21109406650066376, fake loss 0.20017576217651367\tGenerator: loss 0.93653404712677\n",
            "Epoch 587, batch 1/9:\tDiscriminator: real loss 0.21112820506095886, fake loss 0.20930136740207672\tGenerator: loss 1.0410367250442505\n",
            "Epoch 588, batch 1/9:\tDiscriminator: real loss 0.21771171689033508, fake loss 0.19508221745491028\tGenerator: loss 0.9696928262710571\n",
            "Epoch 589, batch 1/9:\tDiscriminator: real loss 0.22183939814567566, fake loss 0.18643173575401306\tGenerator: loss 1.0185270309448242\n",
            "Epoch 590, batch 1/9:\tDiscriminator: real loss 0.1635274589061737, fake loss 0.17762750387191772\tGenerator: loss 1.0466530323028564\n",
            "Epoch 591, batch 1/9:\tDiscriminator: real loss 0.20163565874099731, fake loss 0.1586025059223175\tGenerator: loss 1.0121407508850098\n",
            "Epoch 592, batch 1/9:\tDiscriminator: real loss 0.15663495659828186, fake loss 0.1504693627357483\tGenerator: loss 1.1003007888793945\n",
            "Epoch 593, batch 1/9:\tDiscriminator: real loss 0.14455285668373108, fake loss 0.1623544842004776\tGenerator: loss 1.109703540802002\n",
            "Epoch 594, batch 1/9:\tDiscriminator: real loss 0.12450140714645386, fake loss 0.18520669639110565\tGenerator: loss 1.2332345247268677\n",
            "Epoch 595, batch 1/9:\tDiscriminator: real loss 0.14128270745277405, fake loss 0.14500579237937927\tGenerator: loss 1.1269909143447876\n",
            "Epoch 596, batch 1/9:\tDiscriminator: real loss 0.14698487520217896, fake loss 0.12212902307510376\tGenerator: loss 1.2195730209350586\n",
            "Epoch 597, batch 1/9:\tDiscriminator: real loss 0.13052034378051758, fake loss 0.14364618062973022\tGenerator: loss 1.3558032512664795\n",
            "Epoch 598, batch 1/9:\tDiscriminator: real loss 0.13889844715595245, fake loss 0.1652970314025879\tGenerator: loss 1.2528340816497803\n",
            "Epoch 599, batch 1/9:\tDiscriminator: real loss 0.13497626781463623, fake loss 0.1611795574426651\tGenerator: loss 1.1649963855743408\n",
            "Epoch 600, batch 1/9:\tDiscriminator: real loss 0.1244957447052002, fake loss 0.13933688402175903\tGenerator: loss 1.2344917058944702\n",
            "Epoch 601, batch 1/9:\tDiscriminator: real loss 0.10755083709955215, fake loss 0.14162394404411316\tGenerator: loss 1.3927078247070312\n",
            "Epoch 602, batch 1/9:\tDiscriminator: real loss 0.13052742183208466, fake loss 0.14373624324798584\tGenerator: loss 1.3461142778396606\n",
            "Epoch 603, batch 1/9:\tDiscriminator: real loss 0.10766195505857468, fake loss 0.14546062052249908\tGenerator: loss 1.3102960586547852\n",
            "Epoch 604, batch 1/9:\tDiscriminator: real loss 0.16403749585151672, fake loss 0.221047043800354\tGenerator: loss 0.9632754325866699\n",
            "Epoch 605, batch 1/9:\tDiscriminator: real loss 0.2172764092683792, fake loss 0.1840878427028656\tGenerator: loss 1.0314706563949585\n",
            "Epoch 606, batch 1/9:\tDiscriminator: real loss 0.21913573145866394, fake loss 0.16283704340457916\tGenerator: loss 1.0606786012649536\n",
            "Epoch 607, batch 1/9:\tDiscriminator: real loss 0.23206283152103424, fake loss 0.16783012449741364\tGenerator: loss 1.0632731914520264\n",
            "Epoch 608, batch 1/9:\tDiscriminator: real loss 0.2252771556377411, fake loss 0.21535630524158478\tGenerator: loss 0.9765356183052063\n",
            "Epoch 609, batch 1/9:\tDiscriminator: real loss 0.21927973628044128, fake loss 0.17726784944534302\tGenerator: loss 1.091271996498108\n",
            "Epoch 610, batch 1/9:\tDiscriminator: real loss 0.18873167037963867, fake loss 0.1574312150478363\tGenerator: loss 1.0972425937652588\n",
            "Epoch 611, batch 1/9:\tDiscriminator: real loss 0.14800626039505005, fake loss 0.16970905661582947\tGenerator: loss 1.1259551048278809\n",
            "Epoch 612, batch 1/9:\tDiscriminator: real loss 0.1457594931125641, fake loss 0.16138970851898193\tGenerator: loss 1.1221609115600586\n",
            "Epoch 613, batch 1/9:\tDiscriminator: real loss 0.15633343160152435, fake loss 0.2061479091644287\tGenerator: loss 0.9654179811477661\n",
            "Epoch 614, batch 1/9:\tDiscriminator: real loss 0.18369945883750916, fake loss 0.16480568051338196\tGenerator: loss 0.9548457264900208\n",
            "Epoch 615, batch 1/9:\tDiscriminator: real loss 0.19050750136375427, fake loss 0.18719986081123352\tGenerator: loss 1.0973373651504517\n",
            "Epoch 616, batch 1/9:\tDiscriminator: real loss 0.17823825776576996, fake loss 0.15455496311187744\tGenerator: loss 1.2188884019851685\n",
            "Epoch 617, batch 1/9:\tDiscriminator: real loss 0.13286074995994568, fake loss 0.12708577513694763\tGenerator: loss 1.192262053489685\n",
            "Epoch 618, batch 1/9:\tDiscriminator: real loss 0.14436787366867065, fake loss 0.12242545932531357\tGenerator: loss 1.20624577999115\n",
            "Epoch 619, batch 1/9:\tDiscriminator: real loss 0.14690598845481873, fake loss 0.15688534080982208\tGenerator: loss 1.137439250946045\n",
            "Epoch 620, batch 1/9:\tDiscriminator: real loss 0.17857429385185242, fake loss 0.16576075553894043\tGenerator: loss 1.1295831203460693\n",
            "Epoch 621, batch 1/9:\tDiscriminator: real loss 0.20824319124221802, fake loss 0.17455072700977325\tGenerator: loss 1.0361745357513428\n",
            "Epoch 622, batch 1/9:\tDiscriminator: real loss 0.2408883422613144, fake loss 0.16467387974262238\tGenerator: loss 1.1456427574157715\n",
            "Epoch 623, batch 1/9:\tDiscriminator: real loss 0.2400052398443222, fake loss 0.1540948748588562\tGenerator: loss 1.1565232276916504\n",
            "Epoch 624, batch 1/9:\tDiscriminator: real loss 0.2576112449169159, fake loss 0.1935173124074936\tGenerator: loss 1.0831446647644043\n",
            "Epoch 625, batch 1/9:\tDiscriminator: real loss 0.22714287042617798, fake loss 0.16689562797546387\tGenerator: loss 1.1080057621002197\n",
            "Epoch 626, batch 1/9:\tDiscriminator: real loss 0.2052910327911377, fake loss 0.12850579619407654\tGenerator: loss 1.14665687084198\n",
            "Epoch 627, batch 1/9:\tDiscriminator: real loss 0.19104218482971191, fake loss 0.14594122767448425\tGenerator: loss 1.0454925298690796\n",
            "Epoch 628, batch 1/9:\tDiscriminator: real loss 0.19109119474887848, fake loss 0.17361560463905334\tGenerator: loss 1.0613102912902832\n",
            "Epoch 629, batch 1/9:\tDiscriminator: real loss 0.16514229774475098, fake loss 0.20832522213459015\tGenerator: loss 1.0935648679733276\n",
            "Epoch 630, batch 1/9:\tDiscriminator: real loss 0.20905038714408875, fake loss 0.1817256659269333\tGenerator: loss 1.018622636795044\n",
            "Epoch 631, batch 1/9:\tDiscriminator: real loss 0.19040824472904205, fake loss 0.19066733121871948\tGenerator: loss 0.9637842178344727\n",
            "Epoch 632, batch 1/9:\tDiscriminator: real loss 0.2078210413455963, fake loss 0.1758849024772644\tGenerator: loss 0.978724479675293\n",
            "Epoch 633, batch 1/9:\tDiscriminator: real loss 0.18296071887016296, fake loss 0.18717056512832642\tGenerator: loss 1.005068063735962\n",
            "Epoch 634, batch 1/9:\tDiscriminator: real loss 0.16623404622077942, fake loss 0.20163282752037048\tGenerator: loss 1.0749619007110596\n",
            "Epoch 635, batch 1/9:\tDiscriminator: real loss 0.1866546869277954, fake loss 0.21374519169330597\tGenerator: loss 0.9597119092941284\n",
            "Epoch 636, batch 1/9:\tDiscriminator: real loss 0.1666753739118576, fake loss 0.16096311807632446\tGenerator: loss 1.0194745063781738\n",
            "Epoch 637, batch 1/9:\tDiscriminator: real loss 0.15657690167427063, fake loss 0.18066734075546265\tGenerator: loss 1.0329077243804932\n",
            "Epoch 638, batch 1/9:\tDiscriminator: real loss 0.16356904804706573, fake loss 0.17212599515914917\tGenerator: loss 1.0597803592681885\n",
            "Epoch 639, batch 1/9:\tDiscriminator: real loss 0.1764441728591919, fake loss 0.16928158700466156\tGenerator: loss 1.0332410335540771\n",
            "Epoch 640, batch 1/9:\tDiscriminator: real loss 0.15348325669765472, fake loss 0.14371496438980103\tGenerator: loss 1.0883084535598755\n",
            "Epoch 641, batch 1/9:\tDiscriminator: real loss 0.17109674215316772, fake loss 0.15371078252792358\tGenerator: loss 1.0900797843933105\n",
            "Epoch 642, batch 1/9:\tDiscriminator: real loss 0.12173931300640106, fake loss 0.15416209399700165\tGenerator: loss 1.2190948724746704\n",
            "Epoch 643, batch 1/9:\tDiscriminator: real loss 0.1321955919265747, fake loss 0.13234171271324158\tGenerator: loss 1.186624526977539\n",
            "Epoch 644, batch 1/9:\tDiscriminator: real loss 0.14946848154067993, fake loss 0.1804235875606537\tGenerator: loss 1.092973232269287\n",
            "Epoch 645, batch 1/9:\tDiscriminator: real loss 0.13380590081214905, fake loss 0.1737973988056183\tGenerator: loss 1.0401265621185303\n",
            "Epoch 646, batch 1/9:\tDiscriminator: real loss 0.19781410694122314, fake loss 0.20010778307914734\tGenerator: loss 1.0266188383102417\n",
            "Epoch 647, batch 1/9:\tDiscriminator: real loss 0.23152436316013336, fake loss 0.2024671584367752\tGenerator: loss 0.9973365068435669\n",
            "Epoch 648, batch 1/9:\tDiscriminator: real loss 0.19963565468788147, fake loss 0.1345376819372177\tGenerator: loss 1.274280309677124\n",
            "Epoch 649, batch 1/9:\tDiscriminator: real loss 0.22953292727470398, fake loss 0.17820970714092255\tGenerator: loss 1.155318021774292\n",
            "Epoch 650, batch 1/9:\tDiscriminator: real loss 0.1999422311782837, fake loss 0.15411536395549774\tGenerator: loss 1.1095998287200928\n",
            "Epoch 651, batch 1/9:\tDiscriminator: real loss 0.1913527250289917, fake loss 0.18494415283203125\tGenerator: loss 0.9455195665359497\n",
            "Epoch 652, batch 1/9:\tDiscriminator: real loss 0.23607797920703888, fake loss 0.2192474901676178\tGenerator: loss 1.1032617092132568\n",
            "Epoch 653, batch 1/9:\tDiscriminator: real loss 0.23439821600914001, fake loss 0.1490744948387146\tGenerator: loss 1.34261155128479\n",
            "Epoch 654, batch 1/9:\tDiscriminator: real loss 0.19674566388130188, fake loss 0.15951010584831238\tGenerator: loss 1.2047393321990967\n",
            "Epoch 655, batch 1/9:\tDiscriminator: real loss 0.19635054469108582, fake loss 0.15954306721687317\tGenerator: loss 1.1926522254943848\n",
            "Epoch 656, batch 1/9:\tDiscriminator: real loss 0.20106396079063416, fake loss 0.24932192265987396\tGenerator: loss 0.8800386786460876\n",
            "Epoch 657, batch 1/9:\tDiscriminator: real loss 0.1953314244747162, fake loss 0.16835328936576843\tGenerator: loss 1.1268655061721802\n",
            "Epoch 658, batch 1/9:\tDiscriminator: real loss 0.2108236849308014, fake loss 0.18762236833572388\tGenerator: loss 1.0067614316940308\n",
            "Epoch 659, batch 1/9:\tDiscriminator: real loss 0.2176506519317627, fake loss 0.1631990671157837\tGenerator: loss 1.1473422050476074\n",
            "Epoch 660, batch 1/9:\tDiscriminator: real loss 0.210393488407135, fake loss 0.1859092265367508\tGenerator: loss 0.9655439257621765\n",
            "Epoch 661, batch 1/9:\tDiscriminator: real loss 0.18992505967617035, fake loss 0.14243243634700775\tGenerator: loss 1.1014976501464844\n",
            "Epoch 662, batch 1/9:\tDiscriminator: real loss 0.17072856426239014, fake loss 0.18798938393592834\tGenerator: loss 1.0165966749191284\n",
            "Epoch 663, batch 1/9:\tDiscriminator: real loss 0.17170818150043488, fake loss 0.18506896495819092\tGenerator: loss 0.9813188314437866\n",
            "Epoch 664, batch 1/9:\tDiscriminator: real loss 0.1852937638759613, fake loss 0.16552765667438507\tGenerator: loss 1.1244068145751953\n",
            "Epoch 665, batch 1/9:\tDiscriminator: real loss 0.14606967568397522, fake loss 0.142990380525589\tGenerator: loss 1.0424736738204956\n",
            "Epoch 666, batch 1/9:\tDiscriminator: real loss 0.19321510195732117, fake loss 0.20388615131378174\tGenerator: loss 0.9738340377807617\n",
            "Epoch 667, batch 1/9:\tDiscriminator: real loss 0.21044349670410156, fake loss 0.19831538200378418\tGenerator: loss 1.002227544784546\n",
            "Epoch 668, batch 1/9:\tDiscriminator: real loss 0.18792122602462769, fake loss 0.15847015380859375\tGenerator: loss 1.099081039428711\n",
            "Epoch 669, batch 1/9:\tDiscriminator: real loss 0.1745307445526123, fake loss 0.1580977737903595\tGenerator: loss 1.0227725505828857\n",
            "Epoch 670, batch 1/9:\tDiscriminator: real loss 0.19830572605133057, fake loss 0.15933725237846375\tGenerator: loss 1.1347599029541016\n",
            "Epoch 671, batch 1/9:\tDiscriminator: real loss 0.15675020217895508, fake loss 0.17144055664539337\tGenerator: loss 1.1105886697769165\n",
            "Epoch 672, batch 1/9:\tDiscriminator: real loss 0.17433878779411316, fake loss 0.17292088270187378\tGenerator: loss 1.0559380054473877\n",
            "Epoch 673, batch 1/9:\tDiscriminator: real loss 0.1811809539794922, fake loss 0.15442250669002533\tGenerator: loss 1.0089571475982666\n",
            "Epoch 674, batch 1/9:\tDiscriminator: real loss 0.21725404262542725, fake loss 0.15845665335655212\tGenerator: loss 1.050654411315918\n",
            "Epoch 675, batch 1/9:\tDiscriminator: real loss 0.17737020552158356, fake loss 0.15620774030685425\tGenerator: loss 1.0895332098007202\n",
            "Epoch 676, batch 1/9:\tDiscriminator: real loss 0.13603225350379944, fake loss 0.16200032830238342\tGenerator: loss 1.007068395614624\n",
            "Epoch 677, batch 1/9:\tDiscriminator: real loss 0.15842454135417938, fake loss 0.18307864665985107\tGenerator: loss 1.0119833946228027\n",
            "Epoch 678, batch 1/9:\tDiscriminator: real loss 0.14934992790222168, fake loss 0.15897534787654877\tGenerator: loss 1.1588667631149292\n",
            "Epoch 679, batch 1/9:\tDiscriminator: real loss 0.1579103171825409, fake loss 0.1718752235174179\tGenerator: loss 1.0490286350250244\n",
            "Epoch 680, batch 1/9:\tDiscriminator: real loss 0.16328902542591095, fake loss 0.15886357426643372\tGenerator: loss 1.140129566192627\n",
            "Epoch 681, batch 1/9:\tDiscriminator: real loss 0.15066483616828918, fake loss 0.15332669019699097\tGenerator: loss 1.0865530967712402\n",
            "Epoch 682, batch 1/9:\tDiscriminator: real loss 0.16450835764408112, fake loss 0.18228814005851746\tGenerator: loss 1.2090623378753662\n",
            "Epoch 683, batch 1/9:\tDiscriminator: real loss 0.15459758043289185, fake loss 0.15853562951087952\tGenerator: loss 1.0305553674697876\n",
            "Epoch 684, batch 1/9:\tDiscriminator: real loss 0.19391468167304993, fake loss 0.14890891313552856\tGenerator: loss 1.087733507156372\n",
            "Epoch 685, batch 1/9:\tDiscriminator: real loss 0.1844819039106369, fake loss 0.17549359798431396\tGenerator: loss 1.098632574081421\n",
            "Epoch 686, batch 1/9:\tDiscriminator: real loss 0.22025951743125916, fake loss 0.19301480054855347\tGenerator: loss 1.0112619400024414\n",
            "Epoch 687, batch 1/9:\tDiscriminator: real loss 0.18650643527507782, fake loss 0.19170919060707092\tGenerator: loss 1.0911550521850586\n",
            "Epoch 688, batch 1/9:\tDiscriminator: real loss 0.23299194872379303, fake loss 0.1699293702840805\tGenerator: loss 1.1078386306762695\n",
            "Epoch 689, batch 1/9:\tDiscriminator: real loss 0.22703081369400024, fake loss 0.18853268027305603\tGenerator: loss 1.0986769199371338\n",
            "Epoch 690, batch 1/9:\tDiscriminator: real loss 0.2473260909318924, fake loss 0.1983247846364975\tGenerator: loss 1.0699517726898193\n",
            "Epoch 691, batch 1/9:\tDiscriminator: real loss 0.20214655995368958, fake loss 0.20599403977394104\tGenerator: loss 1.0136586427688599\n",
            "Epoch 692, batch 1/9:\tDiscriminator: real loss 0.19797047972679138, fake loss 0.16646048426628113\tGenerator: loss 0.9733060598373413\n",
            "Epoch 693, batch 1/9:\tDiscriminator: real loss 0.22150075435638428, fake loss 0.199480339884758\tGenerator: loss 1.0257607698440552\n",
            "Epoch 694, batch 1/9:\tDiscriminator: real loss 0.1688431352376938, fake loss 0.20169931650161743\tGenerator: loss 1.0527596473693848\n",
            "Epoch 695, batch 1/9:\tDiscriminator: real loss 0.19371330738067627, fake loss 0.19098487496376038\tGenerator: loss 1.0016573667526245\n",
            "Epoch 696, batch 1/9:\tDiscriminator: real loss 0.21017484366893768, fake loss 0.20109480619430542\tGenerator: loss 0.9235876798629761\n",
            "Epoch 697, batch 1/9:\tDiscriminator: real loss 0.2151067554950714, fake loss 0.15865051746368408\tGenerator: loss 0.9557033777236938\n",
            "Epoch 698, batch 1/9:\tDiscriminator: real loss 0.15805906057357788, fake loss 0.15711519122123718\tGenerator: loss 1.1127592325210571\n",
            "Epoch 699, batch 1/9:\tDiscriminator: real loss 0.16664890944957733, fake loss 0.18617099523544312\tGenerator: loss 0.9571565389633179\n",
            "Epoch 700, batch 1/9:\tDiscriminator: real loss 0.14656075835227966, fake loss 0.17852304875850677\tGenerator: loss 1.050258755683899\n",
            "Epoch 701, batch 1/9:\tDiscriminator: real loss 0.16061799228191376, fake loss 0.17201954126358032\tGenerator: loss 1.0087531805038452\n",
            "Epoch 702, batch 1/9:\tDiscriminator: real loss 0.16672712564468384, fake loss 0.16547110676765442\tGenerator: loss 0.9904056787490845\n",
            "Epoch 703, batch 1/9:\tDiscriminator: real loss 0.1551552712917328, fake loss 0.1919434666633606\tGenerator: loss 1.0108290910720825\n",
            "Epoch 704, batch 1/9:\tDiscriminator: real loss 0.17034494876861572, fake loss 0.19795367121696472\tGenerator: loss 1.0335114002227783\n",
            "Epoch 705, batch 1/9:\tDiscriminator: real loss 0.18737882375717163, fake loss 0.18489986658096313\tGenerator: loss 0.994316816329956\n",
            "Epoch 706, batch 1/9:\tDiscriminator: real loss 0.17461177706718445, fake loss 0.18810723721981049\tGenerator: loss 1.1131033897399902\n",
            "Epoch 707, batch 1/9:\tDiscriminator: real loss 0.1625908762216568, fake loss 0.1736631840467453\tGenerator: loss 1.151561975479126\n",
            "Epoch 708, batch 1/9:\tDiscriminator: real loss 0.16942310333251953, fake loss 0.16507653892040253\tGenerator: loss 1.0787979364395142\n",
            "Epoch 709, batch 1/9:\tDiscriminator: real loss 0.15848308801651, fake loss 0.21728046238422394\tGenerator: loss 1.121061086654663\n",
            "Epoch 710, batch 1/9:\tDiscriminator: real loss 0.175352081656456, fake loss 0.13817965984344482\tGenerator: loss 1.100559949874878\n",
            "Epoch 711, batch 1/9:\tDiscriminator: real loss 0.14175428450107574, fake loss 0.16232943534851074\tGenerator: loss 1.1404786109924316\n",
            "Epoch 712, batch 1/9:\tDiscriminator: real loss 0.1667248010635376, fake loss 0.15826573967933655\tGenerator: loss 1.1607325077056885\n",
            "Epoch 713, batch 1/9:\tDiscriminator: real loss 0.17341870069503784, fake loss 0.18889403343200684\tGenerator: loss 1.080389380455017\n",
            "Epoch 714, batch 1/9:\tDiscriminator: real loss 0.17009511590003967, fake loss 0.19261017441749573\tGenerator: loss 1.0318282842636108\n",
            "Epoch 715, batch 1/9:\tDiscriminator: real loss 0.1989719271659851, fake loss 0.26716217398643494\tGenerator: loss 0.8769620060920715\n",
            "Epoch 716, batch 1/9:\tDiscriminator: real loss 0.21300365030765533, fake loss 0.1326640397310257\tGenerator: loss 1.2638200521469116\n",
            "Epoch 717, batch 1/9:\tDiscriminator: real loss 0.20736199617385864, fake loss 0.21127969026565552\tGenerator: loss 1.109114646911621\n",
            "Epoch 718, batch 1/9:\tDiscriminator: real loss 0.17454317212104797, fake loss 0.1675090193748474\tGenerator: loss 1.1891095638275146\n",
            "Epoch 719, batch 1/9:\tDiscriminator: real loss 0.16172727942466736, fake loss 0.16189777851104736\tGenerator: loss 1.128488540649414\n",
            "Epoch 720, batch 1/9:\tDiscriminator: real loss 0.15899360179901123, fake loss 0.16505016386508942\tGenerator: loss 1.1447618007659912\n",
            "Epoch 721, batch 1/9:\tDiscriminator: real loss 0.14349627494812012, fake loss 0.12826944887638092\tGenerator: loss 1.1673979759216309\n",
            "Epoch 722, batch 1/9:\tDiscriminator: real loss 0.17456138134002686, fake loss 0.12800583243370056\tGenerator: loss 1.1808383464813232\n",
            "Epoch 723, batch 1/9:\tDiscriminator: real loss 0.13618943095207214, fake loss 0.1623237580060959\tGenerator: loss 1.1612550020217896\n",
            "Epoch 724, batch 1/9:\tDiscriminator: real loss 0.1299406886100769, fake loss 0.16953851282596588\tGenerator: loss 1.2323822975158691\n",
            "Epoch 725, batch 1/9:\tDiscriminator: real loss 0.17166781425476074, fake loss 0.23071807622909546\tGenerator: loss 1.0269709825515747\n",
            "Epoch 726, batch 1/9:\tDiscriminator: real loss 0.17347833514213562, fake loss 0.21317091584205627\tGenerator: loss 0.9773111343383789\n",
            "Epoch 727, batch 1/9:\tDiscriminator: real loss 0.24349656701087952, fake loss 0.16603824496269226\tGenerator: loss 1.1080453395843506\n",
            "Epoch 728, batch 1/9:\tDiscriminator: real loss 0.2025354504585266, fake loss 0.13975292444229126\tGenerator: loss 1.1772642135620117\n",
            "Epoch 729, batch 1/9:\tDiscriminator: real loss 0.1994991898536682, fake loss 0.16011472046375275\tGenerator: loss 1.0676240921020508\n",
            "Epoch 730, batch 1/9:\tDiscriminator: real loss 0.21950310468673706, fake loss 0.18733401596546173\tGenerator: loss 1.1279550790786743\n",
            "Epoch 731, batch 1/9:\tDiscriminator: real loss 0.19560924172401428, fake loss 0.17208346724510193\tGenerator: loss 1.0479035377502441\n",
            "Epoch 732, batch 1/9:\tDiscriminator: real loss 0.18252666294574738, fake loss 0.1800013780593872\tGenerator: loss 1.007796049118042\n",
            "Epoch 733, batch 1/9:\tDiscriminator: real loss 0.17765691876411438, fake loss 0.17050451040267944\tGenerator: loss 1.0819988250732422\n",
            "Epoch 734, batch 1/9:\tDiscriminator: real loss 0.17093601822853088, fake loss 0.17531844973564148\tGenerator: loss 1.0708813667297363\n",
            "Epoch 735, batch 1/9:\tDiscriminator: real loss 0.15432725846767426, fake loss 0.1852884292602539\tGenerator: loss 1.0458239316940308\n",
            "Epoch 736, batch 1/9:\tDiscriminator: real loss 0.15555070340633392, fake loss 0.18173670768737793\tGenerator: loss 0.9889202117919922\n",
            "Epoch 737, batch 1/9:\tDiscriminator: real loss 0.17628547549247742, fake loss 0.16000314056873322\tGenerator: loss 1.0457611083984375\n",
            "Epoch 738, batch 1/9:\tDiscriminator: real loss 0.1565782129764557, fake loss 0.15817780792713165\tGenerator: loss 1.039413332939148\n",
            "Epoch 739, batch 1/9:\tDiscriminator: real loss 0.15815703570842743, fake loss 0.1440877616405487\tGenerator: loss 1.1351406574249268\n",
            "Epoch 740, batch 1/9:\tDiscriminator: real loss 0.12995219230651855, fake loss 0.15652669966220856\tGenerator: loss 1.1152691841125488\n",
            "Epoch 741, batch 1/9:\tDiscriminator: real loss 0.1369968056678772, fake loss 0.15650349855422974\tGenerator: loss 1.0390640497207642\n",
            "Epoch 742, batch 1/9:\tDiscriminator: real loss 0.1573900431394577, fake loss 0.15916725993156433\tGenerator: loss 1.207920789718628\n",
            "Epoch 743, batch 1/9:\tDiscriminator: real loss 0.15056711435317993, fake loss 0.15565451979637146\tGenerator: loss 1.0699013471603394\n",
            "Epoch 744, batch 1/9:\tDiscriminator: real loss 0.18111911416053772, fake loss 0.1829424649477005\tGenerator: loss 1.2074248790740967\n",
            "Epoch 745, batch 1/9:\tDiscriminator: real loss 0.17892801761627197, fake loss 0.13704873621463776\tGenerator: loss 1.2039687633514404\n",
            "Epoch 746, batch 1/9:\tDiscriminator: real loss 0.1462550014257431, fake loss 0.16586068272590637\tGenerator: loss 1.2297536134719849\n",
            "Epoch 747, batch 1/9:\tDiscriminator: real loss 0.13072410225868225, fake loss 0.14529219269752502\tGenerator: loss 1.250645399093628\n",
            "Epoch 748, batch 1/9:\tDiscriminator: real loss 0.14818722009658813, fake loss 0.1597151756286621\tGenerator: loss 1.0773998498916626\n",
            "Epoch 749, batch 1/9:\tDiscriminator: real loss 0.13919509947299957, fake loss 0.15370944142341614\tGenerator: loss 1.2419662475585938\n",
            "Epoch 750, batch 1/9:\tDiscriminator: real loss 0.1600690335035324, fake loss 0.15223518013954163\tGenerator: loss 1.148366093635559\n",
            "Epoch 751, batch 1/9:\tDiscriminator: real loss 0.17363907396793365, fake loss 0.14102384448051453\tGenerator: loss 1.144639253616333\n",
            "Epoch 752, batch 1/9:\tDiscriminator: real loss 0.15643221139907837, fake loss 0.16711431741714478\tGenerator: loss 1.1730225086212158\n",
            "Epoch 753, batch 1/9:\tDiscriminator: real loss 0.17146800458431244, fake loss 0.22286739945411682\tGenerator: loss 1.0688321590423584\n",
            "Epoch 754, batch 1/9:\tDiscriminator: real loss 0.21652808785438538, fake loss 0.1868460476398468\tGenerator: loss 1.2389111518859863\n",
            "Epoch 755, batch 1/9:\tDiscriminator: real loss 0.22753211855888367, fake loss 0.17141735553741455\tGenerator: loss 1.033791184425354\n",
            "Epoch 756, batch 1/9:\tDiscriminator: real loss 0.18611766397953033, fake loss 0.14497525990009308\tGenerator: loss 1.0471745729446411\n",
            "Epoch 757, batch 1/9:\tDiscriminator: real loss 0.18679621815681458, fake loss 0.15870222449302673\tGenerator: loss 1.1302340030670166\n",
            "Epoch 758, batch 1/9:\tDiscriminator: real loss 0.2038014531135559, fake loss 0.22257816791534424\tGenerator: loss 0.905622661113739\n",
            "Epoch 759, batch 1/9:\tDiscriminator: real loss 0.2336488962173462, fake loss 0.1865789145231247\tGenerator: loss 1.0470224618911743\n",
            "Epoch 760, batch 1/9:\tDiscriminator: real loss 0.17588837444782257, fake loss 0.17500901222229004\tGenerator: loss 1.0546574592590332\n",
            "Epoch 761, batch 1/9:\tDiscriminator: real loss 0.17683210968971252, fake loss 0.1692456752061844\tGenerator: loss 1.0913344621658325\n",
            "Epoch 762, batch 1/9:\tDiscriminator: real loss 0.16208422183990479, fake loss 0.15905189514160156\tGenerator: loss 1.0631498098373413\n",
            "Epoch 763, batch 1/9:\tDiscriminator: real loss 0.1810382604598999, fake loss 0.1564929038286209\tGenerator: loss 1.0162023305892944\n",
            "Epoch 764, batch 1/9:\tDiscriminator: real loss 0.17137068510055542, fake loss 0.1631898134946823\tGenerator: loss 1.0742356777191162\n",
            "Epoch 765, batch 1/9:\tDiscriminator: real loss 0.21671168506145477, fake loss 0.24809440970420837\tGenerator: loss 0.8884944915771484\n",
            "Epoch 766, batch 1/9:\tDiscriminator: real loss 0.2559937536716461, fake loss 0.14469291269779205\tGenerator: loss 1.1243685483932495\n",
            "Epoch 767, batch 1/9:\tDiscriminator: real loss 0.2176828682422638, fake loss 0.17773261666297913\tGenerator: loss 1.145113468170166\n",
            "Epoch 768, batch 1/9:\tDiscriminator: real loss 0.23951968550682068, fake loss 0.19570845365524292\tGenerator: loss 1.1330796480178833\n",
            "Epoch 769, batch 1/9:\tDiscriminator: real loss 0.2190389782190323, fake loss 0.14955686032772064\tGenerator: loss 1.051722764968872\n",
            "Epoch 770, batch 1/9:\tDiscriminator: real loss 0.18741627037525177, fake loss 0.20339375734329224\tGenerator: loss 1.0199605226516724\n",
            "Epoch 771, batch 1/9:\tDiscriminator: real loss 0.16336962580680847, fake loss 0.2016788274049759\tGenerator: loss 0.9959762692451477\n",
            "Epoch 772, batch 1/9:\tDiscriminator: real loss 0.2102164924144745, fake loss 0.20552295446395874\tGenerator: loss 0.9860004186630249\n",
            "Epoch 773, batch 1/9:\tDiscriminator: real loss 0.16786731779575348, fake loss 0.17758938670158386\tGenerator: loss 1.073403239250183\n",
            "Epoch 774, batch 1/9:\tDiscriminator: real loss 0.16831360757350922, fake loss 0.1553376019001007\tGenerator: loss 1.0909751653671265\n",
            "Epoch 775, batch 1/9:\tDiscriminator: real loss 0.17376604676246643, fake loss 0.14240284264087677\tGenerator: loss 1.061295509338379\n",
            "Epoch 776, batch 1/9:\tDiscriminator: real loss 0.173604816198349, fake loss 0.18787342309951782\tGenerator: loss 0.9908788204193115\n",
            "Epoch 777, batch 1/9:\tDiscriminator: real loss 0.1803293526172638, fake loss 0.17835655808448792\tGenerator: loss 1.0457805395126343\n",
            "Epoch 778, batch 1/9:\tDiscriminator: real loss 0.16363686323165894, fake loss 0.1742561161518097\tGenerator: loss 1.115318775177002\n",
            "Epoch 779, batch 1/9:\tDiscriminator: real loss 0.18113788962364197, fake loss 0.18704082071781158\tGenerator: loss 0.9825220108032227\n",
            "Epoch 780, batch 1/9:\tDiscriminator: real loss 0.17173686623573303, fake loss 0.19512352347373962\tGenerator: loss 0.9622254371643066\n",
            "Epoch 781, batch 1/9:\tDiscriminator: real loss 0.24085715413093567, fake loss 0.18288446962833405\tGenerator: loss 1.1284658908843994\n",
            "Epoch 782, batch 1/9:\tDiscriminator: real loss 0.22872072458267212, fake loss 0.1801498681306839\tGenerator: loss 0.9402380585670471\n",
            "Epoch 783, batch 1/9:\tDiscriminator: real loss 0.21536439657211304, fake loss 0.19195851683616638\tGenerator: loss 0.9317246675491333\n",
            "Epoch 784, batch 1/9:\tDiscriminator: real loss 0.18595720827579498, fake loss 0.18125560879707336\tGenerator: loss 1.0261121988296509\n",
            "Epoch 785, batch 1/9:\tDiscriminator: real loss 0.1641138345003128, fake loss 0.15565702319145203\tGenerator: loss 1.0440030097961426\n",
            "Epoch 786, batch 1/9:\tDiscriminator: real loss 0.17246109247207642, fake loss 0.15248383581638336\tGenerator: loss 1.0455920696258545\n",
            "Epoch 787, batch 1/9:\tDiscriminator: real loss 0.16415810585021973, fake loss 0.1688196212053299\tGenerator: loss 1.1057831048965454\n",
            "Epoch 788, batch 1/9:\tDiscriminator: real loss 0.15230973064899445, fake loss 0.16692209243774414\tGenerator: loss 1.0094130039215088\n",
            "Epoch 789, batch 1/9:\tDiscriminator: real loss 0.15957383811473846, fake loss 0.1492091417312622\tGenerator: loss 1.0325895547866821\n",
            "Epoch 790, batch 1/9:\tDiscriminator: real loss 0.1595693826675415, fake loss 0.16493090987205505\tGenerator: loss 0.981670618057251\n",
            "Epoch 791, batch 1/9:\tDiscriminator: real loss 0.15060248970985413, fake loss 0.2010682076215744\tGenerator: loss 1.0662314891815186\n",
            "Epoch 792, batch 1/9:\tDiscriminator: real loss 0.17172792553901672, fake loss 0.21037952601909637\tGenerator: loss 0.9754367470741272\n",
            "Epoch 793, batch 1/9:\tDiscriminator: real loss 0.1613190472126007, fake loss 0.20343513786792755\tGenerator: loss 0.9291051030158997\n",
            "Epoch 794, batch 1/9:\tDiscriminator: real loss 0.19444361329078674, fake loss 0.16743798553943634\tGenerator: loss 1.0157511234283447\n",
            "Epoch 795, batch 1/9:\tDiscriminator: real loss 0.1765451282262802, fake loss 0.1646745204925537\tGenerator: loss 1.1130123138427734\n",
            "Epoch 796, batch 1/9:\tDiscriminator: real loss 0.17393237352371216, fake loss 0.15589244663715363\tGenerator: loss 1.122329592704773\n",
            "Epoch 797, batch 1/9:\tDiscriminator: real loss 0.17194199562072754, fake loss 0.143136128783226\tGenerator: loss 1.0535438060760498\n",
            "Epoch 798, batch 1/9:\tDiscriminator: real loss 0.15733850002288818, fake loss 0.1559600681066513\tGenerator: loss 1.1305584907531738\n",
            "Epoch 799, batch 1/9:\tDiscriminator: real loss 0.13190609216690063, fake loss 0.15342003107070923\tGenerator: loss 1.0456407070159912\n",
            "Epoch 800, batch 1/9:\tDiscriminator: real loss 0.14086513221263885, fake loss 0.1386345624923706\tGenerator: loss 1.086115837097168\n",
            "Epoch 801, batch 1/9:\tDiscriminator: real loss 0.1395731270313263, fake loss 0.15176695585250854\tGenerator: loss 1.1557642221450806\n",
            "Epoch 802, batch 1/9:\tDiscriminator: real loss 0.16645807027816772, fake loss 0.19357961416244507\tGenerator: loss 0.9671460390090942\n",
            "Epoch 803, batch 1/9:\tDiscriminator: real loss 0.14633052051067352, fake loss 0.15315650403499603\tGenerator: loss 1.1452698707580566\n",
            "Epoch 804, batch 1/9:\tDiscriminator: real loss 0.13317054510116577, fake loss 0.16365587711334229\tGenerator: loss 1.13520348072052\n",
            "Epoch 805, batch 1/9:\tDiscriminator: real loss 0.13034497201442719, fake loss 0.16833554208278656\tGenerator: loss 1.1958130598068237\n",
            "Epoch 806, batch 1/9:\tDiscriminator: real loss 0.12645407021045685, fake loss 0.1503341794013977\tGenerator: loss 1.120455026626587\n",
            "Epoch 807, batch 1/9:\tDiscriminator: real loss 0.1362789124250412, fake loss 0.16448359191417694\tGenerator: loss 1.2313082218170166\n",
            "Epoch 808, batch 1/9:\tDiscriminator: real loss 0.17389193177223206, fake loss 0.22775301337242126\tGenerator: loss 0.9740740656852722\n",
            "Epoch 809, batch 1/9:\tDiscriminator: real loss 0.20198465883731842, fake loss 0.1720130741596222\tGenerator: loss 1.0857475996017456\n",
            "Epoch 810, batch 1/9:\tDiscriminator: real loss 0.19260069727897644, fake loss 0.1307760328054428\tGenerator: loss 1.1530402898788452\n",
            "Epoch 811, batch 1/9:\tDiscriminator: real loss 0.1718020737171173, fake loss 0.145994633436203\tGenerator: loss 1.094266414642334\n",
            "Epoch 812, batch 1/9:\tDiscriminator: real loss 0.15742790699005127, fake loss 0.1420055329799652\tGenerator: loss 1.1336278915405273\n",
            "Epoch 813, batch 1/9:\tDiscriminator: real loss 0.15515434741973877, fake loss 0.16587062180042267\tGenerator: loss 1.1144037246704102\n",
            "Epoch 814, batch 1/9:\tDiscriminator: real loss 0.1823345273733139, fake loss 0.14466522634029388\tGenerator: loss 1.193366289138794\n",
            "Epoch 815, batch 1/9:\tDiscriminator: real loss 0.20567187666893005, fake loss 0.16784268617630005\tGenerator: loss 1.0663254261016846\n",
            "Epoch 816, batch 1/9:\tDiscriminator: real loss 0.2627480626106262, fake loss 0.15115566551685333\tGenerator: loss 1.255758285522461\n",
            "Epoch 817, batch 1/9:\tDiscriminator: real loss 0.1934938132762909, fake loss 0.1955360472202301\tGenerator: loss 1.0380299091339111\n",
            "Epoch 818, batch 1/9:\tDiscriminator: real loss 0.17915204167366028, fake loss 0.15243303775787354\tGenerator: loss 1.0251522064208984\n",
            "Epoch 819, batch 1/9:\tDiscriminator: real loss 0.20993830263614655, fake loss 0.1690637618303299\tGenerator: loss 1.0676636695861816\n",
            "Epoch 820, batch 1/9:\tDiscriminator: real loss 0.15279439091682434, fake loss 0.15155890583992004\tGenerator: loss 1.1258314847946167\n",
            "Epoch 821, batch 1/9:\tDiscriminator: real loss 0.1456609070301056, fake loss 0.14519956707954407\tGenerator: loss 1.0629031658172607\n",
            "Epoch 822, batch 1/9:\tDiscriminator: real loss 0.21207809448242188, fake loss 0.20776274800300598\tGenerator: loss 1.0013046264648438\n",
            "Epoch 823, batch 1/9:\tDiscriminator: real loss 0.2062709927558899, fake loss 0.18463313579559326\tGenerator: loss 1.0874214172363281\n",
            "Epoch 824, batch 1/9:\tDiscriminator: real loss 0.21123246848583221, fake loss 0.19327512383460999\tGenerator: loss 1.0759384632110596\n",
            "Epoch 825, batch 1/9:\tDiscriminator: real loss 0.18667057156562805, fake loss 0.15268895030021667\tGenerator: loss 1.0793834924697876\n",
            "Epoch 826, batch 1/9:\tDiscriminator: real loss 0.20764648914337158, fake loss 0.19258375465869904\tGenerator: loss 1.025252342224121\n",
            "Epoch 827, batch 1/9:\tDiscriminator: real loss 0.16821786761283875, fake loss 0.18170791864395142\tGenerator: loss 1.157914161682129\n",
            "Epoch 828, batch 1/9:\tDiscriminator: real loss 0.1772158443927765, fake loss 0.15577003359794617\tGenerator: loss 1.0990062952041626\n",
            "Epoch 829, batch 1/9:\tDiscriminator: real loss 0.1737738400697708, fake loss 0.17388281226158142\tGenerator: loss 1.0577607154846191\n",
            "Epoch 830, batch 1/9:\tDiscriminator: real loss 0.170389324426651, fake loss 0.1477590948343277\tGenerator: loss 1.0957189798355103\n",
            "Epoch 831, batch 1/9:\tDiscriminator: real loss 0.16081465780735016, fake loss 0.15509656071662903\tGenerator: loss 1.117077112197876\n",
            "Epoch 832, batch 1/9:\tDiscriminator: real loss 0.1587476134300232, fake loss 0.16243359446525574\tGenerator: loss 1.0855426788330078\n",
            "Epoch 833, batch 1/9:\tDiscriminator: real loss 0.1468518078327179, fake loss 0.14439232647418976\tGenerator: loss 1.1467781066894531\n",
            "Epoch 834, batch 1/9:\tDiscriminator: real loss 0.17428848147392273, fake loss 0.15607991814613342\tGenerator: loss 1.0782617330551147\n",
            "Epoch 835, batch 1/9:\tDiscriminator: real loss 0.18910163640975952, fake loss 0.16483086347579956\tGenerator: loss 1.0371614694595337\n",
            "Epoch 836, batch 1/9:\tDiscriminator: real loss 0.1925337314605713, fake loss 0.1913990080356598\tGenerator: loss 1.0571811199188232\n",
            "Epoch 837, batch 1/9:\tDiscriminator: real loss 0.16815811395645142, fake loss 0.19582071900367737\tGenerator: loss 1.1205416917800903\n",
            "Epoch 838, batch 1/9:\tDiscriminator: real loss 0.17095404863357544, fake loss 0.15412941575050354\tGenerator: loss 1.1200257539749146\n",
            "Epoch 839, batch 1/9:\tDiscriminator: real loss 0.1587616205215454, fake loss 0.16320271790027618\tGenerator: loss 1.0692254304885864\n",
            "Epoch 840, batch 1/9:\tDiscriminator: real loss 0.1599491983652115, fake loss 0.17231670022010803\tGenerator: loss 0.9698238372802734\n",
            "Epoch 841, batch 1/9:\tDiscriminator: real loss 0.16815099120140076, fake loss 0.13414281606674194\tGenerator: loss 1.1963624954223633\n",
            "Epoch 842, batch 1/9:\tDiscriminator: real loss 0.14897719025611877, fake loss 0.14564462006092072\tGenerator: loss 1.1468762159347534\n",
            "Epoch 843, batch 1/9:\tDiscriminator: real loss 0.19882243871688843, fake loss 0.19072487950325012\tGenerator: loss 1.0085995197296143\n",
            "Epoch 844, batch 1/9:\tDiscriminator: real loss 0.16340097784996033, fake loss 0.1574486494064331\tGenerator: loss 1.0974647998809814\n",
            "Epoch 845, batch 1/9:\tDiscriminator: real loss 0.1592329740524292, fake loss 0.15656161308288574\tGenerator: loss 1.010953664779663\n",
            "Epoch 846, batch 1/9:\tDiscriminator: real loss 0.16129866242408752, fake loss 0.17179261147975922\tGenerator: loss 1.0267913341522217\n",
            "Epoch 847, batch 1/9:\tDiscriminator: real loss 0.1376756876707077, fake loss 0.1546812653541565\tGenerator: loss 1.1809805631637573\n",
            "Epoch 848, batch 1/9:\tDiscriminator: real loss 0.1636340171098709, fake loss 0.15086543560028076\tGenerator: loss 1.2429074048995972\n",
            "Epoch 849, batch 1/9:\tDiscriminator: real loss 0.13877353072166443, fake loss 0.1542479693889618\tGenerator: loss 1.0532221794128418\n",
            "Epoch 850, batch 1/9:\tDiscriminator: real loss 0.11304943263530731, fake loss 0.1415850967168808\tGenerator: loss 1.2715134620666504\n",
            "Epoch 851, batch 1/9:\tDiscriminator: real loss 0.13679386675357819, fake loss 0.14342793822288513\tGenerator: loss 1.1210613250732422\n",
            "Epoch 852, batch 1/9:\tDiscriminator: real loss 0.12801922857761383, fake loss 0.18975748121738434\tGenerator: loss 1.156081199645996\n",
            "Epoch 853, batch 1/9:\tDiscriminator: real loss 0.170045867562294, fake loss 0.16252028942108154\tGenerator: loss 1.1885474920272827\n",
            "Epoch 854, batch 1/9:\tDiscriminator: real loss 0.14572493731975555, fake loss 0.15828415751457214\tGenerator: loss 1.213623285293579\n",
            "Epoch 855, batch 1/9:\tDiscriminator: real loss 0.1428426206111908, fake loss 0.15919604897499084\tGenerator: loss 1.1463273763656616\n",
            "Epoch 856, batch 1/9:\tDiscriminator: real loss 0.14923787117004395, fake loss 0.16904360055923462\tGenerator: loss 1.1111561059951782\n",
            "Epoch 857, batch 1/9:\tDiscriminator: real loss 0.15844866633415222, fake loss 0.1698475182056427\tGenerator: loss 1.1448194980621338\n",
            "Epoch 858, batch 1/9:\tDiscriminator: real loss 0.17335648834705353, fake loss 0.17219822108745575\tGenerator: loss 1.1013363599777222\n",
            "Epoch 859, batch 1/9:\tDiscriminator: real loss 0.2231590896844864, fake loss 0.22653204202651978\tGenerator: loss 0.9782359004020691\n",
            "Epoch 860, batch 1/9:\tDiscriminator: real loss 0.21416643261909485, fake loss 0.17497222125530243\tGenerator: loss 0.9709762334823608\n",
            "Epoch 861, batch 1/9:\tDiscriminator: real loss 0.23881147801876068, fake loss 0.16893024742603302\tGenerator: loss 1.0979223251342773\n",
            "Epoch 862, batch 1/9:\tDiscriminator: real loss 0.1895981878042221, fake loss 0.15977995097637177\tGenerator: loss 1.0716005563735962\n",
            "Epoch 863, batch 1/9:\tDiscriminator: real loss 0.19327399134635925, fake loss 0.1733061969280243\tGenerator: loss 1.0823590755462646\n",
            "Epoch 864, batch 1/9:\tDiscriminator: real loss 0.1507333368062973, fake loss 0.15422025322914124\tGenerator: loss 1.14604651927948\n",
            "Epoch 865, batch 1/9:\tDiscriminator: real loss 0.1650114357471466, fake loss 0.17497719824314117\tGenerator: loss 1.0923123359680176\n",
            "Epoch 866, batch 1/9:\tDiscriminator: real loss 0.16270405054092407, fake loss 0.17571286857128143\tGenerator: loss 1.1433385610580444\n",
            "Epoch 867, batch 1/9:\tDiscriminator: real loss 0.17363299429416656, fake loss 0.1800224483013153\tGenerator: loss 1.094845175743103\n",
            "Epoch 868, batch 1/9:\tDiscriminator: real loss 0.20569902658462524, fake loss 0.2090359479188919\tGenerator: loss 1.095408320426941\n",
            "Epoch 869, batch 1/9:\tDiscriminator: real loss 0.17429345846176147, fake loss 0.16232481598854065\tGenerator: loss 1.2822381258010864\n",
            "Epoch 870, batch 1/9:\tDiscriminator: real loss 0.15757916867733002, fake loss 0.13507091999053955\tGenerator: loss 1.1457735300064087\n",
            "Epoch 871, batch 1/9:\tDiscriminator: real loss 0.15205591917037964, fake loss 0.1478613317012787\tGenerator: loss 1.2150224447250366\n",
            "Epoch 872, batch 1/9:\tDiscriminator: real loss 0.14453695714473724, fake loss 0.1740848571062088\tGenerator: loss 1.1222729682922363\n",
            "Epoch 873, batch 1/9:\tDiscriminator: real loss 0.1353529691696167, fake loss 0.16406145691871643\tGenerator: loss 1.1823029518127441\n",
            "Epoch 874, batch 1/9:\tDiscriminator: real loss 0.1780739724636078, fake loss 0.1612347960472107\tGenerator: loss 1.1478697061538696\n",
            "Epoch 875, batch 1/9:\tDiscriminator: real loss 0.2514163553714752, fake loss 0.24190500378608704\tGenerator: loss 0.8653169870376587\n",
            "Epoch 876, batch 1/9:\tDiscriminator: real loss 0.23573297262191772, fake loss 0.16890758275985718\tGenerator: loss 1.0443158149719238\n",
            "Epoch 877, batch 1/9:\tDiscriminator: real loss 0.2477811574935913, fake loss 0.1584668606519699\tGenerator: loss 1.1045030355453491\n",
            "Epoch 878, batch 1/9:\tDiscriminator: real loss 0.27164310216903687, fake loss 0.18112097680568695\tGenerator: loss 1.0858948230743408\n",
            "Epoch 879, batch 1/9:\tDiscriminator: real loss 0.30909156799316406, fake loss 0.16072717308998108\tGenerator: loss 1.2068004608154297\n",
            "Epoch 880, batch 1/9:\tDiscriminator: real loss 0.3112391233444214, fake loss 0.20214436948299408\tGenerator: loss 1.1627907752990723\n",
            "Epoch 881, batch 1/9:\tDiscriminator: real loss 0.23018410801887512, fake loss 0.1725100576877594\tGenerator: loss 1.265089750289917\n",
            "Epoch 882, batch 1/9:\tDiscriminator: real loss 0.19009166955947876, fake loss 0.1731095016002655\tGenerator: loss 1.1296876668930054\n",
            "Epoch 883, batch 1/9:\tDiscriminator: real loss 0.19016483426094055, fake loss 0.16024789214134216\tGenerator: loss 1.0614709854125977\n",
            "Epoch 884, batch 1/9:\tDiscriminator: real loss 0.1729438602924347, fake loss 0.16916024684906006\tGenerator: loss 0.9898103475570679\n",
            "Epoch 885, batch 1/9:\tDiscriminator: real loss 0.18606743216514587, fake loss 0.16662825644016266\tGenerator: loss 1.0651602745056152\n",
            "Epoch 886, batch 1/9:\tDiscriminator: real loss 0.1435878574848175, fake loss 0.14292176067829132\tGenerator: loss 1.1466825008392334\n",
            "Epoch 887, batch 1/9:\tDiscriminator: real loss 0.1576996147632599, fake loss 0.20629870891571045\tGenerator: loss 0.9551112651824951\n",
            "Epoch 888, batch 1/9:\tDiscriminator: real loss 0.22535206377506256, fake loss 0.18813258409500122\tGenerator: loss 0.9726929664611816\n",
            "Epoch 889, batch 1/9:\tDiscriminator: real loss 0.24374665319919586, fake loss 0.1888393759727478\tGenerator: loss 1.0107080936431885\n",
            "Epoch 890, batch 1/9:\tDiscriminator: real loss 0.21115605533123016, fake loss 0.17044885456562042\tGenerator: loss 1.0626327991485596\n",
            "Epoch 891, batch 1/9:\tDiscriminator: real loss 0.19757506251335144, fake loss 0.16349445283412933\tGenerator: loss 1.0429922342300415\n",
            "Epoch 892, batch 1/9:\tDiscriminator: real loss 0.15798917412757874, fake loss 0.18725326657295227\tGenerator: loss 1.0385507345199585\n",
            "Epoch 893, batch 1/9:\tDiscriminator: real loss 0.14640943706035614, fake loss 0.18078041076660156\tGenerator: loss 1.0650802850723267\n",
            "Epoch 894, batch 1/9:\tDiscriminator: real loss 0.14444269239902496, fake loss 0.16456228494644165\tGenerator: loss 1.0774650573730469\n",
            "Epoch 895, batch 1/9:\tDiscriminator: real loss 0.12230616807937622, fake loss 0.15112221240997314\tGenerator: loss 1.0812650918960571\n",
            "Epoch 896, batch 1/9:\tDiscriminator: real loss 0.14478695392608643, fake loss 0.15389752388000488\tGenerator: loss 1.106855869293213\n",
            "Epoch 897, batch 1/9:\tDiscriminator: real loss 0.12160862237215042, fake loss 0.13335999846458435\tGenerator: loss 1.2333227396011353\n",
            "Epoch 898, batch 1/9:\tDiscriminator: real loss 0.15285751223564148, fake loss 0.14079150557518005\tGenerator: loss 1.2568085193634033\n",
            "Epoch 899, batch 1/9:\tDiscriminator: real loss 0.131020188331604, fake loss 0.14512008428573608\tGenerator: loss 1.1616308689117432\n",
            "Epoch 900, batch 1/9:\tDiscriminator: real loss 0.1274556815624237, fake loss 0.15029452741146088\tGenerator: loss 1.129594087600708\n",
            "Epoch 901, batch 1/9:\tDiscriminator: real loss 0.14130161702632904, fake loss 0.16994336247444153\tGenerator: loss 1.2151169776916504\n",
            "Epoch 902, batch 1/9:\tDiscriminator: real loss 0.16389447450637817, fake loss 0.2146451473236084\tGenerator: loss 1.0288838148117065\n",
            "Epoch 903, batch 1/9:\tDiscriminator: real loss 0.21401280164718628, fake loss 0.17349214851856232\tGenerator: loss 1.157910943031311\n",
            "Epoch 904, batch 1/9:\tDiscriminator: real loss 0.18109393119812012, fake loss 0.13233652710914612\tGenerator: loss 1.1562060117721558\n",
            "Epoch 905, batch 1/9:\tDiscriminator: real loss 0.1657937467098236, fake loss 0.15011601150035858\tGenerator: loss 1.0806605815887451\n",
            "Epoch 906, batch 1/9:\tDiscriminator: real loss 0.1659705638885498, fake loss 0.1704706996679306\tGenerator: loss 0.9870808124542236\n",
            "Epoch 907, batch 1/9:\tDiscriminator: real loss 0.18591603636741638, fake loss 0.17533884942531586\tGenerator: loss 1.0344263315200806\n",
            "Epoch 908, batch 1/9:\tDiscriminator: real loss 0.21047760546207428, fake loss 0.1585053950548172\tGenerator: loss 1.1691358089447021\n",
            "Epoch 909, batch 1/9:\tDiscriminator: real loss 0.18180014193058014, fake loss 0.14822158217430115\tGenerator: loss 1.0526669025421143\n",
            "Epoch 910, batch 1/9:\tDiscriminator: real loss 0.14069265127182007, fake loss 0.15509960055351257\tGenerator: loss 1.0294413566589355\n",
            "Epoch 911, batch 1/9:\tDiscriminator: real loss 0.16979525983333588, fake loss 0.16626915335655212\tGenerator: loss 1.0500619411468506\n",
            "Epoch 912, batch 1/9:\tDiscriminator: real loss 0.1836160123348236, fake loss 0.15427637100219727\tGenerator: loss 1.1826550960540771\n",
            "Epoch 913, batch 1/9:\tDiscriminator: real loss 0.17009484767913818, fake loss 0.17489059269428253\tGenerator: loss 1.1212248802185059\n",
            "Epoch 914, batch 1/9:\tDiscriminator: real loss 0.14998692274093628, fake loss 0.17857114970684052\tGenerator: loss 1.0512681007385254\n",
            "Epoch 915, batch 1/9:\tDiscriminator: real loss 0.13709023594856262, fake loss 0.12274020910263062\tGenerator: loss 1.2598631381988525\n",
            "Epoch 916, batch 1/9:\tDiscriminator: real loss 0.12692373991012573, fake loss 0.15476718544960022\tGenerator: loss 1.0495216846466064\n",
            "Epoch 917, batch 1/9:\tDiscriminator: real loss 0.15433037281036377, fake loss 0.141670823097229\tGenerator: loss 1.2515110969543457\n",
            "Epoch 918, batch 1/9:\tDiscriminator: real loss 0.12921622395515442, fake loss 0.1341584324836731\tGenerator: loss 1.2112579345703125\n",
            "Epoch 919, batch 1/9:\tDiscriminator: real loss 0.1298900544643402, fake loss 0.10937574505805969\tGenerator: loss 1.2866849899291992\n",
            "Epoch 920, batch 1/9:\tDiscriminator: real loss 0.10114661604166031, fake loss 0.12331865727901459\tGenerator: loss 1.3978350162506104\n",
            "Epoch 921, batch 1/9:\tDiscriminator: real loss 0.10618320107460022, fake loss 0.1414744257926941\tGenerator: loss 1.310556411743164\n",
            "Epoch 922, batch 1/9:\tDiscriminator: real loss 0.1536509096622467, fake loss 0.17223399877548218\tGenerator: loss 1.3345246315002441\n",
            "Epoch 923, batch 1/9:\tDiscriminator: real loss 0.12266097217798233, fake loss 0.11631856858730316\tGenerator: loss 1.2695813179016113\n",
            "Epoch 924, batch 1/9:\tDiscriminator: real loss 0.13065925240516663, fake loss 0.14708253741264343\tGenerator: loss 1.2155442237854004\n",
            "Epoch 925, batch 1/9:\tDiscriminator: real loss 0.16539065539836884, fake loss 0.11655393242835999\tGenerator: loss 1.5001732110977173\n",
            "Epoch 926, batch 1/9:\tDiscriminator: real loss 0.14329653978347778, fake loss 0.13738080859184265\tGenerator: loss 1.4266746044158936\n",
            "Epoch 927, batch 1/9:\tDiscriminator: real loss 0.11852573603391647, fake loss 0.14487536251544952\tGenerator: loss 1.4189951419830322\n",
            "Epoch 928, batch 1/9:\tDiscriminator: real loss 0.15284597873687744, fake loss 0.2151503711938858\tGenerator: loss 1.4178543090820312\n",
            "Epoch 929, batch 1/9:\tDiscriminator: real loss 0.1551053673028946, fake loss 0.14199954271316528\tGenerator: loss 1.2626211643218994\n",
            "Epoch 930, batch 1/9:\tDiscriminator: real loss 0.14040598273277283, fake loss 0.19067968428134918\tGenerator: loss 1.1579076051712036\n",
            "Epoch 931, batch 1/9:\tDiscriminator: real loss 0.12709906697273254, fake loss 0.13517041504383087\tGenerator: loss 1.207977294921875\n",
            "Epoch 932, batch 1/9:\tDiscriminator: real loss 0.15956050157546997, fake loss 0.15359947085380554\tGenerator: loss 1.3854312896728516\n",
            "Epoch 933, batch 1/9:\tDiscriminator: real loss 0.13486646115779877, fake loss 0.17444196343421936\tGenerator: loss 1.3037104606628418\n",
            "Epoch 934, batch 1/9:\tDiscriminator: real loss 0.11735839396715164, fake loss 0.19856317341327667\tGenerator: loss 1.252402663230896\n",
            "Epoch 935, batch 1/9:\tDiscriminator: real loss 0.1372465193271637, fake loss 0.14351895451545715\tGenerator: loss 1.3541473150253296\n",
            "Epoch 936, batch 1/9:\tDiscriminator: real loss 0.16419687867164612, fake loss 0.1551418900489807\tGenerator: loss 1.1537621021270752\n",
            "Epoch 937, batch 1/9:\tDiscriminator: real loss 0.15814095735549927, fake loss 0.13967519998550415\tGenerator: loss 1.1645469665527344\n",
            "Epoch 938, batch 1/9:\tDiscriminator: real loss 0.15998677909374237, fake loss 0.13451896607875824\tGenerator: loss 1.2049163579940796\n",
            "Epoch 939, batch 1/9:\tDiscriminator: real loss 0.17051273584365845, fake loss 0.2250685691833496\tGenerator: loss 1.0244555473327637\n",
            "Epoch 940, batch 1/9:\tDiscriminator: real loss 0.2314656376838684, fake loss 0.2420010268688202\tGenerator: loss 0.9648089408874512\n",
            "Epoch 941, batch 1/9:\tDiscriminator: real loss 0.2137233018875122, fake loss 0.1552402526140213\tGenerator: loss 1.1532175540924072\n",
            "Epoch 942, batch 1/9:\tDiscriminator: real loss 0.2044966220855713, fake loss 0.18502619862556458\tGenerator: loss 1.0784133672714233\n",
            "Epoch 943, batch 1/9:\tDiscriminator: real loss 0.19445762038230896, fake loss 0.19273892045021057\tGenerator: loss 0.9513893127441406\n",
            "Epoch 944, batch 1/9:\tDiscriminator: real loss 0.2144797444343567, fake loss 0.18544484674930573\tGenerator: loss 0.9653809070587158\n",
            "Epoch 945, batch 1/9:\tDiscriminator: real loss 0.17793002724647522, fake loss 0.18792231380939484\tGenerator: loss 1.0375007390975952\n",
            "Epoch 946, batch 1/9:\tDiscriminator: real loss 0.19559133052825928, fake loss 0.15612900257110596\tGenerator: loss 1.0203126668930054\n",
            "Epoch 947, batch 1/9:\tDiscriminator: real loss 0.24793469905853271, fake loss 0.1610306352376938\tGenerator: loss 1.1272985935211182\n",
            "Epoch 948, batch 1/9:\tDiscriminator: real loss 0.16975824534893036, fake loss 0.18635261058807373\tGenerator: loss 1.0719064474105835\n",
            "Epoch 949, batch 1/9:\tDiscriminator: real loss 0.197289377450943, fake loss 0.1760016679763794\tGenerator: loss 1.0576478242874146\n",
            "Epoch 950, batch 1/9:\tDiscriminator: real loss 0.20820391178131104, fake loss 0.15179972350597382\tGenerator: loss 1.1993178129196167\n",
            "Epoch 951, batch 1/9:\tDiscriminator: real loss 0.16881930828094482, fake loss 0.10975828766822815\tGenerator: loss 1.2901750802993774\n",
            "Epoch 952, batch 1/9:\tDiscriminator: real loss 0.1681564450263977, fake loss 0.14588353037834167\tGenerator: loss 1.1597905158996582\n",
            "Epoch 953, batch 1/9:\tDiscriminator: real loss 0.15248021483421326, fake loss 0.1520915925502777\tGenerator: loss 1.1733274459838867\n",
            "Epoch 954, batch 1/9:\tDiscriminator: real loss 0.1411673128604889, fake loss 0.14576146006584167\tGenerator: loss 1.165390968322754\n",
            "Epoch 955, batch 1/9:\tDiscriminator: real loss 0.16437697410583496, fake loss 0.18657205998897552\tGenerator: loss 1.1305279731750488\n",
            "Epoch 956, batch 1/9:\tDiscriminator: real loss 0.1795656979084015, fake loss 0.16741368174552917\tGenerator: loss 1.1969013214111328\n",
            "Epoch 957, batch 1/9:\tDiscriminator: real loss 0.20855429768562317, fake loss 0.17389440536499023\tGenerator: loss 1.124796986579895\n",
            "Epoch 958, batch 1/9:\tDiscriminator: real loss 0.1822478175163269, fake loss 0.1543310582637787\tGenerator: loss 1.1699519157409668\n",
            "Epoch 959, batch 1/9:\tDiscriminator: real loss 0.15092793107032776, fake loss 0.11870233714580536\tGenerator: loss 1.2601780891418457\n",
            "Epoch 960, batch 1/9:\tDiscriminator: real loss 0.18088629841804504, fake loss 0.20021545886993408\tGenerator: loss 1.105566143989563\n",
            "Epoch 961, batch 1/9:\tDiscriminator: real loss 0.21643498539924622, fake loss 0.19623737037181854\tGenerator: loss 0.9189574122428894\n",
            "Epoch 962, batch 1/9:\tDiscriminator: real loss 0.20971578359603882, fake loss 0.1652960479259491\tGenerator: loss 1.0503075122833252\n",
            "Epoch 963, batch 1/9:\tDiscriminator: real loss 0.1866014003753662, fake loss 0.162301704287529\tGenerator: loss 1.1391913890838623\n",
            "Epoch 964, batch 1/9:\tDiscriminator: real loss 0.16563032567501068, fake loss 0.17865076661109924\tGenerator: loss 1.1365617513656616\n",
            "Epoch 965, batch 1/9:\tDiscriminator: real loss 0.16965779662132263, fake loss 0.221360981464386\tGenerator: loss 1.0853240489959717\n",
            "Epoch 966, batch 1/9:\tDiscriminator: real loss 0.1588895171880722, fake loss 0.18999233841896057\tGenerator: loss 1.034610390663147\n",
            "Epoch 967, batch 1/9:\tDiscriminator: real loss 0.18791434168815613, fake loss 0.2190438210964203\tGenerator: loss 1.0345083475112915\n",
            "Epoch 968, batch 1/9:\tDiscriminator: real loss 0.18830309808254242, fake loss 0.2006751000881195\tGenerator: loss 0.9590522646903992\n",
            "Epoch 969, batch 1/9:\tDiscriminator: real loss 0.2122984379529953, fake loss 0.18236492574214935\tGenerator: loss 1.000178337097168\n",
            "Epoch 970, batch 1/9:\tDiscriminator: real loss 0.20315225422382355, fake loss 0.19085383415222168\tGenerator: loss 1.0161384344100952\n",
            "Epoch 971, batch 1/9:\tDiscriminator: real loss 0.17413969337940216, fake loss 0.20049408078193665\tGenerator: loss 1.0148422718048096\n",
            "Epoch 972, batch 1/9:\tDiscriminator: real loss 0.17243608832359314, fake loss 0.18105638027191162\tGenerator: loss 0.9559479355812073\n",
            "Epoch 973, batch 1/9:\tDiscriminator: real loss 0.19164589047431946, fake loss 0.18211127817630768\tGenerator: loss 1.0413416624069214\n",
            "Epoch 974, batch 1/9:\tDiscriminator: real loss 0.20436063408851624, fake loss 0.16774947941303253\tGenerator: loss 1.084341287612915\n",
            "Epoch 975, batch 1/9:\tDiscriminator: real loss 0.20418135821819305, fake loss 0.15007182955741882\tGenerator: loss 1.1121344566345215\n",
            "Epoch 976, batch 1/9:\tDiscriminator: real loss 0.19749370217323303, fake loss 0.1536674052476883\tGenerator: loss 1.0773028135299683\n",
            "Epoch 977, batch 1/9:\tDiscriminator: real loss 0.16203874349594116, fake loss 0.1456291526556015\tGenerator: loss 1.1862983703613281\n",
            "Epoch 978, batch 1/9:\tDiscriminator: real loss 0.14250075817108154, fake loss 0.17360633611679077\tGenerator: loss 1.046189785003662\n",
            "Epoch 979, batch 1/9:\tDiscriminator: real loss 0.1709158569574356, fake loss 0.1836235523223877\tGenerator: loss 0.9915171265602112\n",
            "Epoch 980, batch 1/9:\tDiscriminator: real loss 0.15901857614517212, fake loss 0.15181279182434082\tGenerator: loss 1.1301043033599854\n",
            "Epoch 981, batch 1/9:\tDiscriminator: real loss 0.17568686604499817, fake loss 0.18443581461906433\tGenerator: loss 1.0296720266342163\n",
            "Epoch 982, batch 1/9:\tDiscriminator: real loss 0.15728230774402618, fake loss 0.13848364353179932\tGenerator: loss 1.1016623973846436\n",
            "Epoch 983, batch 1/9:\tDiscriminator: real loss 0.15227949619293213, fake loss 0.17153748869895935\tGenerator: loss 0.9789240956306458\n",
            "Epoch 984, batch 1/9:\tDiscriminator: real loss 0.17659476399421692, fake loss 0.15197449922561646\tGenerator: loss 1.1238480806350708\n",
            "Epoch 985, batch 1/9:\tDiscriminator: real loss 0.13939619064331055, fake loss 0.16115495562553406\tGenerator: loss 1.282773494720459\n",
            "Epoch 986, batch 1/9:\tDiscriminator: real loss 0.12332534790039062, fake loss 0.17414170503616333\tGenerator: loss 1.0897760391235352\n",
            "Epoch 987, batch 1/9:\tDiscriminator: real loss 0.12435044348239899, fake loss 0.1555788666009903\tGenerator: loss 1.2959011793136597\n",
            "Epoch 988, batch 1/9:\tDiscriminator: real loss 0.14487497508525848, fake loss 0.18977044522762299\tGenerator: loss 1.066036343574524\n",
            "Epoch 989, batch 1/9:\tDiscriminator: real loss 0.22980251908302307, fake loss 0.22134482860565186\tGenerator: loss 0.975431501865387\n",
            "Epoch 990, batch 1/9:\tDiscriminator: real loss 0.23576171696186066, fake loss 0.22703278064727783\tGenerator: loss 1.2440184354782104\n",
            "Epoch 991, batch 1/9:\tDiscriminator: real loss 0.22390782833099365, fake loss 0.1513136774301529\tGenerator: loss 1.112884283065796\n",
            "Epoch 992, batch 1/9:\tDiscriminator: real loss 0.24585479497909546, fake loss 0.15330427885055542\tGenerator: loss 1.074220061302185\n",
            "Epoch 993, batch 1/9:\tDiscriminator: real loss 0.18149729073047638, fake loss 0.17829737067222595\tGenerator: loss 1.0870412588119507\n",
            "Epoch 994, batch 1/9:\tDiscriminator: real loss 0.18736153841018677, fake loss 0.1993522197008133\tGenerator: loss 1.0753952264785767\n",
            "Epoch 995, batch 1/9:\tDiscriminator: real loss 0.1845332384109497, fake loss 0.16703209280967712\tGenerator: loss 1.0066312551498413\n",
            "Epoch 996, batch 1/9:\tDiscriminator: real loss 0.18588288128376007, fake loss 0.18261343240737915\tGenerator: loss 0.9828000664710999\n",
            "Epoch 997, batch 1/9:\tDiscriminator: real loss 0.1940854787826538, fake loss 0.1649615317583084\tGenerator: loss 1.1056301593780518\n",
            "Epoch 998, batch 1/9:\tDiscriminator: real loss 0.19791653752326965, fake loss 0.1844388097524643\tGenerator: loss 1.0796400308609009\n",
            "Epoch 999, batch 1/9:\tDiscriminator: real loss 0.2112886905670166, fake loss 0.17302435636520386\tGenerator: loss 1.1634020805358887\n",
            "Epoch 1000, batch 1/9:\tDiscriminator: real loss 0.18874429166316986, fake loss 0.17094610631465912\tGenerator: loss 1.18399977684021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1ZZM2KwySf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fb116a2-81a9-41c2-b15c-84b2fa002999"
      },
      "source": [
        "def show_generated(examples, n, c):\n",
        "  fig = plt.figure(figsize=(c, n))\n",
        "  for i in range(n*c):\n",
        "    img = fig.add_subplot(n, c, i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(examples[i,:,:,0], cmap='gray_r')\n",
        "  plt.show()\n",
        "\n",
        "n = 10\n",
        "c = 4\n",
        "\n",
        "model = load_model('generator.h5')\n",
        "noise, _ = generate_latent_noise(latent_dim, n*c)\n",
        "labs = np.asarray([x for _ in range(n) for x in range(c)])\n",
        "\n",
        "X = model.predict([noise, labs])\n",
        "# scale from [-1, 1] to [0, 1]\n",
        "X = (X + 1) / 2.0\n",
        "x = np.squeeze(X[0])\n",
        "plt.imshow(x)\n",
        "\n",
        "show_generated(X, n, c)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x720 with 40 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZxgQ6QW_x4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "30d93b4e-2df4-4e58-e440-c1e8bcacddd5"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "dflag = True\n",
        "if dflag:\n",
        "  files.download('gan.h5')\n",
        "  files.download('generator.h5')\n",
        "  files.download('discriminator.h5')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-14b5e11a29ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gan.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generator.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'discriminator.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsGQ5nl3-P-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}