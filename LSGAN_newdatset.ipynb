{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSGAN_newdatset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "foR_ZBqnWQu5",
        "colab_type": "code",
        "outputId": "54b233a5-d732-4a21-877f-57fdba29a62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!pip install python-resize-image\n",
        "!pip install -U scipy==1.2.0\n",
        "!pip install python-archive\n",
        "import numpy as np\n",
        "from numpy.random import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from skimage import data, io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "from archive import Archive\n",
        "from archive import extract\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-resize-image\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/89/008481c95551992e1a77503eba490b75fd17c0a98e33dd4dc39e0b99e5e8/python_resize_image-1.1.19-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.19.1 in /usr/local/lib/python3.6/dist-packages (from python-resize-image) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from python-resize-image) (7.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.1->python-resize-image) (2.9)\n",
            "Installing collected packages: python-resize-image\n",
            "Successfully installed python-resize-image-1.1.19\n",
            "Collecting scipy==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 113kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.0) (1.18.3)\n",
            "\u001b[31mERROR: umap-learn 0.4.2 has requirement scipy>=1.3.1, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc4 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.2.0\n",
            "Collecting python-archive\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/17/560776b5c88c0c665b54db9d076e7334b70fee5100a1bb52b18b92f9e123/python-archive-0.2.tar.gz\n",
            "Building wheels for collected packages: python-archive\n",
            "  Building wheel for python-archive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-archive: filename=python_archive-0.2-cp36-none-any.whl size=6473 sha256=9adadfab3fbc1f6ae7232cd43e4022a5ebf12e4705a5c58e07f9106318636148\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/3b/c2/cde06bb709d37d647cc99a812cc1a06f2704d5feb20e97fee6\n",
            "Successfully built python-archive\n",
            "Installing collected packages: python-archive\n",
            "Successfully installed python-archive-0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KneN9w6nyOsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "from resizeimage import resizeimage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfDk5_9YWcj5",
        "colab_type": "code",
        "outputId": "4acf10c2-cada-4933-e8d5-cec70e693e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3-eu-west-1.amazonaws.com/handwriting-curated-database/curated.tar.gz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 07:16:21--  https://s3-eu-west-1.amazonaws.com/handwriting-curated-database/curated.tar.gz\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.108.51\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.108.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30058219 (29M) [application/x-gzip]\n",
            "Saving to: ‘curated.tar.gz’\n",
            "\n",
            "curated.tar.gz      100%[===================>]  28.67M  6.93MB/s    in 4.1s    \n",
            "\n",
            "2020-05-06 07:16:26 (6.93 MB/s) - ‘curated.tar.gz’ saved [30058219/30058219]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGG5g7yvqQNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extract('curated.tar.gz', 'curated_data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNAPAoDqd-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from scipy.misc import imresize, imrotate, imsave\n",
        "\n",
        "# display plots in this notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# set display defaults\n",
        "plt.rcParams['figure.figsize'] = (10, 10) \n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray' \n",
        "\n",
        "\n",
        "path = 'curated_data' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQIJql34rZiD",
        "colab_type": "code",
        "outputId": "a0f15b96-bfcc-4063-a6a1-374391d3c7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "punc = '!\\\"#$%&\\'()*+,-./:;<=>?@[]^_`{|}~'\n",
        "lett = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
        "num = '0123456789'\n",
        "character_curated = [ord(c) for c in lett+num+punc]\n",
        "print([chr(i) for i in character_curated])\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "for i in character_curated:\n",
        "    path_img = path + '/curated/' + str(i) + '/'\n",
        "    for file_name in [f for f in listdir(path_img) if isfile(join(path_img, f))]:\n",
        "        img = cv2.imread(path_img + file_name, 0)\n",
        "        img = cv2.resize(img,(28, 28), interpolation = cv2.INTER_AREA)\n",
        "        images += [img]\n",
        "        labels += [i]\n",
        "\n",
        "images = np.array(images, dtype=np.uint8)\n",
        "labels = np.array(labels, dtype=np.uint8)\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~']\n",
            "(62382, 28, 28)\n",
            "(62382,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_aMVoMdtQf1",
        "colab_type": "code",
        "outputId": "940ed835-c671-4def-a884-c3f8f20278e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "plt.imshow(images[0])\n",
        "print(labels[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPC37DrQyNeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_discriminator(input_shape=(28, 28, 1), n_classes=93):\n",
        "    # label input and embedding\n",
        "    label_in = Input(shape=(1, ))\n",
        "    print(label_in.shape)\n",
        "    emb = Embedding(n_classes, 50)(label_in)\n",
        "    print(emb.shape)\n",
        "    label_h = Dense(input_shape[0] * input_shape[1])(emb)\n",
        "    print(label_h.shape)\n",
        "    re_label_h = Reshape((input_shape[0], input_shape[1], 1))(label_h)\n",
        "    print(re_label_h.shape)\n",
        "    # image input\n",
        "    image_in = Input(shape=input_shape)\n",
        "    # combine inputs\n",
        "    merge = Concatenate()([image_in, re_label_h])\n",
        "    # convnet\n",
        "    h1 = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(merge)\n",
        "    r1 = LeakyReLU(alpha=0.2)(h1)\n",
        "    h2 = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(r1)\n",
        "    r2 = LeakyReLU(alpha=0.2)(h2)\n",
        "    # fully connected net\n",
        "    fl = Flatten()(r2) \n",
        "    dr = Dropout(0.4)(fl)\n",
        "    # output\n",
        "    out = Dense(1, activation='sigmoid')(dr)\n",
        "    # define and compile model\n",
        "    model = Model([image_in, label_in], out)\n",
        "    opt = Adam(lr=2e-4, beta_1=0.5)\n",
        "    model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGeRstGNKcXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_generator(latent_dim, n_classes=93):\n",
        "    # label input and embedding\n",
        "    label_in = Input(shape=(1, ))\n",
        "    emb = Embedding(n_classes, 50)(label_in)\n",
        "    label_h = Dense(7*7)(emb)\n",
        "    re_label_h = Reshape((7, 7, 1))(label_h)\n",
        "    # noisy image input\n",
        "    noise_in = Input(shape=(latent_dim,))\n",
        "    noise_h = Dense(128*7*7)(noise_in)\n",
        "    noise_r = LeakyReLU(alpha=0.2)(noise_h)\n",
        "    re_noise_r = Reshape((7, 7, 128))(noise_r)\n",
        "    # combine inputs\n",
        "    merge = Concatenate()([re_noise_r, re_label_h])\n",
        "    # upsampling\n",
        "    u1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(merge)\n",
        "    r1 = LeakyReLU(alpha=0.2)(u1)\n",
        "    u2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(r1)\n",
        "    r2 = LeakyReLU(alpha=0.2)(u2)\n",
        "    # output\n",
        "    out = Conv2D(1, (7, 7), activation='tanh', padding='same')(r2)\n",
        "    # define model\n",
        "    model = Model([noise_in, label_in], out)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK-BXmPXLygJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_gan(gen, dis):\n",
        "    # discriminator shouldn't be trainable\n",
        "    dis.trainable = False\n",
        "    # get generator inputs and outputs\n",
        "    gen_noise, gen_label = gen.input\n",
        "    gen_output = gen.output\n",
        "    print\n",
        "    # feed to discriminator\n",
        "    gan_output = dis([gen_output, gen_label])\n",
        "    # define and compile GAN model\n",
        "    model = Model([gen_noise, gen_label], gan_output)\n",
        "    opt = Adam(lr=2e-4, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-lpv0kd5_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labels_updated(labels):\n",
        "  for i in range(labels.shape[0]):\n",
        "    if labels[i] in range(65,91):\n",
        "      labels[i] -=65\n",
        "    if labels[i] in range(97,123):\n",
        "      labels[i]-=71\n",
        "    if labels[i] in range(48,58):\n",
        "      labels[i]+=4\n",
        "    if labels[i] in range(33,48):\n",
        "      labels[i]+=29\n",
        "    if labels[i] in range(58,65):\n",
        "      labels[i]+=19\n",
        "    if labels[i] == 91:\n",
        "      labels[i] = 84\n",
        "    if labels[i] in range(93,97):\n",
        "      labels[i]-=8\n",
        "    if labels[i] in range(123,127):\n",
        "      labels[i]-=34\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFDDNNAnL2lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_inputs(images, labels):\n",
        "    if len(images.shape) > 2:\n",
        "      images = np.squeeze(images)\n",
        "    X = np.expand_dims(images, axis=-1)\n",
        "    X = X.astype('float32')\n",
        "    X = (X-127.5) / 127.5\n",
        "    labels = labels_updated(labels)\n",
        "    return [X, labels]\n",
        "    \n",
        "def generate_real_samples(images, labels, n_samples):\n",
        "    rand_index = randint(0, images.shape[0], n_samples)\n",
        "    X, labels = images[rand_index], labels[rand_index]\n",
        "    y = np.ones((n_samples, 1)) * 0.9 # discriminator target label\n",
        "    # label smoothing\n",
        "    #ind = np.random.choice(list(range(len(y))), size=int(len(y) * 0.01), replace=False)\n",
        "    #y[ind] = 1-y[ind]\n",
        "    return [X, labels], y\n",
        "\n",
        "def generate_latent_noise(latent_dim, n_samples, n_classes=93):\n",
        "    xin = randn(latent_dim * n_samples)\n",
        "    xin = xin.reshape(n_samples, latent_dim)\n",
        "    labels = randint(0, n_classes, n_samples)  #  generator class label\n",
        "    return xin, labels\n",
        "\n",
        "def generate_fake_samples(gen, latent_dim, n_samples):\n",
        "    zin, lin = generate_latent_noise(latent_dim, n_samples)\n",
        "    images = gen.predict([zin, lin])\n",
        "    y = np.zeros((n_samples, 1))  # discriminator target label\n",
        "    # label smoothing\n",
        "    #ind = np.random.choice(list(range(len(y))), size=int(len(y) * 0.01), replace=False)\n",
        "    #y[ind] = 1-y[ind]\n",
        "    return [images, lin], y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdaO0cQYoY1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_gan(gen, dis, gan_model, images, labels, latent_dim, n_epochs=100, batch_size=128):\n",
        "    batch_per_epoch = int(images.shape[0] / batch_size)\n",
        "    half_batch = int(batch_size / 2)\n",
        "    # enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        for j in range(batch_per_epoch):\n",
        "            # train discriminator on real images\n",
        "            [X_real, labels_real], y_real = generate_real_samples(images, labels, half_batch)\n",
        "            d_loss1, _ = dis.train_on_batch([X_real, labels_real], y_real)\n",
        "            # train discriminator on generated images\n",
        "            [X_fake, labels_fake], y_fake = generate_fake_samples(gen, latent_dim, half_batch)\n",
        "            #d_loss2 = dis.evaluate([X_fake, labels_fake], y_fake)[0]\n",
        "            #if d_loss2 > 0.5:\n",
        "            d_loss2, _ = dis.train_on_batch([X_fake, labels_fake], y_fake)\n",
        "            # prepare generator input\n",
        "            [zin, label_in] = generate_latent_noise(latent_dim, batch_size)\n",
        "            # invert labels for fake samples (prevent vanishing gradients)\n",
        "            y_gan = np.ones((batch_size, 1))\n",
        "            # label smoothing\n",
        "            #ind = np.random.choice(list(range(len(y_gan))), size=int(len(y_gan) * 0.01), replace=False)\n",
        "            #y_gan[ind] = 1-y_gan[ind]\n",
        "            # update generator loss\n",
        "            g_loss = gan_model.train_on_batch([zin, label_in], y_gan)\n",
        "            # output losses\n",
        "            if j % 50 == 0:\n",
        "              print('Epoch {}, batch {}/{}:\\tDiscriminator: real loss {}, fake loss {}\\tGenerator: loss {}'\n",
        "                  .format(i+1, j+1, batch_per_epoch, d_loss1, d_loss2, g_loss))\n",
        "    # save the models\n",
        "    gen.save('generator.h5')\n",
        "    dis.save('discriminator.h5')\n",
        "    gan_model.save('gan.h5')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QYYbvDtbiQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def labels_mapping():\n",
        "#  labels_map = {i-65 :i for i in range(65, 91)}\n",
        "#  labels_map.update({i-71 : i for i in range(97,123)})\n",
        "#  return labels_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0brxTIVodZ8",
        "colab_type": "code",
        "outputId": "9e4ac400-3d4a-4f52-d6f6-542051917e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "latent_dim = 100\n",
        "images, labels = prepare_inputs(images, labels)\n",
        "print(images.shape, labels.shape)\n",
        "#labels_map = labels_mapping()\n",
        "dis = define_discriminator()\n",
        "gen = define_generator(latent_dim)\n",
        "gan_model = define_gan(gen, dis)\n",
        "print(\"\\nDiscriminator\\n\")\n",
        "dis.summary()\n",
        "print(\"\\nGenerator\\n\")\n",
        "gen.summary()\n",
        "print(\"\\nGAN\\n\")\n",
        "gan_model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(62382, 28, 28, 1) (62382,)\n",
            "(None, 1)\n",
            "(None, 1, 50)\n",
            "(None, 1, 784)\n",
            "(None, 28, 28, 1)\n",
            "\n",
            "Discriminator\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 50)        4650        input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1, 784)       39984       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 28, 28, 1)    0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 28, 28, 2)    0           input_6[0][0]                    \n",
            "                                                                 reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 14, 14, 64)   1216        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 14, 14, 64)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 7, 7, 64)     36928       leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 7, 7, 64)     0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 3136)         0           leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 3136)         0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            3137        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 85,915\n",
            "Trainable params: 0\n",
            "Non-trainable params: 85,915\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Generator\n",
            "\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 6272)         633472      input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 50)        4650        input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 6272)         0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1, 49)        2499        embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 7, 7, 128)    0           leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 7, 7, 1)      0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 7, 7, 129)    0           reshape_5[0][0]                  \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 14, 14, 128)  264320      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 128)  0           conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 128)  262272      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 28, 28, 128)  0           conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 1)    6273        leaky_re_lu_9[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,173,486\n",
            "Trainable params: 1,173,486\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "GAN\n",
            "\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 6272)         633472      input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 50)        4650        input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 6272)         0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1, 49)        2499        embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 7, 7, 128)    0           leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 7, 7, 1)      0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 7, 7, 129)    0           reshape_5[0][0]                  \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 14, 14, 128)  264320      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 128)  0           conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 128)  262272      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 28, 28, 128)  0           conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 1)    6273        leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "model_3 (Model)                 (None, 1)            85915       conv2d_5[0][0]                   \n",
            "                                                                 input_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,259,401\n",
            "Trainable params: 1,173,486\n",
            "Non-trainable params: 85,915\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXPvq0QWoiGH",
        "colab_type": "code",
        "outputId": "74473270-51dd-4e4f-8f4c-9c6aed788595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_gan(gen, dis, gan_model, images, labels, latent_dim, n_epochs=50, batch_size=128)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, batch 1/487:\tDiscriminator: real loss 0.20036926865577698, fake loss 0.24964603781700134\tGenerator: loss 0.6944939494132996\n",
            "Epoch 1, batch 51/487:\tDiscriminator: real loss 0.20332366228103638, fake loss 0.21022924780845642\tGenerator: loss 0.8157287240028381\n",
            "Epoch 1, batch 101/487:\tDiscriminator: real loss 0.1781672090291977, fake loss 0.1949683129787445\tGenerator: loss 0.8613522052764893\n",
            "Epoch 1, batch 151/487:\tDiscriminator: real loss 0.13776400685310364, fake loss 0.17611536383628845\tGenerator: loss 1.0645649433135986\n",
            "Epoch 1, batch 201/487:\tDiscriminator: real loss 0.13514678180217743, fake loss 0.17674651741981506\tGenerator: loss 1.3198058605194092\n",
            "Epoch 1, batch 251/487:\tDiscriminator: real loss 0.13206741213798523, fake loss 0.15032783150672913\tGenerator: loss 1.3841850757598877\n",
            "Epoch 1, batch 301/487:\tDiscriminator: real loss 0.12055915594100952, fake loss 0.12529587745666504\tGenerator: loss 1.4147858619689941\n",
            "Epoch 1, batch 351/487:\tDiscriminator: real loss 0.12006881833076477, fake loss 0.16987945139408112\tGenerator: loss 1.5177550315856934\n",
            "Epoch 1, batch 401/487:\tDiscriminator: real loss 0.14389237761497498, fake loss 0.1825876235961914\tGenerator: loss 1.4058096408843994\n",
            "Epoch 1, batch 451/487:\tDiscriminator: real loss 0.12648798525333405, fake loss 0.20613983273506165\tGenerator: loss 1.4034593105316162\n",
            "Epoch 2, batch 1/487:\tDiscriminator: real loss 0.12087522447109222, fake loss 0.17774975299835205\tGenerator: loss 1.4522842168807983\n",
            "Epoch 2, batch 51/487:\tDiscriminator: real loss 0.1296987235546112, fake loss 0.12053654342889786\tGenerator: loss 1.5470054149627686\n",
            "Epoch 2, batch 101/487:\tDiscriminator: real loss 0.11976099014282227, fake loss 0.26981857419013977\tGenerator: loss 1.7079031467437744\n",
            "Epoch 2, batch 151/487:\tDiscriminator: real loss 0.1254298985004425, fake loss 0.20471888780593872\tGenerator: loss 1.5255646705627441\n",
            "Epoch 2, batch 201/487:\tDiscriminator: real loss 0.11137037724256516, fake loss 0.13711802661418915\tGenerator: loss 1.689699411392212\n",
            "Epoch 2, batch 251/487:\tDiscriminator: real loss 0.10944880545139313, fake loss 0.20519773662090302\tGenerator: loss 1.6731202602386475\n",
            "Epoch 2, batch 301/487:\tDiscriminator: real loss 0.13084840774536133, fake loss 0.16737154126167297\tGenerator: loss 1.657735824584961\n",
            "Epoch 2, batch 351/487:\tDiscriminator: real loss 0.12956182658672333, fake loss 0.16307015717029572\tGenerator: loss 1.8550200462341309\n",
            "Epoch 2, batch 401/487:\tDiscriminator: real loss 0.11143413931131363, fake loss 0.19572213292121887\tGenerator: loss 1.4383251667022705\n",
            "Epoch 2, batch 451/487:\tDiscriminator: real loss 0.12706325948238373, fake loss 0.20138223469257355\tGenerator: loss 1.7370686531066895\n",
            "Epoch 3, batch 1/487:\tDiscriminator: real loss 0.10547230392694473, fake loss 0.13170960545539856\tGenerator: loss 1.7070221900939941\n",
            "Epoch 3, batch 51/487:\tDiscriminator: real loss 0.12776044011116028, fake loss 0.17016823589801788\tGenerator: loss 1.891214370727539\n",
            "Epoch 3, batch 101/487:\tDiscriminator: real loss 0.11038199812173843, fake loss 0.17929725348949432\tGenerator: loss 1.5102250576019287\n",
            "Epoch 3, batch 151/487:\tDiscriminator: real loss 0.12372329831123352, fake loss 0.13173756003379822\tGenerator: loss 1.7331249713897705\n",
            "Epoch 3, batch 201/487:\tDiscriminator: real loss 0.12979519367218018, fake loss 0.18953464925289154\tGenerator: loss 1.7073659896850586\n",
            "Epoch 3, batch 251/487:\tDiscriminator: real loss 0.1127890795469284, fake loss 0.2133391797542572\tGenerator: loss 1.7599730491638184\n",
            "Epoch 3, batch 301/487:\tDiscriminator: real loss 0.11745696514844894, fake loss 0.1713925302028656\tGenerator: loss 1.845610499382019\n",
            "Epoch 3, batch 351/487:\tDiscriminator: real loss 0.11590394377708435, fake loss 0.14508694410324097\tGenerator: loss 1.935643196105957\n",
            "Epoch 3, batch 401/487:\tDiscriminator: real loss 0.12725692987442017, fake loss 0.14647336304187775\tGenerator: loss 1.8855648040771484\n",
            "Epoch 3, batch 451/487:\tDiscriminator: real loss 0.12539705634117126, fake loss 0.18348261713981628\tGenerator: loss 1.8235313892364502\n",
            "Epoch 4, batch 1/487:\tDiscriminator: real loss 0.1280052661895752, fake loss 0.15695466101169586\tGenerator: loss 1.8241276741027832\n",
            "Epoch 4, batch 51/487:\tDiscriminator: real loss 0.10786104202270508, fake loss 0.20317581295967102\tGenerator: loss 1.830771803855896\n",
            "Epoch 4, batch 101/487:\tDiscriminator: real loss 0.10949196666479111, fake loss 0.18441149592399597\tGenerator: loss 2.119413375854492\n",
            "Epoch 4, batch 151/487:\tDiscriminator: real loss 0.1354588270187378, fake loss 0.16726863384246826\tGenerator: loss 1.8693230152130127\n",
            "Epoch 4, batch 201/487:\tDiscriminator: real loss 0.12879762053489685, fake loss 0.1617409586906433\tGenerator: loss 1.9114779233932495\n",
            "Epoch 4, batch 251/487:\tDiscriminator: real loss 0.1252729594707489, fake loss 0.1704581379890442\tGenerator: loss 1.9332473278045654\n",
            "Epoch 4, batch 301/487:\tDiscriminator: real loss 0.12905237078666687, fake loss 0.16687752306461334\tGenerator: loss 1.7674260139465332\n",
            "Epoch 4, batch 351/487:\tDiscriminator: real loss 0.14119084179401398, fake loss 0.19273310899734497\tGenerator: loss 2.0377297401428223\n",
            "Epoch 4, batch 401/487:\tDiscriminator: real loss 0.12010224908590317, fake loss 0.18772053718566895\tGenerator: loss 2.0795154571533203\n",
            "Epoch 4, batch 451/487:\tDiscriminator: real loss 0.11826391518115997, fake loss 0.16808998584747314\tGenerator: loss 2.0825047492980957\n",
            "Epoch 5, batch 1/487:\tDiscriminator: real loss 0.10487359017133713, fake loss 0.19680555164813995\tGenerator: loss 1.8851513862609863\n",
            "Epoch 5, batch 51/487:\tDiscriminator: real loss 0.13333848118782043, fake loss 0.16105864942073822\tGenerator: loss 1.7471234798431396\n",
            "Epoch 5, batch 101/487:\tDiscriminator: real loss 0.12770487368106842, fake loss 0.16789165139198303\tGenerator: loss 2.033773422241211\n",
            "Epoch 5, batch 151/487:\tDiscriminator: real loss 0.11539077013731003, fake loss 0.18550166487693787\tGenerator: loss 1.6955598592758179\n",
            "Epoch 5, batch 201/487:\tDiscriminator: real loss 0.11807742714881897, fake loss 0.1774672418832779\tGenerator: loss 1.9394421577453613\n",
            "Epoch 5, batch 251/487:\tDiscriminator: real loss 0.1310032457113266, fake loss 0.1779213547706604\tGenerator: loss 2.0311636924743652\n",
            "Epoch 5, batch 301/487:\tDiscriminator: real loss 0.13177068531513214, fake loss 0.18367137014865875\tGenerator: loss 1.943057656288147\n",
            "Epoch 5, batch 351/487:\tDiscriminator: real loss 0.11506400257349014, fake loss 0.15037161111831665\tGenerator: loss 2.1339335441589355\n",
            "Epoch 5, batch 401/487:\tDiscriminator: real loss 0.10470785200595856, fake loss 0.15963834524154663\tGenerator: loss 2.044847011566162\n",
            "Epoch 5, batch 451/487:\tDiscriminator: real loss 0.11339083313941956, fake loss 0.180781751871109\tGenerator: loss 1.6484410762786865\n",
            "Epoch 6, batch 1/487:\tDiscriminator: real loss 0.14417102932929993, fake loss 0.16940626502037048\tGenerator: loss 2.040086030960083\n",
            "Epoch 6, batch 51/487:\tDiscriminator: real loss 0.12256063520908356, fake loss 0.16519692540168762\tGenerator: loss 2.12667179107666\n",
            "Epoch 6, batch 101/487:\tDiscriminator: real loss 0.1199629157781601, fake loss 0.15502546727657318\tGenerator: loss 2.1039350032806396\n",
            "Epoch 6, batch 151/487:\tDiscriminator: real loss 0.09745801240205765, fake loss 0.16509681940078735\tGenerator: loss 1.9325578212738037\n",
            "Epoch 6, batch 201/487:\tDiscriminator: real loss 0.11842388659715652, fake loss 0.16181115806102753\tGenerator: loss 1.7838454246520996\n",
            "Epoch 6, batch 251/487:\tDiscriminator: real loss 0.10939668118953705, fake loss 0.16313892602920532\tGenerator: loss 2.1275463104248047\n",
            "Epoch 6, batch 301/487:\tDiscriminator: real loss 0.11055173724889755, fake loss 0.18619570136070251\tGenerator: loss 1.9889498949050903\n",
            "Epoch 6, batch 351/487:\tDiscriminator: real loss 0.1252308040857315, fake loss 0.20871560275554657\tGenerator: loss 2.044452667236328\n",
            "Epoch 6, batch 401/487:\tDiscriminator: real loss 0.14461316168308258, fake loss 0.19855627417564392\tGenerator: loss 2.3586716651916504\n",
            "Epoch 6, batch 451/487:\tDiscriminator: real loss 0.1470174491405487, fake loss 0.20240789651870728\tGenerator: loss 2.1667046546936035\n",
            "Epoch 7, batch 1/487:\tDiscriminator: real loss 0.1122875064611435, fake loss 0.1740664541721344\tGenerator: loss 1.7550913095474243\n",
            "Epoch 7, batch 51/487:\tDiscriminator: real loss 0.12096609175205231, fake loss 0.20190301537513733\tGenerator: loss 2.0350754261016846\n",
            "Epoch 7, batch 101/487:\tDiscriminator: real loss 0.14907032251358032, fake loss 0.15563461184501648\tGenerator: loss 2.1778955459594727\n",
            "Epoch 7, batch 151/487:\tDiscriminator: real loss 0.134563609957695, fake loss 0.1539093405008316\tGenerator: loss 2.0860018730163574\n",
            "Epoch 7, batch 201/487:\tDiscriminator: real loss 0.12862858176231384, fake loss 0.16984856128692627\tGenerator: loss 2.0689198970794678\n",
            "Epoch 7, batch 251/487:\tDiscriminator: real loss 0.11977570503950119, fake loss 0.23483559489250183\tGenerator: loss 2.1278958320617676\n",
            "Epoch 7, batch 301/487:\tDiscriminator: real loss 0.08308165520429611, fake loss 0.22264505922794342\tGenerator: loss 2.65191650390625\n",
            "Epoch 7, batch 351/487:\tDiscriminator: real loss 0.11076942086219788, fake loss 0.14137250185012817\tGenerator: loss 2.1134419441223145\n",
            "Epoch 7, batch 401/487:\tDiscriminator: real loss 0.10333338379859924, fake loss 0.20346270501613617\tGenerator: loss 2.2416608333587646\n",
            "Epoch 7, batch 451/487:\tDiscriminator: real loss 0.12355008721351624, fake loss 0.19085830450057983\tGenerator: loss 2.139469623565674\n",
            "Epoch 8, batch 1/487:\tDiscriminator: real loss 0.14859741926193237, fake loss 0.17797741293907166\tGenerator: loss 2.156487226486206\n",
            "Epoch 8, batch 51/487:\tDiscriminator: real loss 0.12657542526721954, fake loss 0.15536458790302277\tGenerator: loss 2.0452473163604736\n",
            "Epoch 8, batch 101/487:\tDiscriminator: real loss 0.12515771389007568, fake loss 0.19073668122291565\tGenerator: loss 1.9781413078308105\n",
            "Epoch 8, batch 151/487:\tDiscriminator: real loss 0.13479432463645935, fake loss 0.1559218466281891\tGenerator: loss 2.2572131156921387\n",
            "Epoch 8, batch 201/487:\tDiscriminator: real loss 0.11761976778507233, fake loss 0.14796650409698486\tGenerator: loss 2.0409069061279297\n",
            "Epoch 8, batch 251/487:\tDiscriminator: real loss 0.10227124392986298, fake loss 0.18642942607402802\tGenerator: loss 2.363515853881836\n",
            "Epoch 8, batch 301/487:\tDiscriminator: real loss 0.10160934180021286, fake loss 0.174758642911911\tGenerator: loss 2.5326297283172607\n",
            "Epoch 8, batch 351/487:\tDiscriminator: real loss 0.14028334617614746, fake loss 0.19461853802204132\tGenerator: loss 2.0917508602142334\n",
            "Epoch 8, batch 401/487:\tDiscriminator: real loss 0.12173687666654587, fake loss 0.19112521409988403\tGenerator: loss 2.0062553882598877\n",
            "Epoch 8, batch 451/487:\tDiscriminator: real loss 0.11209672689437866, fake loss 0.1576502025127411\tGenerator: loss 2.4848227500915527\n",
            "Epoch 9, batch 1/487:\tDiscriminator: real loss 0.12164929509162903, fake loss 0.16634652018547058\tGenerator: loss 2.2317302227020264\n",
            "Epoch 9, batch 51/487:\tDiscriminator: real loss 0.14392822980880737, fake loss 0.18274366855621338\tGenerator: loss 2.2782671451568604\n",
            "Epoch 9, batch 101/487:\tDiscriminator: real loss 0.12829646468162537, fake loss 0.18860957026481628\tGenerator: loss 2.013720750808716\n",
            "Epoch 9, batch 151/487:\tDiscriminator: real loss 0.13584965467453003, fake loss 0.205998957157135\tGenerator: loss 2.0629212856292725\n",
            "Epoch 9, batch 201/487:\tDiscriminator: real loss 0.13919168710708618, fake loss 0.17479749023914337\tGenerator: loss 1.9708808660507202\n",
            "Epoch 9, batch 251/487:\tDiscriminator: real loss 0.11264461278915405, fake loss 0.19372911751270294\tGenerator: loss 2.264982223510742\n",
            "Epoch 9, batch 301/487:\tDiscriminator: real loss 0.1262986958026886, fake loss 0.18711261451244354\tGenerator: loss 2.3395602703094482\n",
            "Epoch 9, batch 351/487:\tDiscriminator: real loss 0.12673884630203247, fake loss 0.18286558985710144\tGenerator: loss 2.1267476081848145\n",
            "Epoch 9, batch 401/487:\tDiscriminator: real loss 0.107171431183815, fake loss 0.15615323185920715\tGenerator: loss 2.388075351715088\n",
            "Epoch 9, batch 451/487:\tDiscriminator: real loss 0.13969802856445312, fake loss 0.1588272601366043\tGenerator: loss 2.4497008323669434\n",
            "Epoch 10, batch 1/487:\tDiscriminator: real loss 0.11310926079750061, fake loss 0.13161368668079376\tGenerator: loss 2.1697206497192383\n",
            "Epoch 10, batch 51/487:\tDiscriminator: real loss 0.11733724176883698, fake loss 0.1811734139919281\tGenerator: loss 2.4514527320861816\n",
            "Epoch 10, batch 101/487:\tDiscriminator: real loss 0.12715643644332886, fake loss 0.1842147707939148\tGenerator: loss 2.4280591011047363\n",
            "Epoch 10, batch 151/487:\tDiscriminator: real loss 0.12849019467830658, fake loss 0.1654524803161621\tGenerator: loss 2.334217071533203\n",
            "Epoch 10, batch 201/487:\tDiscriminator: real loss 0.13177213072776794, fake loss 0.15337252616882324\tGenerator: loss 2.1172938346862793\n",
            "Epoch 10, batch 251/487:\tDiscriminator: real loss 0.12155058979988098, fake loss 0.16964831948280334\tGenerator: loss 2.2459464073181152\n",
            "Epoch 10, batch 301/487:\tDiscriminator: real loss 0.12005769461393356, fake loss 0.16141018271446228\tGenerator: loss 1.9214134216308594\n",
            "Epoch 10, batch 351/487:\tDiscriminator: real loss 0.12818260490894318, fake loss 0.1812545657157898\tGenerator: loss 2.0450215339660645\n",
            "Epoch 10, batch 401/487:\tDiscriminator: real loss 0.13254545629024506, fake loss 0.14556115865707397\tGenerator: loss 1.933826208114624\n",
            "Epoch 10, batch 451/487:\tDiscriminator: real loss 0.11117923259735107, fake loss 0.1706671416759491\tGenerator: loss 2.26745343208313\n",
            "Epoch 11, batch 1/487:\tDiscriminator: real loss 0.11277374625205994, fake loss 0.16417324542999268\tGenerator: loss 2.4799671173095703\n",
            "Epoch 11, batch 51/487:\tDiscriminator: real loss 0.1029471755027771, fake loss 0.15836524963378906\tGenerator: loss 2.2048861980438232\n",
            "Epoch 11, batch 101/487:\tDiscriminator: real loss 0.10744458436965942, fake loss 0.14195993542671204\tGenerator: loss 2.0663681030273438\n",
            "Epoch 11, batch 151/487:\tDiscriminator: real loss 0.13210341334342957, fake loss 0.19827605783939362\tGenerator: loss 2.432589530944824\n",
            "Epoch 11, batch 201/487:\tDiscriminator: real loss 0.10254129022359848, fake loss 0.1675211787223816\tGenerator: loss 2.7787537574768066\n",
            "Epoch 11, batch 251/487:\tDiscriminator: real loss 0.1252957284450531, fake loss 0.18223315477371216\tGenerator: loss 2.118922233581543\n",
            "Epoch 11, batch 301/487:\tDiscriminator: real loss 0.12045606225728989, fake loss 0.18842947483062744\tGenerator: loss 2.0383048057556152\n",
            "Epoch 11, batch 351/487:\tDiscriminator: real loss 0.1083022803068161, fake loss 0.15389619767665863\tGenerator: loss 2.330845355987549\n",
            "Epoch 11, batch 401/487:\tDiscriminator: real loss 0.1249447911977768, fake loss 0.16914837062358856\tGenerator: loss 2.649296760559082\n",
            "Epoch 11, batch 451/487:\tDiscriminator: real loss 0.10749001801013947, fake loss 0.1547589898109436\tGenerator: loss 2.3697409629821777\n",
            "Epoch 12, batch 1/487:\tDiscriminator: real loss 0.1477888822555542, fake loss 0.1809275895357132\tGenerator: loss 2.179304838180542\n",
            "Epoch 12, batch 51/487:\tDiscriminator: real loss 0.11940774321556091, fake loss 0.18665115535259247\tGenerator: loss 2.300218105316162\n",
            "Epoch 12, batch 101/487:\tDiscriminator: real loss 0.12887990474700928, fake loss 0.17847579717636108\tGenerator: loss 2.317513942718506\n",
            "Epoch 12, batch 151/487:\tDiscriminator: real loss 0.13430900871753693, fake loss 0.1497529149055481\tGenerator: loss 2.192092180252075\n",
            "Epoch 12, batch 201/487:\tDiscriminator: real loss 0.11085814237594604, fake loss 0.18427233397960663\tGenerator: loss 2.5828590393066406\n",
            "Epoch 12, batch 251/487:\tDiscriminator: real loss 0.12618118524551392, fake loss 0.1504092812538147\tGenerator: loss 2.92254638671875\n",
            "Epoch 12, batch 301/487:\tDiscriminator: real loss 0.1155896857380867, fake loss 0.15189231932163239\tGenerator: loss 2.3741347789764404\n",
            "Epoch 12, batch 351/487:\tDiscriminator: real loss 0.14202885329723358, fake loss 0.20249003171920776\tGenerator: loss 2.2279319763183594\n",
            "Epoch 12, batch 401/487:\tDiscriminator: real loss 0.1184714064002037, fake loss 0.16878630220890045\tGenerator: loss 2.5469954013824463\n",
            "Epoch 12, batch 451/487:\tDiscriminator: real loss 0.11950866878032684, fake loss 0.14091460406780243\tGenerator: loss 2.6350507736206055\n",
            "Epoch 13, batch 1/487:\tDiscriminator: real loss 0.11570806801319122, fake loss 0.15922659635543823\tGenerator: loss 2.742281436920166\n",
            "Epoch 13, batch 51/487:\tDiscriminator: real loss 0.14937078952789307, fake loss 0.2152189314365387\tGenerator: loss 2.3824949264526367\n",
            "Epoch 13, batch 101/487:\tDiscriminator: real loss 0.14118796586990356, fake loss 0.18929848074913025\tGenerator: loss 2.5251822471618652\n",
            "Epoch 13, batch 151/487:\tDiscriminator: real loss 0.1259298026561737, fake loss 0.18013305962085724\tGenerator: loss 2.275850772857666\n",
            "Epoch 13, batch 201/487:\tDiscriminator: real loss 0.09418188780546188, fake loss 0.1723858118057251\tGenerator: loss 2.300997257232666\n",
            "Epoch 13, batch 251/487:\tDiscriminator: real loss 0.12373963743448257, fake loss 0.16424992680549622\tGenerator: loss 2.396178722381592\n",
            "Epoch 13, batch 301/487:\tDiscriminator: real loss 0.1366603821516037, fake loss 0.17613065242767334\tGenerator: loss 2.4233665466308594\n",
            "Epoch 13, batch 351/487:\tDiscriminator: real loss 0.1437075287103653, fake loss 0.15840519964694977\tGenerator: loss 2.4164605140686035\n",
            "Epoch 13, batch 401/487:\tDiscriminator: real loss 0.13076689839363098, fake loss 0.18790054321289062\tGenerator: loss 2.5790486335754395\n",
            "Epoch 13, batch 451/487:\tDiscriminator: real loss 0.10553685575723648, fake loss 0.15935024619102478\tGenerator: loss 2.7783424854278564\n",
            "Epoch 14, batch 1/487:\tDiscriminator: real loss 0.13176703453063965, fake loss 0.18589547276496887\tGenerator: loss 2.359109401702881\n",
            "Epoch 14, batch 51/487:\tDiscriminator: real loss 0.12447041273117065, fake loss 0.1644463837146759\tGenerator: loss 2.4852652549743652\n",
            "Epoch 14, batch 101/487:\tDiscriminator: real loss 0.11743584275245667, fake loss 0.1845153421163559\tGenerator: loss 2.2843515872955322\n",
            "Epoch 14, batch 151/487:\tDiscriminator: real loss 0.13899537920951843, fake loss 0.18250814080238342\tGenerator: loss 2.5886712074279785\n",
            "Epoch 14, batch 201/487:\tDiscriminator: real loss 0.12547001242637634, fake loss 0.17818745970726013\tGenerator: loss 2.6571526527404785\n",
            "Epoch 14, batch 251/487:\tDiscriminator: real loss 0.10779321193695068, fake loss 0.16027244925498962\tGenerator: loss 2.758331298828125\n",
            "Epoch 14, batch 301/487:\tDiscriminator: real loss 0.11339472234249115, fake loss 0.19756419956684113\tGenerator: loss 2.511781692504883\n",
            "Epoch 14, batch 351/487:\tDiscriminator: real loss 0.1157776340842247, fake loss 0.2136501967906952\tGenerator: loss 2.8390049934387207\n",
            "Epoch 14, batch 401/487:\tDiscriminator: real loss 0.12112635374069214, fake loss 0.17985400557518005\tGenerator: loss 2.571268081665039\n",
            "Epoch 14, batch 451/487:\tDiscriminator: real loss 0.1181926354765892, fake loss 0.2032308727502823\tGenerator: loss 2.2744317054748535\n",
            "Epoch 15, batch 1/487:\tDiscriminator: real loss 0.11679767072200775, fake loss 0.16885662078857422\tGenerator: loss 2.5716662406921387\n",
            "Epoch 15, batch 51/487:\tDiscriminator: real loss 0.11198163032531738, fake loss 0.15573662519454956\tGenerator: loss 2.125201940536499\n",
            "Epoch 15, batch 101/487:\tDiscriminator: real loss 0.10476583242416382, fake loss 0.1681465059518814\tGenerator: loss 2.3614344596862793\n",
            "Epoch 15, batch 151/487:\tDiscriminator: real loss 0.11618160456418991, fake loss 0.1645922064781189\tGenerator: loss 2.4666659832000732\n",
            "Epoch 15, batch 201/487:\tDiscriminator: real loss 0.11062279343605042, fake loss 0.17765331268310547\tGenerator: loss 2.5203611850738525\n",
            "Epoch 15, batch 251/487:\tDiscriminator: real loss 0.10905606299638748, fake loss 0.1383073627948761\tGenerator: loss 2.5885109901428223\n",
            "Epoch 15, batch 301/487:\tDiscriminator: real loss 0.1244487315416336, fake loss 0.17791904509067535\tGenerator: loss 2.6400973796844482\n",
            "Epoch 15, batch 351/487:\tDiscriminator: real loss 0.10586060583591461, fake loss 0.15691354870796204\tGenerator: loss 2.260009527206421\n",
            "Epoch 15, batch 401/487:\tDiscriminator: real loss 0.11842592060565948, fake loss 0.13624684512615204\tGenerator: loss 2.2940406799316406\n",
            "Epoch 15, batch 451/487:\tDiscriminator: real loss 0.1139763742685318, fake loss 0.13905085623264313\tGenerator: loss 2.744744300842285\n",
            "Epoch 16, batch 1/487:\tDiscriminator: real loss 0.11710843443870544, fake loss 0.1954563558101654\tGenerator: loss 2.451802968978882\n",
            "Epoch 16, batch 51/487:\tDiscriminator: real loss 0.1187112107872963, fake loss 0.15290935337543488\tGenerator: loss 2.521790027618408\n",
            "Epoch 16, batch 101/487:\tDiscriminator: real loss 0.11578647792339325, fake loss 0.17407041788101196\tGenerator: loss 3.1695704460144043\n",
            "Epoch 16, batch 151/487:\tDiscriminator: real loss 0.1309279501438141, fake loss 0.18966782093048096\tGenerator: loss 2.6392645835876465\n",
            "Epoch 16, batch 201/487:\tDiscriminator: real loss 0.13428233563899994, fake loss 0.20723065733909607\tGenerator: loss 2.4993979930877686\n",
            "Epoch 16, batch 251/487:\tDiscriminator: real loss 0.12625476717948914, fake loss 0.1496087908744812\tGenerator: loss 2.8199453353881836\n",
            "Epoch 16, batch 301/487:\tDiscriminator: real loss 0.11923784017562866, fake loss 0.1864275336265564\tGenerator: loss 2.684757709503174\n",
            "Epoch 16, batch 351/487:\tDiscriminator: real loss 0.11798416078090668, fake loss 0.19369599223136902\tGenerator: loss 2.4734385013580322\n",
            "Epoch 16, batch 401/487:\tDiscriminator: real loss 0.1366955041885376, fake loss 0.1788732260465622\tGenerator: loss 2.4030375480651855\n",
            "Epoch 16, batch 451/487:\tDiscriminator: real loss 0.1278132200241089, fake loss 0.15478235483169556\tGenerator: loss 2.627929925918579\n",
            "Epoch 17, batch 1/487:\tDiscriminator: real loss 0.13325560092926025, fake loss 0.1281389594078064\tGenerator: loss 2.6303622722625732\n",
            "Epoch 17, batch 51/487:\tDiscriminator: real loss 0.1332806944847107, fake loss 0.1898774653673172\tGenerator: loss 2.7727787494659424\n",
            "Epoch 17, batch 101/487:\tDiscriminator: real loss 0.1321849524974823, fake loss 0.1480439007282257\tGenerator: loss 2.4927914142608643\n",
            "Epoch 17, batch 151/487:\tDiscriminator: real loss 0.12139337509870529, fake loss 0.17360171675682068\tGenerator: loss 2.5861992835998535\n",
            "Epoch 17, batch 201/487:\tDiscriminator: real loss 0.10830666124820709, fake loss 0.18953709304332733\tGenerator: loss 2.5985724925994873\n",
            "Epoch 17, batch 251/487:\tDiscriminator: real loss 0.1267809122800827, fake loss 0.19107487797737122\tGenerator: loss 2.8001813888549805\n",
            "Epoch 17, batch 301/487:\tDiscriminator: real loss 0.12741637229919434, fake loss 0.16164064407348633\tGenerator: loss 2.5901169776916504\n",
            "Epoch 17, batch 351/487:\tDiscriminator: real loss 0.1314573734998703, fake loss 0.15958891808986664\tGenerator: loss 2.6296796798706055\n",
            "Epoch 17, batch 401/487:\tDiscriminator: real loss 0.1486842930316925, fake loss 0.12760674953460693\tGenerator: loss 2.3938164710998535\n",
            "Epoch 17, batch 451/487:\tDiscriminator: real loss 0.12033034861087799, fake loss 0.21115249395370483\tGenerator: loss 2.356254816055298\n",
            "Epoch 18, batch 1/487:\tDiscriminator: real loss 0.11464928090572357, fake loss 0.17786139249801636\tGenerator: loss 2.853964328765869\n",
            "Epoch 18, batch 51/487:\tDiscriminator: real loss 0.12365692108869553, fake loss 0.1806081235408783\tGenerator: loss 2.1730897426605225\n",
            "Epoch 18, batch 101/487:\tDiscriminator: real loss 0.12955579161643982, fake loss 0.1825307160615921\tGenerator: loss 2.4905056953430176\n",
            "Epoch 18, batch 151/487:\tDiscriminator: real loss 0.12561994791030884, fake loss 0.18110224604606628\tGenerator: loss 2.961820125579834\n",
            "Epoch 18, batch 201/487:\tDiscriminator: real loss 0.13069352507591248, fake loss 0.14892683923244476\tGenerator: loss 2.5942940711975098\n",
            "Epoch 18, batch 251/487:\tDiscriminator: real loss 0.11417487263679504, fake loss 0.1512717604637146\tGenerator: loss 2.74749755859375\n",
            "Epoch 18, batch 301/487:\tDiscriminator: real loss 0.12176576256752014, fake loss 0.18230867385864258\tGenerator: loss 2.501875400543213\n",
            "Epoch 18, batch 351/487:\tDiscriminator: real loss 0.13302680850028992, fake loss 0.16788828372955322\tGenerator: loss 3.024571657180786\n",
            "Epoch 18, batch 401/487:\tDiscriminator: real loss 0.09536676108837128, fake loss 0.16867369413375854\tGenerator: loss 2.8110837936401367\n",
            "Epoch 18, batch 451/487:\tDiscriminator: real loss 0.1292765736579895, fake loss 0.2117442488670349\tGenerator: loss 2.222996950149536\n",
            "Epoch 19, batch 1/487:\tDiscriminator: real loss 0.12501607835292816, fake loss 0.1806706190109253\tGenerator: loss 2.807730197906494\n",
            "Epoch 19, batch 51/487:\tDiscriminator: real loss 0.1111358255147934, fake loss 0.18537892401218414\tGenerator: loss 2.6139683723449707\n",
            "Epoch 19, batch 101/487:\tDiscriminator: real loss 0.12424865365028381, fake loss 0.13763828575611115\tGenerator: loss 3.022028923034668\n",
            "Epoch 19, batch 151/487:\tDiscriminator: real loss 0.11627783626317978, fake loss 0.16353864967823029\tGenerator: loss 3.026700019836426\n",
            "Epoch 19, batch 201/487:\tDiscriminator: real loss 0.1326974779367447, fake loss 0.12568670511245728\tGenerator: loss 2.1016368865966797\n",
            "Epoch 19, batch 251/487:\tDiscriminator: real loss 0.12773072719573975, fake loss 0.18473471701145172\tGenerator: loss 2.545165777206421\n",
            "Epoch 19, batch 301/487:\tDiscriminator: real loss 0.1232103705406189, fake loss 0.1713331788778305\tGenerator: loss 3.1312456130981445\n",
            "Epoch 19, batch 351/487:\tDiscriminator: real loss 0.10360223054885864, fake loss 0.18942409753799438\tGenerator: loss 2.8384084701538086\n",
            "Epoch 19, batch 401/487:\tDiscriminator: real loss 0.1313963681459427, fake loss 0.15691882371902466\tGenerator: loss 2.9900665283203125\n",
            "Epoch 19, batch 451/487:\tDiscriminator: real loss 0.10696668922901154, fake loss 0.16700869798660278\tGenerator: loss 2.625502586364746\n",
            "Epoch 20, batch 1/487:\tDiscriminator: real loss 0.13454686105251312, fake loss 0.15628908574581146\tGenerator: loss 2.5008344650268555\n",
            "Epoch 20, batch 51/487:\tDiscriminator: real loss 0.1364910900592804, fake loss 0.18434765934944153\tGenerator: loss 2.5538318157196045\n",
            "Epoch 20, batch 101/487:\tDiscriminator: real loss 0.11629561334848404, fake loss 0.17124412953853607\tGenerator: loss 2.7852635383605957\n",
            "Epoch 20, batch 151/487:\tDiscriminator: real loss 0.13113097846508026, fake loss 0.2077844738960266\tGenerator: loss 2.500492572784424\n",
            "Epoch 20, batch 201/487:\tDiscriminator: real loss 0.13216155767440796, fake loss 0.16298344731330872\tGenerator: loss 3.030484199523926\n",
            "Epoch 20, batch 251/487:\tDiscriminator: real loss 0.11483270674943924, fake loss 0.16767674684524536\tGenerator: loss 3.05947208404541\n",
            "Epoch 20, batch 301/487:\tDiscriminator: real loss 0.12285542488098145, fake loss 0.152854323387146\tGenerator: loss 3.1551101207733154\n",
            "Epoch 20, batch 351/487:\tDiscriminator: real loss 0.13408297300338745, fake loss 0.1672789454460144\tGenerator: loss 2.7324111461639404\n",
            "Epoch 20, batch 401/487:\tDiscriminator: real loss 0.12448588758707047, fake loss 0.18846991658210754\tGenerator: loss 2.6358094215393066\n",
            "Epoch 20, batch 451/487:\tDiscriminator: real loss 0.11554369330406189, fake loss 0.19451242685317993\tGenerator: loss 2.719086170196533\n",
            "Epoch 21, batch 1/487:\tDiscriminator: real loss 0.03160514310002327, fake loss 0.3397999703884125\tGenerator: loss 2.8200645446777344\n",
            "Epoch 21, batch 51/487:\tDiscriminator: real loss 0.12511205673217773, fake loss 0.16854916512966156\tGenerator: loss 2.9521987438201904\n",
            "Epoch 21, batch 101/487:\tDiscriminator: real loss 0.1333642601966858, fake loss 0.16958211362361908\tGenerator: loss 2.3135859966278076\n",
            "Epoch 21, batch 151/487:\tDiscriminator: real loss 0.12107813358306885, fake loss 0.18168190121650696\tGenerator: loss 2.7432639598846436\n",
            "Epoch 21, batch 201/487:\tDiscriminator: real loss 0.12737680971622467, fake loss 0.1810029298067093\tGenerator: loss 2.6107897758483887\n",
            "Epoch 21, batch 251/487:\tDiscriminator: real loss 0.1185007244348526, fake loss 0.19180928170681\tGenerator: loss 2.4051246643066406\n",
            "Epoch 21, batch 301/487:\tDiscriminator: real loss 0.1194712445139885, fake loss 0.1463066041469574\tGenerator: loss 2.5864009857177734\n",
            "Epoch 21, batch 351/487:\tDiscriminator: real loss 0.12348015606403351, fake loss 0.1941375434398651\tGenerator: loss 2.3224198818206787\n",
            "Epoch 21, batch 401/487:\tDiscriminator: real loss 0.14070504903793335, fake loss 0.1716994047164917\tGenerator: loss 2.6082727909088135\n",
            "Epoch 21, batch 451/487:\tDiscriminator: real loss 0.13806892931461334, fake loss 0.21991516649723053\tGenerator: loss 3.0063047409057617\n",
            "Epoch 22, batch 1/487:\tDiscriminator: real loss 0.12140542268753052, fake loss 0.19652202725410461\tGenerator: loss 2.945089340209961\n",
            "Epoch 22, batch 51/487:\tDiscriminator: real loss 0.12320980429649353, fake loss 0.1758735179901123\tGenerator: loss 2.982887029647827\n",
            "Epoch 22, batch 101/487:\tDiscriminator: real loss 0.14313006401062012, fake loss 0.18037012219429016\tGenerator: loss 2.5493574142456055\n",
            "Epoch 22, batch 151/487:\tDiscriminator: real loss 0.12586906552314758, fake loss 0.1822051703929901\tGenerator: loss 2.2659997940063477\n",
            "Epoch 22, batch 201/487:\tDiscriminator: real loss 0.12423098087310791, fake loss 0.1778561770915985\tGenerator: loss 2.3692686557769775\n",
            "Epoch 22, batch 251/487:\tDiscriminator: real loss 0.13856928050518036, fake loss 0.21775080263614655\tGenerator: loss 2.3621010780334473\n",
            "Epoch 22, batch 301/487:\tDiscriminator: real loss 0.11563444137573242, fake loss 0.15493105351924896\tGenerator: loss 2.622407913208008\n",
            "Epoch 22, batch 351/487:\tDiscriminator: real loss 0.11721599102020264, fake loss 0.20890992879867554\tGenerator: loss 2.7597756385803223\n",
            "Epoch 22, batch 401/487:\tDiscriminator: real loss 0.11907074600458145, fake loss 0.168974369764328\tGenerator: loss 2.3854427337646484\n",
            "Epoch 22, batch 451/487:\tDiscriminator: real loss 0.09905314445495605, fake loss 0.1720493733882904\tGenerator: loss 2.4260356426239014\n",
            "Epoch 23, batch 1/487:\tDiscriminator: real loss 0.12649640440940857, fake loss 0.18103225529193878\tGenerator: loss 2.612302780151367\n",
            "Epoch 23, batch 51/487:\tDiscriminator: real loss 0.12525835633277893, fake loss 0.17241641879081726\tGenerator: loss 2.5171594619750977\n",
            "Epoch 23, batch 101/487:\tDiscriminator: real loss 0.12046545743942261, fake loss 0.15006819367408752\tGenerator: loss 3.0885438919067383\n",
            "Epoch 23, batch 151/487:\tDiscriminator: real loss 0.12170157581567764, fake loss 0.18553519248962402\tGenerator: loss 2.680722951889038\n",
            "Epoch 23, batch 201/487:\tDiscriminator: real loss 0.13519619405269623, fake loss 0.18212953209877014\tGenerator: loss 2.825040340423584\n",
            "Epoch 23, batch 251/487:\tDiscriminator: real loss 0.11510530114173889, fake loss 0.17809924483299255\tGenerator: loss 2.992246150970459\n",
            "Epoch 23, batch 301/487:\tDiscriminator: real loss 0.12824860215187073, fake loss 0.17804846167564392\tGenerator: loss 2.7756552696228027\n",
            "Epoch 23, batch 351/487:\tDiscriminator: real loss 0.10836285352706909, fake loss 0.15014143288135529\tGenerator: loss 2.866727113723755\n",
            "Epoch 23, batch 401/487:\tDiscriminator: real loss 0.10267667472362518, fake loss 0.1666596531867981\tGenerator: loss 3.0284910202026367\n",
            "Epoch 23, batch 451/487:\tDiscriminator: real loss 0.11862162500619888, fake loss 0.16708330810070038\tGenerator: loss 2.5962419509887695\n",
            "Epoch 24, batch 1/487:\tDiscriminator: real loss 0.10842140018939972, fake loss 0.1591184139251709\tGenerator: loss 3.3453481197357178\n",
            "Epoch 24, batch 51/487:\tDiscriminator: real loss 0.12095846980810165, fake loss 0.18688949942588806\tGenerator: loss 3.064925193786621\n",
            "Epoch 24, batch 101/487:\tDiscriminator: real loss 0.12471191585063934, fake loss 0.1761511266231537\tGenerator: loss 2.239908218383789\n",
            "Epoch 24, batch 151/487:\tDiscriminator: real loss 0.13214531540870667, fake loss 0.19717921316623688\tGenerator: loss 2.9330685138702393\n",
            "Epoch 24, batch 201/487:\tDiscriminator: real loss 0.11664896458387375, fake loss 0.15413886308670044\tGenerator: loss 2.3868460655212402\n",
            "Epoch 24, batch 251/487:\tDiscriminator: real loss 0.1335565745830536, fake loss 0.14647060632705688\tGenerator: loss 2.7772574424743652\n",
            "Epoch 24, batch 301/487:\tDiscriminator: real loss 0.12564225494861603, fake loss 0.13217732310295105\tGenerator: loss 2.904179573059082\n",
            "Epoch 24, batch 351/487:\tDiscriminator: real loss 0.12447500228881836, fake loss 0.18354563415050507\tGenerator: loss 2.2974801063537598\n",
            "Epoch 24, batch 401/487:\tDiscriminator: real loss 0.11908739060163498, fake loss 0.15811753273010254\tGenerator: loss 2.3840489387512207\n",
            "Epoch 24, batch 451/487:\tDiscriminator: real loss 0.11190660297870636, fake loss 0.14721012115478516\tGenerator: loss 2.544930934906006\n",
            "Epoch 25, batch 1/487:\tDiscriminator: real loss 0.12129046022891998, fake loss 0.17461246252059937\tGenerator: loss 2.5494730472564697\n",
            "Epoch 25, batch 51/487:\tDiscriminator: real loss 0.10658363252878189, fake loss 0.18029312789440155\tGenerator: loss 2.4811887741088867\n",
            "Epoch 25, batch 101/487:\tDiscriminator: real loss 0.13008630275726318, fake loss 0.1283995658159256\tGenerator: loss 2.4024267196655273\n",
            "Epoch 25, batch 151/487:\tDiscriminator: real loss 0.12978185713291168, fake loss 0.18351709842681885\tGenerator: loss 2.6948647499084473\n",
            "Epoch 25, batch 201/487:\tDiscriminator: real loss 0.12064148485660553, fake loss 0.172200545668602\tGenerator: loss 2.9093356132507324\n",
            "Epoch 25, batch 251/487:\tDiscriminator: real loss 0.10485725104808807, fake loss 0.1737625002861023\tGenerator: loss 3.0598316192626953\n",
            "Epoch 25, batch 301/487:\tDiscriminator: real loss 0.1370849311351776, fake loss 0.16514086723327637\tGenerator: loss 2.4378628730773926\n",
            "Epoch 25, batch 351/487:\tDiscriminator: real loss 0.11738098412752151, fake loss 0.13989323377609253\tGenerator: loss 3.493577480316162\n",
            "Epoch 25, batch 401/487:\tDiscriminator: real loss 0.12152659893035889, fake loss 0.14321953058242798\tGenerator: loss 2.7412686347961426\n",
            "Epoch 25, batch 451/487:\tDiscriminator: real loss 0.11762630194425583, fake loss 0.1499166190624237\tGenerator: loss 2.7404520511627197\n",
            "Epoch 26, batch 1/487:\tDiscriminator: real loss 0.10894978046417236, fake loss 0.16398176550865173\tGenerator: loss 2.9111557006835938\n",
            "Epoch 26, batch 51/487:\tDiscriminator: real loss 0.11575338244438171, fake loss 0.21294420957565308\tGenerator: loss 2.62363338470459\n",
            "Epoch 26, batch 101/487:\tDiscriminator: real loss 0.12013725936412811, fake loss 0.20143546164035797\tGenerator: loss 2.6679863929748535\n",
            "Epoch 26, batch 151/487:\tDiscriminator: real loss 0.1296679973602295, fake loss 0.16817782819271088\tGenerator: loss 2.732987403869629\n",
            "Epoch 26, batch 201/487:\tDiscriminator: real loss 0.12138605117797852, fake loss 0.13975267112255096\tGenerator: loss 2.7785019874572754\n",
            "Epoch 26, batch 251/487:\tDiscriminator: real loss 0.12086585909128189, fake loss 0.15134844183921814\tGenerator: loss 3.246608257293701\n",
            "Epoch 26, batch 301/487:\tDiscriminator: real loss 0.11130533367395401, fake loss 0.1594180464744568\tGenerator: loss 2.504934787750244\n",
            "Epoch 26, batch 351/487:\tDiscriminator: real loss 0.12577511370182037, fake loss 0.20520219206809998\tGenerator: loss 2.766998291015625\n",
            "Epoch 26, batch 401/487:\tDiscriminator: real loss 0.11995626986026764, fake loss 0.14871591329574585\tGenerator: loss 2.8170127868652344\n",
            "Epoch 26, batch 451/487:\tDiscriminator: real loss 0.13022500276565552, fake loss 0.16851675510406494\tGenerator: loss 3.074246883392334\n",
            "Epoch 27, batch 1/487:\tDiscriminator: real loss 0.129984050989151, fake loss 0.16968035697937012\tGenerator: loss 2.9730563163757324\n",
            "Epoch 27, batch 51/487:\tDiscriminator: real loss 0.11784332245588303, fake loss 0.16358590126037598\tGenerator: loss 2.7765698432922363\n",
            "Epoch 27, batch 101/487:\tDiscriminator: real loss 0.11706911027431488, fake loss 0.15230849385261536\tGenerator: loss 2.582883834838867\n",
            "Epoch 27, batch 151/487:\tDiscriminator: real loss 0.11621998250484467, fake loss 0.17228996753692627\tGenerator: loss 2.8403472900390625\n",
            "Epoch 27, batch 201/487:\tDiscriminator: real loss 0.11375927925109863, fake loss 0.17496535181999207\tGenerator: loss 2.6382827758789062\n",
            "Epoch 27, batch 251/487:\tDiscriminator: real loss 0.14263543486595154, fake loss 0.1597752571105957\tGenerator: loss 2.6100082397460938\n",
            "Epoch 27, batch 301/487:\tDiscriminator: real loss 0.12579727172851562, fake loss 0.17093659937381744\tGenerator: loss 2.8163607120513916\n",
            "Epoch 27, batch 351/487:\tDiscriminator: real loss 0.138158917427063, fake loss 0.21003277599811554\tGenerator: loss 2.91552996635437\n",
            "Epoch 27, batch 401/487:\tDiscriminator: real loss 0.12451718747615814, fake loss 0.19754813611507416\tGenerator: loss 2.8865365982055664\n",
            "Epoch 27, batch 451/487:\tDiscriminator: real loss 0.11832177639007568, fake loss 0.15694034099578857\tGenerator: loss 3.2465600967407227\n",
            "Epoch 28, batch 1/487:\tDiscriminator: real loss 0.10879795253276825, fake loss 0.13834425806999207\tGenerator: loss 2.5560054779052734\n",
            "Epoch 28, batch 51/487:\tDiscriminator: real loss 0.12531358003616333, fake loss 0.17565205693244934\tGenerator: loss 2.4978249073028564\n",
            "Epoch 28, batch 101/487:\tDiscriminator: real loss 0.11935722827911377, fake loss 0.15699881315231323\tGenerator: loss 2.81591796875\n",
            "Epoch 28, batch 151/487:\tDiscriminator: real loss 0.10537818819284439, fake loss 0.14435431361198425\tGenerator: loss 2.7470364570617676\n",
            "Epoch 28, batch 201/487:\tDiscriminator: real loss 0.14061883091926575, fake loss 0.22310885787010193\tGenerator: loss 2.544301986694336\n",
            "Epoch 28, batch 251/487:\tDiscriminator: real loss 0.13977521657943726, fake loss 0.18234995007514954\tGenerator: loss 2.6302764415740967\n",
            "Epoch 28, batch 301/487:\tDiscriminator: real loss 0.1138424277305603, fake loss 0.1578763723373413\tGenerator: loss 2.7564077377319336\n",
            "Epoch 28, batch 351/487:\tDiscriminator: real loss 0.12582966685295105, fake loss 0.18056008219718933\tGenerator: loss 2.9865779876708984\n",
            "Epoch 28, batch 401/487:\tDiscriminator: real loss 0.11591338366270065, fake loss 0.17038309574127197\tGenerator: loss 2.899484395980835\n",
            "Epoch 28, batch 451/487:\tDiscriminator: real loss 0.11582859605550766, fake loss 0.1845773607492447\tGenerator: loss 2.7462775707244873\n",
            "Epoch 29, batch 1/487:\tDiscriminator: real loss 0.12377139925956726, fake loss 0.18788759410381317\tGenerator: loss 2.6485633850097656\n",
            "Epoch 29, batch 51/487:\tDiscriminator: real loss 0.13770142197608948, fake loss 0.2098066359758377\tGenerator: loss 2.8517818450927734\n",
            "Epoch 29, batch 101/487:\tDiscriminator: real loss 0.13348643481731415, fake loss 0.16050061583518982\tGenerator: loss 2.80802059173584\n",
            "Epoch 29, batch 151/487:\tDiscriminator: real loss 0.11514723300933838, fake loss 0.2022911012172699\tGenerator: loss 2.6154427528381348\n",
            "Epoch 29, batch 201/487:\tDiscriminator: real loss 0.11614860594272614, fake loss 0.16484421491622925\tGenerator: loss 3.295360565185547\n",
            "Epoch 29, batch 251/487:\tDiscriminator: real loss 0.1149565577507019, fake loss 0.15327277779579163\tGenerator: loss 2.861335277557373\n",
            "Epoch 29, batch 301/487:\tDiscriminator: real loss 0.10453833639621735, fake loss 0.17088618874549866\tGenerator: loss 3.0332441329956055\n",
            "Epoch 29, batch 351/487:\tDiscriminator: real loss 0.09730828553438187, fake loss 0.16695186495780945\tGenerator: loss 2.50358247756958\n",
            "Epoch 29, batch 401/487:\tDiscriminator: real loss 0.11773323267698288, fake loss 0.17294946312904358\tGenerator: loss 3.09909725189209\n",
            "Epoch 29, batch 451/487:\tDiscriminator: real loss 0.10329544544219971, fake loss 0.17977869510650635\tGenerator: loss 3.2453267574310303\n",
            "Epoch 30, batch 1/487:\tDiscriminator: real loss 0.1439235806465149, fake loss 0.16756242513656616\tGenerator: loss 3.017240285873413\n",
            "Epoch 30, batch 51/487:\tDiscriminator: real loss 0.12894172966480255, fake loss 0.17583757638931274\tGenerator: loss 2.5410614013671875\n",
            "Epoch 30, batch 101/487:\tDiscriminator: real loss 0.13140511512756348, fake loss 0.15096554160118103\tGenerator: loss 3.3267982006073\n",
            "Epoch 30, batch 151/487:\tDiscriminator: real loss 0.1499897837638855, fake loss 0.16293874382972717\tGenerator: loss 3.344764471054077\n",
            "Epoch 30, batch 201/487:\tDiscriminator: real loss 0.13974879682064056, fake loss 0.1773448884487152\tGenerator: loss 3.4138588905334473\n",
            "Epoch 30, batch 251/487:\tDiscriminator: real loss 0.11395351588726044, fake loss 0.18452249467372894\tGenerator: loss 2.8322739601135254\n",
            "Epoch 30, batch 301/487:\tDiscriminator: real loss 0.10018695890903473, fake loss 0.17475518584251404\tGenerator: loss 2.864443302154541\n",
            "Epoch 30, batch 351/487:\tDiscriminator: real loss 0.16048038005828857, fake loss 0.1742570996284485\tGenerator: loss 2.7171359062194824\n",
            "Epoch 30, batch 401/487:\tDiscriminator: real loss 0.118851438164711, fake loss 0.16599637269973755\tGenerator: loss 2.8544976711273193\n",
            "Epoch 30, batch 451/487:\tDiscriminator: real loss 0.13676349818706512, fake loss 0.17443004250526428\tGenerator: loss 2.4532079696655273\n",
            "Epoch 31, batch 1/487:\tDiscriminator: real loss 0.10909977555274963, fake loss 0.14507439732551575\tGenerator: loss 2.4660024642944336\n",
            "Epoch 31, batch 51/487:\tDiscriminator: real loss 0.1166524887084961, fake loss 0.1745373010635376\tGenerator: loss 2.941636562347412\n",
            "Epoch 31, batch 101/487:\tDiscriminator: real loss 0.10745258629322052, fake loss 0.17935605347156525\tGenerator: loss 2.7762248516082764\n",
            "Epoch 31, batch 151/487:\tDiscriminator: real loss 0.12754151225090027, fake loss 0.18548907339572906\tGenerator: loss 2.3484792709350586\n",
            "Epoch 31, batch 201/487:\tDiscriminator: real loss 0.1298709660768509, fake loss 0.16539812088012695\tGenerator: loss 2.784984588623047\n",
            "Epoch 31, batch 251/487:\tDiscriminator: real loss 0.11165188252925873, fake loss 0.1514865905046463\tGenerator: loss 3.0424325466156006\n",
            "Epoch 31, batch 301/487:\tDiscriminator: real loss 0.1152089536190033, fake loss 0.1727534830570221\tGenerator: loss 3.113774299621582\n",
            "Epoch 31, batch 351/487:\tDiscriminator: real loss 0.12945665419101715, fake loss 0.18509960174560547\tGenerator: loss 2.8715944290161133\n",
            "Epoch 31, batch 401/487:\tDiscriminator: real loss 0.11125069856643677, fake loss 0.16302406787872314\tGenerator: loss 2.9893555641174316\n",
            "Epoch 31, batch 451/487:\tDiscriminator: real loss 0.10336712747812271, fake loss 0.17558030784130096\tGenerator: loss 2.7550644874572754\n",
            "Epoch 32, batch 1/487:\tDiscriminator: real loss 0.11090685427188873, fake loss 0.16313587129116058\tGenerator: loss 3.0457708835601807\n",
            "Epoch 32, batch 51/487:\tDiscriminator: real loss 0.12419023364782333, fake loss 0.15806743502616882\tGenerator: loss 2.962503433227539\n",
            "Epoch 32, batch 101/487:\tDiscriminator: real loss 0.09895224869251251, fake loss 0.17532014846801758\tGenerator: loss 2.6947996616363525\n",
            "Epoch 32, batch 151/487:\tDiscriminator: real loss 0.12683433294296265, fake loss 0.1741260290145874\tGenerator: loss 2.6320557594299316\n",
            "Epoch 32, batch 201/487:\tDiscriminator: real loss 0.13542132079601288, fake loss 0.18127977848052979\tGenerator: loss 2.7646450996398926\n",
            "Epoch 32, batch 251/487:\tDiscriminator: real loss 0.121197409927845, fake loss 0.1512022763490677\tGenerator: loss 2.562863349914551\n",
            "Epoch 32, batch 301/487:\tDiscriminator: real loss 0.14693830907344818, fake loss 0.16232863068580627\tGenerator: loss 2.6244113445281982\n",
            "Epoch 32, batch 351/487:\tDiscriminator: real loss 0.09944812953472137, fake loss 0.17326238751411438\tGenerator: loss 2.9264473915100098\n",
            "Epoch 32, batch 401/487:\tDiscriminator: real loss 0.11339966207742691, fake loss 0.18291126191616058\tGenerator: loss 2.749211311340332\n",
            "Epoch 32, batch 451/487:\tDiscriminator: real loss 0.1335534006357193, fake loss 0.16845035552978516\tGenerator: loss 3.202770233154297\n",
            "Epoch 33, batch 1/487:\tDiscriminator: real loss 0.09453634172677994, fake loss 0.1790861189365387\tGenerator: loss 2.9538936614990234\n",
            "Epoch 33, batch 51/487:\tDiscriminator: real loss 0.11223220825195312, fake loss 0.2279002070426941\tGenerator: loss 3.3000473976135254\n",
            "Epoch 33, batch 101/487:\tDiscriminator: real loss 0.12369188666343689, fake loss 0.21061867475509644\tGenerator: loss 2.843045473098755\n",
            "Epoch 33, batch 151/487:\tDiscriminator: real loss 0.14283788204193115, fake loss 0.1531338393688202\tGenerator: loss 1.8335639238357544\n",
            "Epoch 33, batch 201/487:\tDiscriminator: real loss 0.12972283363342285, fake loss 0.17174892127513885\tGenerator: loss 2.9791054725646973\n",
            "Epoch 33, batch 251/487:\tDiscriminator: real loss 0.12934404611587524, fake loss 0.2053825557231903\tGenerator: loss 2.170555353164673\n",
            "Epoch 33, batch 301/487:\tDiscriminator: real loss 0.12731382250785828, fake loss 0.1689629703760147\tGenerator: loss 3.3711581230163574\n",
            "Epoch 33, batch 351/487:\tDiscriminator: real loss 0.13461732864379883, fake loss 0.13340838253498077\tGenerator: loss 2.983063220977783\n",
            "Epoch 33, batch 401/487:\tDiscriminator: real loss 0.13605190813541412, fake loss 0.16029423475265503\tGenerator: loss 2.6507537364959717\n",
            "Epoch 33, batch 451/487:\tDiscriminator: real loss 0.1284908652305603, fake loss 0.15938079357147217\tGenerator: loss 2.1175179481506348\n",
            "Epoch 34, batch 1/487:\tDiscriminator: real loss 0.1255815625190735, fake loss 0.15199412405490875\tGenerator: loss 2.5762810707092285\n",
            "Epoch 34, batch 51/487:\tDiscriminator: real loss 0.10102851688861847, fake loss 0.1640823483467102\tGenerator: loss 2.8897719383239746\n",
            "Epoch 34, batch 101/487:\tDiscriminator: real loss 0.1266278624534607, fake loss 0.15921296179294586\tGenerator: loss 1.9692997932434082\n",
            "Epoch 34, batch 151/487:\tDiscriminator: real loss 0.09827546775341034, fake loss 0.18487076461315155\tGenerator: loss 2.9414217472076416\n",
            "Epoch 34, batch 201/487:\tDiscriminator: real loss 0.12040141969919205, fake loss 0.18909600377082825\tGenerator: loss 2.71978759765625\n",
            "Epoch 34, batch 251/487:\tDiscriminator: real loss 0.13274577260017395, fake loss 0.13293486833572388\tGenerator: loss 2.9718825817108154\n",
            "Epoch 34, batch 301/487:\tDiscriminator: real loss 0.10736152529716492, fake loss 0.15474317967891693\tGenerator: loss 2.7727065086364746\n",
            "Epoch 34, batch 351/487:\tDiscriminator: real loss 0.11109106242656708, fake loss 0.1623699963092804\tGenerator: loss 2.9728822708129883\n",
            "Epoch 34, batch 401/487:\tDiscriminator: real loss 0.1234409511089325, fake loss 0.2092890590429306\tGenerator: loss 2.769742488861084\n",
            "Epoch 34, batch 451/487:\tDiscriminator: real loss 0.0992470532655716, fake loss 0.16507211327552795\tGenerator: loss 2.910613536834717\n",
            "Epoch 35, batch 1/487:\tDiscriminator: real loss 0.13158223032951355, fake loss 0.1431085467338562\tGenerator: loss 2.587867498397827\n",
            "Epoch 35, batch 51/487:\tDiscriminator: real loss 0.12499886006116867, fake loss 0.20147112011909485\tGenerator: loss 2.327519416809082\n",
            "Epoch 35, batch 101/487:\tDiscriminator: real loss 0.11120544373989105, fake loss 0.1683136522769928\tGenerator: loss 2.6316356658935547\n",
            "Epoch 35, batch 151/487:\tDiscriminator: real loss 0.1176275908946991, fake loss 0.16810239851474762\tGenerator: loss 3.0736148357391357\n",
            "Epoch 35, batch 201/487:\tDiscriminator: real loss 0.09629340469837189, fake loss 0.1349111944437027\tGenerator: loss 2.38490629196167\n",
            "Epoch 35, batch 251/487:\tDiscriminator: real loss 0.11214137822389603, fake loss 0.15744303166866302\tGenerator: loss 2.4622550010681152\n",
            "Epoch 35, batch 301/487:\tDiscriminator: real loss 0.11733818054199219, fake loss 0.17292574048042297\tGenerator: loss 2.697192907333374\n",
            "Epoch 35, batch 351/487:\tDiscriminator: real loss 0.1198703795671463, fake loss 0.17714717984199524\tGenerator: loss 2.8108580112457275\n",
            "Epoch 35, batch 401/487:\tDiscriminator: real loss 0.11288146674633026, fake loss 0.16772697865962982\tGenerator: loss 2.514005184173584\n",
            "Epoch 35, batch 451/487:\tDiscriminator: real loss 0.10271869599819183, fake loss 0.15892964601516724\tGenerator: loss 3.6501851081848145\n",
            "Epoch 36, batch 1/487:\tDiscriminator: real loss 0.15099909901618958, fake loss 0.16769567131996155\tGenerator: loss 3.0036535263061523\n",
            "Epoch 36, batch 51/487:\tDiscriminator: real loss 0.1275501251220703, fake loss 0.16062138974666595\tGenerator: loss 2.861159086227417\n",
            "Epoch 36, batch 101/487:\tDiscriminator: real loss 0.10598959028720856, fake loss 0.19323787093162537\tGenerator: loss 2.29396653175354\n",
            "Epoch 36, batch 151/487:\tDiscriminator: real loss 0.13307538628578186, fake loss 0.17508141696453094\tGenerator: loss 3.036426305770874\n",
            "Epoch 36, batch 201/487:\tDiscriminator: real loss 0.13495579361915588, fake loss 0.1864406168460846\tGenerator: loss 2.6174070835113525\n",
            "Epoch 36, batch 251/487:\tDiscriminator: real loss 0.1342499852180481, fake loss 0.15231195092201233\tGenerator: loss 3.1582207679748535\n",
            "Epoch 36, batch 301/487:\tDiscriminator: real loss 0.13993452489376068, fake loss 0.16012383997440338\tGenerator: loss 2.6398143768310547\n",
            "Epoch 36, batch 351/487:\tDiscriminator: real loss 0.11337660998106003, fake loss 0.1417796015739441\tGenerator: loss 2.3908753395080566\n",
            "Epoch 36, batch 401/487:\tDiscriminator: real loss 0.11239107698202133, fake loss 0.16931232810020447\tGenerator: loss 3.127063274383545\n",
            "Epoch 36, batch 451/487:\tDiscriminator: real loss 0.12160798907279968, fake loss 0.16314095258712769\tGenerator: loss 2.5375027656555176\n",
            "Epoch 37, batch 1/487:\tDiscriminator: real loss 0.10828663408756256, fake loss 0.2085663229227066\tGenerator: loss 2.6045753955841064\n",
            "Epoch 37, batch 51/487:\tDiscriminator: real loss 0.09623869508504868, fake loss 0.14244452118873596\tGenerator: loss 3.3512961864471436\n",
            "Epoch 37, batch 101/487:\tDiscriminator: real loss 0.10943868011236191, fake loss 0.16926312446594238\tGenerator: loss 2.686300277709961\n",
            "Epoch 37, batch 151/487:\tDiscriminator: real loss 0.09142117202281952, fake loss 0.1894412636756897\tGenerator: loss 2.7980332374572754\n",
            "Epoch 37, batch 201/487:\tDiscriminator: real loss 0.1371089667081833, fake loss 0.17971214652061462\tGenerator: loss 3.4687037467956543\n",
            "Epoch 37, batch 251/487:\tDiscriminator: real loss 0.11586566269397736, fake loss 0.1943848431110382\tGenerator: loss 2.58294415473938\n",
            "Epoch 37, batch 301/487:\tDiscriminator: real loss 0.13821445405483246, fake loss 0.17625004053115845\tGenerator: loss 2.8347105979919434\n",
            "Epoch 37, batch 351/487:\tDiscriminator: real loss 0.11997805535793304, fake loss 0.1869194358587265\tGenerator: loss 2.828608989715576\n",
            "Epoch 37, batch 401/487:\tDiscriminator: real loss 0.1334826946258545, fake loss 0.19729095697402954\tGenerator: loss 2.8214492797851562\n",
            "Epoch 37, batch 451/487:\tDiscriminator: real loss 0.1329050362110138, fake loss 0.17662036418914795\tGenerator: loss 2.730292558670044\n",
            "Epoch 38, batch 1/487:\tDiscriminator: real loss 0.11560443043708801, fake loss 0.18547013401985168\tGenerator: loss 2.6761269569396973\n",
            "Epoch 38, batch 51/487:\tDiscriminator: real loss 0.11921355128288269, fake loss 0.162772536277771\tGenerator: loss 2.6240389347076416\n",
            "Epoch 38, batch 101/487:\tDiscriminator: real loss 0.11910927295684814, fake loss 0.15723630785942078\tGenerator: loss 2.540588855743408\n",
            "Epoch 38, batch 151/487:\tDiscriminator: real loss 0.13414716720581055, fake loss 0.21341633796691895\tGenerator: loss 2.6601362228393555\n",
            "Epoch 38, batch 201/487:\tDiscriminator: real loss 0.12287977337837219, fake loss 0.16015781462192535\tGenerator: loss 3.2709081172943115\n",
            "Epoch 38, batch 251/487:\tDiscriminator: real loss 0.11602860689163208, fake loss 0.1691281646490097\tGenerator: loss 2.6763949394226074\n",
            "Epoch 38, batch 301/487:\tDiscriminator: real loss 0.11711159348487854, fake loss 0.16181257367134094\tGenerator: loss 2.6828713417053223\n",
            "Epoch 38, batch 351/487:\tDiscriminator: real loss 0.12151440978050232, fake loss 0.15479865670204163\tGenerator: loss 2.9716005325317383\n",
            "Epoch 38, batch 401/487:\tDiscriminator: real loss 0.12559068202972412, fake loss 0.1845180094242096\tGenerator: loss 2.828340530395508\n",
            "Epoch 38, batch 451/487:\tDiscriminator: real loss 0.12197025865316391, fake loss 0.20882940292358398\tGenerator: loss 2.6696457862854004\n",
            "Epoch 39, batch 1/487:\tDiscriminator: real loss 0.11994557082653046, fake loss 0.210161954164505\tGenerator: loss 3.203001022338867\n",
            "Epoch 39, batch 51/487:\tDiscriminator: real loss 0.1339847892522812, fake loss 0.1588977575302124\tGenerator: loss 2.4955344200134277\n",
            "Epoch 39, batch 101/487:\tDiscriminator: real loss 0.13729676604270935, fake loss 0.18476426601409912\tGenerator: loss 2.577437400817871\n",
            "Epoch 39, batch 151/487:\tDiscriminator: real loss 0.12917357683181763, fake loss 0.15349917113780975\tGenerator: loss 2.2777063846588135\n",
            "Epoch 39, batch 201/487:\tDiscriminator: real loss 0.13735122978687286, fake loss 0.18476536870002747\tGenerator: loss 2.6663620471954346\n",
            "Epoch 39, batch 251/487:\tDiscriminator: real loss 0.12127518653869629, fake loss 0.15807516872882843\tGenerator: loss 2.3387131690979004\n",
            "Epoch 39, batch 301/487:\tDiscriminator: real loss 0.12568360567092896, fake loss 0.14266395568847656\tGenerator: loss 2.705564260482788\n",
            "Epoch 39, batch 351/487:\tDiscriminator: real loss 0.12563392519950867, fake loss 0.16740867495536804\tGenerator: loss 2.9272537231445312\n",
            "Epoch 39, batch 401/487:\tDiscriminator: real loss 0.1304052472114563, fake loss 0.1606166660785675\tGenerator: loss 3.087777853012085\n",
            "Epoch 39, batch 451/487:\tDiscriminator: real loss 0.12034542113542557, fake loss 0.1770758181810379\tGenerator: loss 2.8556461334228516\n",
            "Epoch 40, batch 1/487:\tDiscriminator: real loss 0.1306435763835907, fake loss 0.17704032361507416\tGenerator: loss 2.857621908187866\n",
            "Epoch 40, batch 51/487:\tDiscriminator: real loss 0.1267780214548111, fake loss 0.17948007583618164\tGenerator: loss 2.5145626068115234\n",
            "Epoch 40, batch 101/487:\tDiscriminator: real loss 0.10934479534626007, fake loss 0.19207051396369934\tGenerator: loss 3.0675418376922607\n",
            "Epoch 40, batch 151/487:\tDiscriminator: real loss 0.11532840132713318, fake loss 0.1850360631942749\tGenerator: loss 2.8489890098571777\n",
            "Epoch 40, batch 201/487:\tDiscriminator: real loss 0.129085510969162, fake loss 0.19082432985305786\tGenerator: loss 2.8230416774749756\n",
            "Epoch 40, batch 251/487:\tDiscriminator: real loss 0.10675504058599472, fake loss 0.16733357310295105\tGenerator: loss 2.6334633827209473\n",
            "Epoch 40, batch 301/487:\tDiscriminator: real loss 0.11916714906692505, fake loss 0.1793094426393509\tGenerator: loss 2.475492477416992\n",
            "Epoch 40, batch 351/487:\tDiscriminator: real loss 0.1209179237484932, fake loss 0.17351114749908447\tGenerator: loss 2.7555434703826904\n",
            "Epoch 40, batch 401/487:\tDiscriminator: real loss 0.12644651532173157, fake loss 0.1626841425895691\tGenerator: loss 2.1949188709259033\n",
            "Epoch 40, batch 451/487:\tDiscriminator: real loss 0.1378372311592102, fake loss 0.15493671596050262\tGenerator: loss 2.743652105331421\n",
            "Epoch 41, batch 1/487:\tDiscriminator: real loss 0.1255151927471161, fake loss 0.1827717125415802\tGenerator: loss 2.301039218902588\n",
            "Epoch 41, batch 51/487:\tDiscriminator: real loss 0.1192222386598587, fake loss 0.17034795880317688\tGenerator: loss 2.9675862789154053\n",
            "Epoch 41, batch 101/487:\tDiscriminator: real loss 0.10917392373085022, fake loss 0.19649778306484222\tGenerator: loss 2.361358404159546\n",
            "Epoch 41, batch 151/487:\tDiscriminator: real loss 0.13915279507637024, fake loss 0.1942409873008728\tGenerator: loss 2.596160888671875\n",
            "Epoch 41, batch 201/487:\tDiscriminator: real loss 0.12617287039756775, fake loss 0.16782903671264648\tGenerator: loss 2.672823429107666\n",
            "Epoch 41, batch 251/487:\tDiscriminator: real loss 0.11639430373907089, fake loss 0.13841581344604492\tGenerator: loss 2.1098039150238037\n",
            "Epoch 41, batch 301/487:\tDiscriminator: real loss 0.11710884422063828, fake loss 0.14930027723312378\tGenerator: loss 2.820730209350586\n",
            "Epoch 41, batch 351/487:\tDiscriminator: real loss 0.10307953506708145, fake loss 0.17251409590244293\tGenerator: loss 2.8574271202087402\n",
            "Epoch 41, batch 401/487:\tDiscriminator: real loss 0.10701169073581696, fake loss 0.15942513942718506\tGenerator: loss 2.601640224456787\n",
            "Epoch 41, batch 451/487:\tDiscriminator: real loss 0.14575926959514618, fake loss 0.17075523734092712\tGenerator: loss 2.5457398891448975\n",
            "Epoch 42, batch 1/487:\tDiscriminator: real loss 0.10388253629207611, fake loss 0.17165184020996094\tGenerator: loss 2.6456820964813232\n",
            "Epoch 42, batch 51/487:\tDiscriminator: real loss 0.11299289762973785, fake loss 0.17920129001140594\tGenerator: loss 3.130096912384033\n",
            "Epoch 42, batch 101/487:\tDiscriminator: real loss 0.11608928442001343, fake loss 0.20570136606693268\tGenerator: loss 2.868173599243164\n",
            "Epoch 42, batch 151/487:\tDiscriminator: real loss 0.11416815966367722, fake loss 0.23857922852039337\tGenerator: loss 3.2667579650878906\n",
            "Epoch 42, batch 201/487:\tDiscriminator: real loss 0.10107804089784622, fake loss 0.16620677709579468\tGenerator: loss 2.5134096145629883\n",
            "Epoch 42, batch 251/487:\tDiscriminator: real loss 0.11318563669919968, fake loss 0.19310694932937622\tGenerator: loss 2.9272897243499756\n",
            "Epoch 42, batch 301/487:\tDiscriminator: real loss 0.1205131858587265, fake loss 0.181955948472023\tGenerator: loss 2.368401288986206\n",
            "Epoch 42, batch 351/487:\tDiscriminator: real loss 0.11776594817638397, fake loss 0.19206812977790833\tGenerator: loss 2.8268916606903076\n",
            "Epoch 42, batch 401/487:\tDiscriminator: real loss 0.11566523462533951, fake loss 0.18296507000923157\tGenerator: loss 2.5436906814575195\n",
            "Epoch 42, batch 451/487:\tDiscriminator: real loss 0.12322865426540375, fake loss 0.16900187730789185\tGenerator: loss 2.5781664848327637\n",
            "Epoch 43, batch 1/487:\tDiscriminator: real loss 0.15112490952014923, fake loss 0.15928129851818085\tGenerator: loss 2.6367969512939453\n",
            "Epoch 43, batch 51/487:\tDiscriminator: real loss 0.09376844763755798, fake loss 0.16661322116851807\tGenerator: loss 2.3082990646362305\n",
            "Epoch 43, batch 101/487:\tDiscriminator: real loss 0.11886361241340637, fake loss 0.16221988201141357\tGenerator: loss 2.6823372840881348\n",
            "Epoch 43, batch 151/487:\tDiscriminator: real loss 0.09938021749258041, fake loss 0.14396624267101288\tGenerator: loss 2.184929370880127\n",
            "Epoch 43, batch 201/487:\tDiscriminator: real loss 0.10358589142560959, fake loss 0.18262062966823578\tGenerator: loss 2.9720823764801025\n",
            "Epoch 43, batch 251/487:\tDiscriminator: real loss 0.11290109157562256, fake loss 0.16308027505874634\tGenerator: loss 2.549252510070801\n",
            "Epoch 43, batch 301/487:\tDiscriminator: real loss 0.1301441788673401, fake loss 0.16574043035507202\tGenerator: loss 2.828057050704956\n",
            "Epoch 43, batch 351/487:\tDiscriminator: real loss 0.11769936233758926, fake loss 0.14901158213615417\tGenerator: loss 2.3828985691070557\n",
            "Epoch 43, batch 401/487:\tDiscriminator: real loss 0.13625963032245636, fake loss 0.14891359210014343\tGenerator: loss 2.5875511169433594\n",
            "Epoch 43, batch 451/487:\tDiscriminator: real loss 0.13597163558006287, fake loss 0.1503376066684723\tGenerator: loss 2.354585647583008\n",
            "Epoch 44, batch 1/487:\tDiscriminator: real loss 0.11982396990060806, fake loss 0.1599542200565338\tGenerator: loss 2.3859777450561523\n",
            "Epoch 44, batch 51/487:\tDiscriminator: real loss 0.11734390258789062, fake loss 0.17195576429367065\tGenerator: loss 2.741180896759033\n",
            "Epoch 44, batch 101/487:\tDiscriminator: real loss 0.11358107626438141, fake loss 0.15052364766597748\tGenerator: loss 2.8504998683929443\n",
            "Epoch 44, batch 151/487:\tDiscriminator: real loss 0.10307764261960983, fake loss 0.16307443380355835\tGenerator: loss 2.437779188156128\n",
            "Epoch 44, batch 201/487:\tDiscriminator: real loss 0.11310146749019623, fake loss 0.16865907609462738\tGenerator: loss 2.3457913398742676\n",
            "Epoch 44, batch 251/487:\tDiscriminator: real loss 0.10927864164113998, fake loss 0.14915594458580017\tGenerator: loss 2.4917967319488525\n",
            "Epoch 44, batch 301/487:\tDiscriminator: real loss 0.1195930689573288, fake loss 0.168467178940773\tGenerator: loss 2.710732936859131\n",
            "Epoch 44, batch 351/487:\tDiscriminator: real loss 0.1119522750377655, fake loss 0.1840636432170868\tGenerator: loss 2.577091693878174\n",
            "Epoch 44, batch 401/487:\tDiscriminator: real loss 0.11919587850570679, fake loss 0.17002424597740173\tGenerator: loss 2.7680745124816895\n",
            "Epoch 44, batch 451/487:\tDiscriminator: real loss 0.10795512050390244, fake loss 0.1598963588476181\tGenerator: loss 2.985038995742798\n",
            "Epoch 45, batch 1/487:\tDiscriminator: real loss 0.11891954392194748, fake loss 0.15085314214229584\tGenerator: loss 2.4650983810424805\n",
            "Epoch 45, batch 51/487:\tDiscriminator: real loss 0.1018933653831482, fake loss 0.15623393654823303\tGenerator: loss 2.509131669998169\n",
            "Epoch 45, batch 101/487:\tDiscriminator: real loss 0.12195093184709549, fake loss 0.1104145348072052\tGenerator: loss 2.523789167404175\n",
            "Epoch 45, batch 151/487:\tDiscriminator: real loss 0.10909475386142731, fake loss 0.13504871726036072\tGenerator: loss 2.75569486618042\n",
            "Epoch 45, batch 201/487:\tDiscriminator: real loss 0.11708996444940567, fake loss 0.18125048279762268\tGenerator: loss 2.701028823852539\n",
            "Epoch 45, batch 251/487:\tDiscriminator: real loss 0.1069841980934143, fake loss 0.13137325644493103\tGenerator: loss 2.842989444732666\n",
            "Epoch 45, batch 301/487:\tDiscriminator: real loss 0.12543416023254395, fake loss 0.16917270421981812\tGenerator: loss 2.7579345703125\n",
            "Epoch 45, batch 351/487:\tDiscriminator: real loss 0.10893576592206955, fake loss 0.16237008571624756\tGenerator: loss 2.8742332458496094\n",
            "Epoch 45, batch 401/487:\tDiscriminator: real loss 0.1131928563117981, fake loss 0.13826659321784973\tGenerator: loss 2.91011905670166\n",
            "Epoch 45, batch 451/487:\tDiscriminator: real loss 0.11450248956680298, fake loss 0.15407200157642365\tGenerator: loss 2.9093832969665527\n",
            "Epoch 46, batch 1/487:\tDiscriminator: real loss 0.09535937011241913, fake loss 0.15706408023834229\tGenerator: loss 3.006138324737549\n",
            "Epoch 46, batch 51/487:\tDiscriminator: real loss 0.11072149872779846, fake loss 0.12679889798164368\tGenerator: loss 2.691330671310425\n",
            "Epoch 46, batch 101/487:\tDiscriminator: real loss 0.10786599665880203, fake loss 0.1531025767326355\tGenerator: loss 2.7361574172973633\n",
            "Epoch 46, batch 151/487:\tDiscriminator: real loss 0.12182916700839996, fake loss 0.15944363176822662\tGenerator: loss 2.8499717712402344\n",
            "Epoch 46, batch 201/487:\tDiscriminator: real loss 0.10618306696414948, fake loss 0.1495017409324646\tGenerator: loss 2.8501815795898438\n",
            "Epoch 46, batch 251/487:\tDiscriminator: real loss 0.10294771194458008, fake loss 0.14623090624809265\tGenerator: loss 2.429075241088867\n",
            "Epoch 46, batch 301/487:\tDiscriminator: real loss 0.10231618583202362, fake loss 0.1623673141002655\tGenerator: loss 3.2797837257385254\n",
            "Epoch 46, batch 351/487:\tDiscriminator: real loss 0.11754932254552841, fake loss 0.1404196321964264\tGenerator: loss 2.2916626930236816\n",
            "Epoch 46, batch 401/487:\tDiscriminator: real loss 0.09907037019729614, fake loss 0.1547485888004303\tGenerator: loss 2.3594021797180176\n",
            "Epoch 46, batch 451/487:\tDiscriminator: real loss 0.1000262200832367, fake loss 0.1325073391199112\tGenerator: loss 2.9277381896972656\n",
            "Epoch 47, batch 1/487:\tDiscriminator: real loss 0.10439067333936691, fake loss 0.12404994666576385\tGenerator: loss 2.9395642280578613\n",
            "Epoch 47, batch 51/487:\tDiscriminator: real loss 0.10220006108283997, fake loss 0.1524374783039093\tGenerator: loss 2.9630215167999268\n",
            "Epoch 47, batch 101/487:\tDiscriminator: real loss 0.12540483474731445, fake loss 0.14030542969703674\tGenerator: loss 2.9183382987976074\n",
            "Epoch 47, batch 151/487:\tDiscriminator: real loss 0.1149071678519249, fake loss 0.15915672481060028\tGenerator: loss 2.579596996307373\n",
            "Epoch 47, batch 201/487:\tDiscriminator: real loss 0.10685941576957703, fake loss 0.13624325394630432\tGenerator: loss 3.088563919067383\n",
            "Epoch 47, batch 251/487:\tDiscriminator: real loss 0.08199316263198853, fake loss 0.17159147560596466\tGenerator: loss 2.600004196166992\n",
            "Epoch 47, batch 301/487:\tDiscriminator: real loss 0.08868851512670517, fake loss 0.158805251121521\tGenerator: loss 2.374967098236084\n",
            "Epoch 47, batch 351/487:\tDiscriminator: real loss 0.09966543316841125, fake loss 0.14310669898986816\tGenerator: loss 2.886733293533325\n",
            "Epoch 47, batch 401/487:\tDiscriminator: real loss 0.11149638891220093, fake loss 0.13379019498825073\tGenerator: loss 2.4815175533294678\n",
            "Epoch 47, batch 451/487:\tDiscriminator: real loss 0.11316487938165665, fake loss 0.15341800451278687\tGenerator: loss 2.3876986503601074\n",
            "Epoch 48, batch 1/487:\tDiscriminator: real loss 0.10604455322027206, fake loss 0.18053564429283142\tGenerator: loss 2.6977617740631104\n",
            "Epoch 48, batch 51/487:\tDiscriminator: real loss 0.09172007441520691, fake loss 0.13679751753807068\tGenerator: loss 2.2804360389709473\n",
            "Epoch 48, batch 101/487:\tDiscriminator: real loss 0.08241944015026093, fake loss 0.11115020513534546\tGenerator: loss 2.3609251976013184\n",
            "Epoch 48, batch 151/487:\tDiscriminator: real loss 0.08778764307498932, fake loss 0.12447869777679443\tGenerator: loss 2.69994854927063\n",
            "Epoch 48, batch 201/487:\tDiscriminator: real loss 0.11226619780063629, fake loss 0.1188448965549469\tGenerator: loss 2.7670464515686035\n",
            "Epoch 48, batch 251/487:\tDiscriminator: real loss 0.11009588837623596, fake loss 0.16738823056221008\tGenerator: loss 2.8870480060577393\n",
            "Epoch 48, batch 301/487:\tDiscriminator: real loss 0.0896017849445343, fake loss 0.1535375416278839\tGenerator: loss 3.0394773483276367\n",
            "Epoch 48, batch 351/487:\tDiscriminator: real loss 0.11861863732337952, fake loss 0.14398905634880066\tGenerator: loss 2.505138397216797\n",
            "Epoch 48, batch 401/487:\tDiscriminator: real loss 0.07855018973350525, fake loss 0.10882739722728729\tGenerator: loss 2.7096991539001465\n",
            "Epoch 48, batch 451/487:\tDiscriminator: real loss 0.09709461778402328, fake loss 0.13386963307857513\tGenerator: loss 2.821023941040039\n",
            "Epoch 49, batch 1/487:\tDiscriminator: real loss 0.10397052764892578, fake loss 0.12833765149116516\tGenerator: loss 2.655754566192627\n",
            "Epoch 49, batch 51/487:\tDiscriminator: real loss 0.10892558097839355, fake loss 0.11525999009609222\tGenerator: loss 2.767324686050415\n",
            "Epoch 49, batch 101/487:\tDiscriminator: real loss 0.09185998141765594, fake loss 0.12334343791007996\tGenerator: loss 2.535031318664551\n",
            "Epoch 49, batch 151/487:\tDiscriminator: real loss 0.10395924001932144, fake loss 0.09860767424106598\tGenerator: loss 2.77382230758667\n",
            "Epoch 49, batch 201/487:\tDiscriminator: real loss 0.08791573345661163, fake loss 0.12496296316385269\tGenerator: loss 2.673107862472534\n",
            "Epoch 49, batch 251/487:\tDiscriminator: real loss 0.10138769447803497, fake loss 0.13774442672729492\tGenerator: loss 2.9643468856811523\n",
            "Epoch 49, batch 301/487:\tDiscriminator: real loss 0.08567558228969574, fake loss 0.11592821776866913\tGenerator: loss 2.382585048675537\n",
            "Epoch 49, batch 351/487:\tDiscriminator: real loss 0.07472503930330276, fake loss 0.09793584048748016\tGenerator: loss 2.7881016731262207\n",
            "Epoch 49, batch 401/487:\tDiscriminator: real loss 0.09411336481571198, fake loss 0.123802550137043\tGenerator: loss 2.871626853942871\n",
            "Epoch 49, batch 451/487:\tDiscriminator: real loss 0.10025551170110703, fake loss 0.12948539853096008\tGenerator: loss 2.5579309463500977\n",
            "Epoch 50, batch 1/487:\tDiscriminator: real loss 0.09317337721586227, fake loss 0.11602652072906494\tGenerator: loss 2.644857883453369\n",
            "Epoch 50, batch 51/487:\tDiscriminator: real loss 0.11011754721403122, fake loss 0.11568441987037659\tGenerator: loss 2.4742629528045654\n",
            "Epoch 50, batch 101/487:\tDiscriminator: real loss 0.10601487010717392, fake loss 0.13712990283966064\tGenerator: loss 2.9031295776367188\n",
            "Epoch 50, batch 151/487:\tDiscriminator: real loss 0.09021878242492676, fake loss 0.12654906511306763\tGenerator: loss 3.06078839302063\n",
            "Epoch 50, batch 201/487:\tDiscriminator: real loss 0.10156895220279694, fake loss 0.11467303335666656\tGenerator: loss 3.008009672164917\n",
            "Epoch 50, batch 251/487:\tDiscriminator: real loss 0.07945599406957626, fake loss 0.12605860829353333\tGenerator: loss 2.626197338104248\n",
            "Epoch 50, batch 301/487:\tDiscriminator: real loss 0.09313591569662094, fake loss 0.11114251613616943\tGenerator: loss 3.1855058670043945\n",
            "Epoch 50, batch 351/487:\tDiscriminator: real loss 0.0975458174943924, fake loss 0.09875346720218658\tGenerator: loss 3.286807060241699\n",
            "Epoch 50, batch 401/487:\tDiscriminator: real loss 0.07581375539302826, fake loss 0.11841171979904175\tGenerator: loss 3.43143892288208\n",
            "Epoch 50, batch 451/487:\tDiscriminator: real loss 0.10485244542360306, fake loss 0.09003517031669617\tGenerator: loss 2.9653327465057373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1ZZM2KwySf0",
        "colab_type": "code",
        "outputId": "8d8b37ab-71d7-40d4-b41e-55e93c21b5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        }
      },
      "source": [
        "def show_generated(examples, n, c):\n",
        "  fig = plt.figure(figsize=(c, n))\n",
        "  for i in range(n*c):\n",
        "    img = fig.add_subplot(n, c, i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(examples[i,:,:,0], cmap='gray_r')\n",
        "  plt.show()\n",
        "\n",
        "n = 10\n",
        "c = 51\n",
        "\n",
        "model = load_model('generator.h5')\n",
        "noise, _ = generate_latent_noise(latent_dim, n*c)\n",
        "labs = np.asarray([x for _ in range(n) for x in range(c)])\n",
        "\n",
        "X = model.predict([noise, labs])\n",
        "# scale from [-1, 1] to [0, 1]\n",
        "X = (X + 1) / 2.0\n",
        "x = np.squeeze(X[0])\n",
        "plt.imshow(x)\n",
        "\n",
        "show_generated(X, n, c)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 3672x720 with 510 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZxgQ6QW_x4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}